{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc258d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ___________________________\n",
      "|                           |\n",
      "|======== YearDream ========|\n",
      "|===========================|\n",
      "|==== DLC Well Imported ====|\n",
      "|===========================|\n",
      "|========= BYJASON =========|\n",
      "|________26th_Jun_2023________|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier as lgb\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch\n",
    "import dacon_law_class as dlc\n",
    "from dacon_law_class import SimpleOps as so\n",
    "from sklearn.model_selection import GridSearchCV as GSCV\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification, pipeline\n",
    "from tqdm import tqdm\n",
    "import xgboost.sklearn as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247af21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "sample_submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011b50f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train.head()\n",
    "# test\n",
    "# sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a8f896",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "@article{turc2019,\n",
    "  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},\n",
    "  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n",
    "  journal={arXiv preprint arXiv:1908.08962v2 },\n",
    "  year={2019}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f6e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_facts = dlc.alpha_numeric_3_cols(train, 'first_party', 'second_party', 'facts')\n",
    "test_facts = dlc.alpha_numeric_3_cols(test, 'first_party', 'second_party', 'facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82f4953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_facts = pd.DataFrame(train['facts'])\n",
    "test_fact = pd.DataFrame(test['facts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84426c7a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "train_all_done, test_all_done = dlc.rename_tokenized(train, test, 'first_party', 'second_party', 'facts')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c0cff0",
   "metadata": {},
   "source": [
    "# 여기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1145d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = pd.read_csv('./train_correlations.csv')\n",
    "test_tokenized = pd.read_csv('./test_correlations.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d177bb7f",
   "metadata": {},
   "source": [
    "train_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a7785",
   "metadata": {},
   "source": [
    "test_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd6173c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 2478/2478 [00:00<00:00, 508736.43it/s]\n",
      "100%|██████████████████████████████████| 2478/2478 [00:00<00:00, 2077035.43it/s]\n",
      "100%|██████████████████████████████████| 2478/2478 [00:00<00:00, 2341929.99it/s]\n",
      "100%|██████████████████████████████████| 1240/1240 [00:00<00:00, 1994224.29it/s]\n",
      "100%|███████████████████████████████████| 1240/1240 [00:00<00:00, 813027.51it/s]\n",
      "100%|██████████████████████████████████| 1240/1240 [00:00<00:00, 1460527.09it/s]\n"
     ]
    }
   ],
   "source": [
    "train_token = dlc.token_to_df(train_tokenized)\n",
    "test_token = dlc.token_to_df(test_tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0215c176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_party_1</th>\n",
       "      <th>first_party_2</th>\n",
       "      <th>second_party_1</th>\n",
       "      <th>second_party_2</th>\n",
       "      <th>facts_1</th>\n",
       "      <th>facts_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.4463822543621063</td>\n",
       "      <td>-0.8948424458503723</td>\n",
       "      <td>-0.9863601922988892</td>\n",
       "      <td>0.16460131108760834</td>\n",
       "      <td>0.21225066483020782</td>\n",
       "      <td>0.9772152304649353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9353076219558716</td>\n",
       "      <td>-0.35383573174476624</td>\n",
       "      <td>-0.9495260119438171</td>\n",
       "      <td>0.31368815898895264</td>\n",
       "      <td>0.2700807750225067</td>\n",
       "      <td>0.9628376364707947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.017951782792806625</td>\n",
       "      <td>-0.9998388290405273</td>\n",
       "      <td>-0.5238009095191956</td>\n",
       "      <td>-0.8518407344818115</td>\n",
       "      <td>0.3102778196334839</td>\n",
       "      <td>0.950645923614502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.19457940757274628</td>\n",
       "      <td>-0.9808868169784546</td>\n",
       "      <td>-0.9964062571525574</td>\n",
       "      <td>0.08470305055379868</td>\n",
       "      <td>0.17634129524230957</td>\n",
       "      <td>0.984329104423523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.012838165275752544</td>\n",
       "      <td>-0.9999175667762756</td>\n",
       "      <td>-0.4199574291706085</td>\n",
       "      <td>0.9075437784194946</td>\n",
       "      <td>0.13304223120212555</td>\n",
       "      <td>0.9911103844642639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>0.7062230110168457</td>\n",
       "      <td>-0.7079894542694092</td>\n",
       "      <td>0.3479677438735962</td>\n",
       "      <td>-0.9375064969062805</td>\n",
       "      <td>0.4467298090457916</td>\n",
       "      <td>0.8946689367294312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>0.7081589102745056</td>\n",
       "      <td>-0.7060530781745911</td>\n",
       "      <td>0.9421269297599792</td>\n",
       "      <td>0.3352562487125397</td>\n",
       "      <td>0.2615622580051422</td>\n",
       "      <td>0.965186595916748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>0.8284182548522949</td>\n",
       "      <td>-0.5601099133491516</td>\n",
       "      <td>-0.4199574291706085</td>\n",
       "      <td>0.9075437784194946</td>\n",
       "      <td>0.2331738919019699</td>\n",
       "      <td>0.9724350571632385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>0.36393409967422485</td>\n",
       "      <td>-0.9314247369766235</td>\n",
       "      <td>-0.7260524034500122</td>\n",
       "      <td>-0.6876394152641296</td>\n",
       "      <td>0.4886071979999542</td>\n",
       "      <td>0.8725038766860962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>0.41166985034942627</td>\n",
       "      <td>-0.9113330245018005</td>\n",
       "      <td>-0.9095264673233032</td>\n",
       "      <td>0.415645956993103</td>\n",
       "      <td>0.2751102149486542</td>\n",
       "      <td>0.9614126682281494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2478 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              first_party_1          first_party_2       second_party_1  \\\n",
       "0       -0.4463822543621063    -0.8948424458503723  -0.9863601922988892   \n",
       "1        0.9353076219558716   -0.35383573174476624  -0.9495260119438171   \n",
       "2     -0.017951782792806625    -0.9998388290405273  -0.5238009095191956   \n",
       "3      -0.19457940757274628    -0.9808868169784546  -0.9964062571525574   \n",
       "4     -0.012838165275752544    -0.9999175667762756  -0.4199574291706085   \n",
       "...                     ...                    ...                  ...   \n",
       "2473     0.7062230110168457    -0.7079894542694092   0.3479677438735962   \n",
       "2474     0.7081589102745056    -0.7060530781745911   0.9421269297599792   \n",
       "2475     0.8284182548522949    -0.5601099133491516  -0.4199574291706085   \n",
       "2476    0.36393409967422485    -0.9314247369766235  -0.7260524034500122   \n",
       "2477    0.41166985034942627    -0.9113330245018005  -0.9095264673233032   \n",
       "\n",
       "            second_party_2              facts_1              facts_2  \n",
       "0      0.16460131108760834  0.21225066483020782   0.9772152304649353  \n",
       "1      0.31368815898895264   0.2700807750225067   0.9628376364707947  \n",
       "2      -0.8518407344818115   0.3102778196334839    0.950645923614502  \n",
       "3      0.08470305055379868  0.17634129524230957    0.984329104423523  \n",
       "4       0.9075437784194946  0.13304223120212555   0.9911103844642639  \n",
       "...                    ...                  ...                  ...  \n",
       "2473   -0.9375064969062805   0.4467298090457916   0.8946689367294312  \n",
       "2474    0.3352562487125397   0.2615622580051422    0.965186595916748  \n",
       "2475    0.9075437784194946   0.2331738919019699   0.9724350571632385  \n",
       "2476   -0.6876394152641296   0.4886071979999542   0.8725038766860962  \n",
       "2477     0.415645956993103   0.2751102149486542   0.9614126682281494  \n",
       "\n",
       "[2478 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97197bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_party_1</th>\n",
       "      <th>first_party_2</th>\n",
       "      <th>second_party_1</th>\n",
       "      <th>second_party_2</th>\n",
       "      <th>facts_1</th>\n",
       "      <th>facts_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.9378802180290222</td>\n",
       "      <td>0.34695932269096375</td>\n",
       "      <td>0.9965190887451172</td>\n",
       "      <td>0.0833643451333046</td>\n",
       "      <td>-0.15676727890968323</td>\n",
       "      <td>0.9876355528831482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.044631924480199814</td>\n",
       "      <td>0.999003529548645</td>\n",
       "      <td>0.9047352075576782</td>\n",
       "      <td>0.4259744882583618</td>\n",
       "      <td>-0.5222179293632507</td>\n",
       "      <td>0.8528120517730713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6751019358634949</td>\n",
       "      <td>0.7377244234085083</td>\n",
       "      <td>0.9083398580551147</td>\n",
       "      <td>-0.41823291778564453</td>\n",
       "      <td>-0.45306575298309326</td>\n",
       "      <td>0.891477108001709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.6089559197425842</td>\n",
       "      <td>0.793204128742218</td>\n",
       "      <td>0.9965190887451172</td>\n",
       "      <td>0.0833643451333046</td>\n",
       "      <td>0.07415395230054855</td>\n",
       "      <td>0.9972467422485352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.6693098545074463</td>\n",
       "      <td>0.7429834604263306</td>\n",
       "      <td>0.999426543712616</td>\n",
       "      <td>-0.033860500901937485</td>\n",
       "      <td>-0.23630794882774353</td>\n",
       "      <td>0.9716782569885254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>0.48429757356643677</td>\n",
       "      <td>0.8749033212661743</td>\n",
       "      <td>0.661633312702179</td>\n",
       "      <td>0.74982750415802</td>\n",
       "      <td>0.48287805914878845</td>\n",
       "      <td>0.8756875991821289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>-0.04792654141783714</td>\n",
       "      <td>0.9988508224487305</td>\n",
       "      <td>0.8590741157531738</td>\n",
       "      <td>0.5118512511253357</td>\n",
       "      <td>-0.6453915238380432</td>\n",
       "      <td>0.7638519406318665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>0.11585450917482376</td>\n",
       "      <td>0.993266224861145</td>\n",
       "      <td>0.9999958276748657</td>\n",
       "      <td>-0.002867066999897361</td>\n",
       "      <td>-0.2503175735473633</td>\n",
       "      <td>0.9681637287139893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>-0.7357914447784424</td>\n",
       "      <td>0.6772081851959229</td>\n",
       "      <td>0.9894566535949707</td>\n",
       "      <td>-0.14482951164245605</td>\n",
       "      <td>0.003927184734493494</td>\n",
       "      <td>0.999992311000824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>0.9855904579162598</td>\n",
       "      <td>0.16914908587932587</td>\n",
       "      <td>0.9365572333335876</td>\n",
       "      <td>0.35051456093788147</td>\n",
       "      <td>0.10095714032649994</td>\n",
       "      <td>0.9948906898498535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             first_party_1         first_party_2      second_party_1  \\\n",
       "0      -0.9378802180290222   0.34695932269096375  0.9965190887451172   \n",
       "1     0.044631924480199814     0.999003529548645  0.9047352075576782   \n",
       "2       0.6751019358634949    0.7377244234085083  0.9083398580551147   \n",
       "3      -0.6089559197425842     0.793204128742218  0.9965190887451172   \n",
       "4      -0.6693098545074463    0.7429834604263306   0.999426543712616   \n",
       "...                    ...                   ...                 ...   \n",
       "1235   0.48429757356643677    0.8749033212661743   0.661633312702179   \n",
       "1236  -0.04792654141783714    0.9988508224487305  0.8590741157531738   \n",
       "1237   0.11585450917482376     0.993266224861145  0.9999958276748657   \n",
       "1238   -0.7357914447784424    0.6772081851959229  0.9894566535949707   \n",
       "1239    0.9855904579162598   0.16914908587932587  0.9365572333335876   \n",
       "\n",
       "              second_party_2               facts_1              facts_2  \n",
       "0         0.0833643451333046  -0.15676727890968323   0.9876355528831482  \n",
       "1         0.4259744882583618   -0.5222179293632507   0.8528120517730713  \n",
       "2       -0.41823291778564453  -0.45306575298309326    0.891477108001709  \n",
       "3         0.0833643451333046   0.07415395230054855   0.9972467422485352  \n",
       "4      -0.033860500901937485  -0.23630794882774353   0.9716782569885254  \n",
       "...                      ...                   ...                  ...  \n",
       "1235        0.74982750415802   0.48287805914878845   0.8756875991821289  \n",
       "1236      0.5118512511253357   -0.6453915238380432   0.7638519406318665  \n",
       "1237   -0.002867066999897361   -0.2503175735473633   0.9681637287139893  \n",
       "1238    -0.14482951164245605  0.003927184734493494    0.999992311000824  \n",
       "1239     0.35051456093788147   0.10095714032649994   0.9948906898498535  \n",
       "\n",
       "[1240 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5a20b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    '''\n",
    "    하단 tokenizer를 위한 definition\n",
    "    '''\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def auto_tokenizer(df, column_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    bert_model = 'nlpaueb/bert-base-uncased-contracts'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(bert_model)\n",
    "    model = model.to(device)\n",
    "    nlp = pipeline('ner', model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "    ei_total_list = []\n",
    "    \n",
    "    \n",
    "    batch_size = 100\n",
    "    num_rows = df.shape[0]\n",
    "    num_batches = (num_rows + batch_size -1) // batch_size\n",
    "    \n",
    "    for i in tqdm(range(num_batches)):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, num_rows)\n",
    "        batch_df = df.iloc[start_idx : end_idx]\n",
    "        encoded_input_list = []    \n",
    "        for text in tqdm(df[column_name]):\n",
    "            text = text.lower()\n",
    "            entities = nlp(text)\n",
    "\n",
    "            party_names = {}\n",
    "            for entity in entities:\n",
    "                if 'entity_group' in entity and entity['entity_group'] == 'LABEL_1':\n",
    "                    if 'word' in entity:\n",
    "                        party = entity['word']\n",
    "                        if party not in party_names:\n",
    "                            party_names[party] = {'first_name': '', 'family_name': ''}\n",
    "                            names = re.findall(r'\\b\\w+\\b', party)\n",
    "                            if len(names) == 2:\n",
    "                                party_names[party]['first_name'] = names[0]\n",
    "                                party_names[party]['family_name'] = names[1]\n",
    "                            elif len(names) == 1:\n",
    "                                party_names[party]['first_name'] = names[0]\n",
    "                else:\n",
    "                    if 'party' in entity:\n",
    "                        party = entity['party']\n",
    "                        if party not in party_names:\n",
    "                            party_names[party] = {'first_name': '', 'family_name': ''}\n",
    "                        if 'first_name' in entity:\n",
    "                            party_names['party']['first_name'] = entity['first_name']\n",
    "                        if 'family_name' in entity:\n",
    "                            party_names[party]['family_name'] = entity['family_name']\n",
    "\n",
    "#             list_of_states = [\n",
    "#                 'wyoming', 'wisconsin', 'west virginia', 'washington', 'virginia',\n",
    "#                 'vermont', 'utah', 'texas', 'tennessee', 'south dakota',\n",
    "#                 'south carolina', 'rhode island', 'pennsylvania', 'oregon', 'oklahoma',\n",
    "#                 'ohio', 'north dakota', 'north carolina', 'new york', 'new mexico',\n",
    "#                 'new jersey', 'new hampshire', 'nevada', 'nebraska', 'montana',\n",
    "#                 'missouri', 'mississippi', 'minnesota', 'michigan', 'massachusetts',\n",
    "#                 'maryland', 'maine', 'louisiana', 'kentucky', 'kansas',\n",
    "#                 'iowa', 'indiana', 'illinois', 'idaho', 'hawaii',\n",
    "#                 'georgia', 'florida', 'delaware', 'connecticut', 'colorado',\n",
    "#                 'california', 'arkansas', 'arizona', 'alaska', 'alabama'\n",
    "\n",
    "#             ]\n",
    "\n",
    "            list_of_usa = ['usa', 'america', 'u.s.', 'united states', 'the states', 'the us', 'the united states',\n",
    "                           'the united states of america', 'the u.s.', 'the usa']\n",
    "\n",
    "            masked_text = text\n",
    "            for party, names in party_names.items():\n",
    "                first_name = names['first_name']\n",
    "                family_name = names['family_name']\n",
    "\n",
    "#                 if first_name in list_of_states:\n",
    "#                     first_name = '[MASK]'\n",
    "\n",
    "#                 if family_name in list_of_states:\n",
    "#                     family_name = '[MASK]'\n",
    "\n",
    "                if first_name in list_of_usa:\n",
    "                    first_name = '[MASK]'\n",
    "                if family_name in list_of_usa:\n",
    "                    family_name = '[MASK]'\n",
    "\n",
    "                masked_text = masked_text.replace(first_name, '[MASK]')\n",
    "                masked_text = masked_text.replace(family_name, '[MASK]')\n",
    "\n",
    "#             for state in list_of_states:\n",
    "#                 masked_text = masked_text.replace(state, '[MASK]')\n",
    "\n",
    "            for usa in list_of_usa:\n",
    "                masked_text = masked_text.replace(usa, '[MASK]')\n",
    "\n",
    "            encoded_input = tokenizer(masked_text, padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n",
    "            encoded_input = {key: value.to(device) for key, value in encoded_input.items()}\n",
    "            encoded_input_list.append(encoded_input)\n",
    "\n",
    "            for encoded_input in encoded_input_list:\n",
    "                with torch.no_grad():\n",
    "                    model_output = model(**encoded_input)\n",
    "\n",
    "                sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "                sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "                ei_total_list.append(sentence_embeddings.squeeze().cpu().numpy())\n",
    "\n",
    "    df_berted = np.array(ei_total_list)\n",
    "\n",
    "    return df_berted\n",
    "\n",
    "def rename_tokenized(df_1, df_2, column_1, column_2, column_3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    df_1_list = []\n",
    "    df_2_list = []\n",
    "    df_list = [df_1, df_2]\n",
    "    column_list = [column_1, column_2, column_3]\n",
    "\n",
    "    for df in df_list:\n",
    "        for col in column_list:\n",
    "            df_berted = auto_tokenizer(df, col)\n",
    "\n",
    "            if isinstance(df_berted, np.ndarray):\n",
    "                column_names = [f'{col}_berted_{i}' for i in range(df_berted.shape[1])]\n",
    "                df_berted = pd.DataFrame(df_berted, columns=column_names)\n",
    "\n",
    "            tokenized_data = []\n",
    "            for _, row in df_berted.iterrows():\n",
    "                tensor = torch.tensor(row.values, device=device)\n",
    "                tokenized_data.append(tensor.tolist())\n",
    "\n",
    "            if df is df_1:\n",
    "                df_1_list.extend([tokenized_data])\n",
    "            elif df is df_2:\n",
    "                df_2_list.extend([tokenized_data])\n",
    "\n",
    "    df_1_df = pd.DataFrame(df_1_list, index=column_list)\n",
    "    df_2_df = pd.DataFrame(df_2_list, index=column_list)\n",
    "\n",
    "    df_1_df = df_1_df.T\n",
    "    df_2_df = df_2_df.T\n",
    "\n",
    "    return df_1_df, df_2_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cd3ade7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/bert-base-uncased-contracts were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at nlpaueb/bert-base-uncased-contracts and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\n",
      "  0%|                                                  | 0/2478 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                          | 1/2478 [00:00<16:52,  2.45it/s]\u001b[A\n",
      "  0%|                                          | 2/2478 [00:01<21:59,  1.88it/s]\u001b[A\n",
      "  0%|                                          | 3/2478 [00:01<28:27,  1.45it/s]\u001b[A\n",
      "  0%|                                          | 4/2478 [00:03<35:57,  1.15it/s]\u001b[A\n",
      "  0%|                                          | 5/2478 [00:04<43:39,  1.06s/it]\u001b[A\n",
      "  0%|                                          | 6/2478 [00:06<52:38,  1.28s/it]\u001b[A\n",
      "  0%|                                        | 7/2478 [00:08<1:06:53,  1.62s/it]\u001b[A\n",
      "  0%|▏                                       | 8/2478 [00:10<1:13:41,  1.79s/it]\u001b[A\n",
      "  0%|▏                                       | 9/2478 [00:13<1:21:15,  1.97s/it]\u001b[A\n",
      "  0%|▏                                      | 10/2478 [00:15<1:29:52,  2.18s/it]\u001b[A\n",
      "  0%|▏                                      | 11/2478 [00:18<1:38:14,  2.39s/it]\u001b[A\n",
      "  0%|▏                                      | 12/2478 [00:21<1:45:58,  2.58s/it]\u001b[A\n",
      "  1%|▏                                      | 13/2478 [00:24<1:54:24,  2.78s/it]\u001b[A\n",
      "  1%|▏                                      | 14/2478 [00:28<2:03:25,  3.01s/it]\u001b[A\n",
      "  1%|▏                                      | 15/2478 [00:32<2:12:39,  3.23s/it]\u001b[A\n",
      "  1%|▎                                      | 16/2478 [00:36<2:22:12,  3.47s/it]\u001b[A\n",
      "  1%|▎                                      | 17/2478 [00:40<2:32:01,  3.71s/it]\u001b[A\n",
      "  1%|▎                                      | 18/2478 [00:44<2:42:00,  3.95s/it]\u001b[A\n",
      "  1%|▎                                      | 19/2478 [00:49<2:51:50,  4.19s/it]\u001b[A\n",
      "  1%|▎                                      | 20/2478 [00:54<3:02:35,  4.46s/it]\u001b[A\n",
      "  1%|▎                                      | 21/2478 [00:59<3:12:14,  4.69s/it]\u001b[A\n",
      "  1%|▎                                      | 22/2478 [01:05<3:21:28,  4.92s/it]\u001b[A\n",
      "  1%|▎                                      | 23/2478 [01:11<3:30:30,  5.14s/it]\u001b[A\n",
      "  1%|▍                                      | 24/2478 [01:16<3:39:56,  5.38s/it]\u001b[A\n",
      "  1%|▍                                      | 25/2478 [01:23<3:50:50,  5.65s/it]\u001b[A\n",
      "  1%|▍                                   | 26/2478 [16:35<189:02:19, 277.54s/it]\u001b[A\n",
      "  1%|▍                                   | 27/2478 [16:41<133:38:04, 196.28s/it]\u001b[A\n",
      "  1%|▍                                    | 28/2478 [16:48<94:52:37, 139.41s/it]\u001b[A\n",
      "  1%|▍                                     | 29/2478 [16:55<67:49:14, 99.70s/it]\u001b[A\n",
      "  1%|▍                                     | 30/2478 [17:02<48:56:04, 71.96s/it]\u001b[A\n",
      "  1%|▍                                     | 31/2478 [17:10<35:45:56, 52.62s/it]\u001b[A\n",
      "  1%|▍                                   | 32/2478 [33:11<221:00:59, 325.29s/it]\u001b[A\n",
      "  1%|▍                                   | 33/2478 [33:20<156:24:57, 230.31s/it]\u001b[A\n",
      "  1%|▍                                   | 34/2478 [33:30<111:29:21, 164.22s/it]\u001b[A\n",
      "  1%|▌                                    | 35/2478 [33:39<79:44:11, 117.50s/it]\u001b[A\n",
      "  1%|▌                                     | 36/2478 [33:47<57:32:54, 84.84s/it]\u001b[A\n",
      "  1%|▌                                     | 37/2478 [33:56<42:06:00, 62.09s/it]\u001b[A\n",
      "  2%|▌                                     | 38/2478 [34:05<31:18:35, 46.19s/it]\u001b[A\n",
      "  2%|▌                                     | 39/2478 [34:15<23:48:36, 35.14s/it]\u001b[A\n",
      "  2%|▌                                     | 40/2478 [38:07<63:50:08, 94.26s/it]\u001b[A\n",
      "  2%|▋                                     | 41/2478 [38:17<46:42:45, 69.00s/it]\u001b[A\n",
      "  2%|▋                                     | 42/2478 [38:27<34:46:28, 51.39s/it]\u001b[A\n",
      "  2%|▋                                     | 43/2478 [38:38<26:28:13, 39.14s/it]\u001b[A\n",
      "  2%|▋                                     | 44/2478 [38:48<20:41:26, 30.60s/it]\u001b[A\n",
      "  2%|▋                                     | 45/2478 [38:59<16:40:01, 24.66s/it]\u001b[A\n",
      "  2%|▋                                     | 46/2478 [39:10<13:53:45, 20.57s/it]\u001b[A\n",
      "  2%|▋                                     | 47/2478 [39:22<12:00:53, 17.79s/it]\u001b[A\n",
      "  2%|▋                                   | 48/2478 [55:35<205:26:39, 304.36s/it]\u001b[A\n",
      "  2%|▋                                   | 49/2478 [55:47<146:13:53, 216.73s/it]\u001b[A\n",
      "  2%|▋                                   | 50/2478 [55:59<104:48:21, 155.40s/it]\u001b[A\n",
      "  2%|▊                                    | 51/2478 [56:11<75:50:11, 112.49s/it]\u001b[A\n",
      "  2%|▊                                     | 52/2478 [56:24<55:37:22, 82.54s/it]\u001b[A\n",
      "  2%|▊                                     | 53/2478 [56:37<41:31:26, 61.64s/it]\u001b[A\n",
      "  2%|▋                                 | 54/2478 [1:08:01<167:14:43, 248.38s/it]\u001b[A\n",
      "  2%|▊                                 | 55/2478 [1:08:15<119:45:56, 177.94s/it]\u001b[A\n",
      "  2%|▊                                  | 56/2478 [1:08:29<86:34:58, 128.69s/it]\u001b[A\n",
      "  2%|▊                                   | 57/2478 [1:08:43<63:25:49, 94.32s/it]\u001b[A\n",
      "  2%|▊                                 | 58/2478 [1:23:57<228:49:51, 340.41s/it]\u001b[A\n",
      "  2%|▊                                 | 59/2478 [1:24:12<163:02:31, 242.64s/it]\u001b[A\n",
      "  2%|▊                                 | 60/2478 [1:24:27<117:03:33, 174.28s/it]\u001b[A\n",
      "  2%|▊                                 | 61/2478 [1:41:37<289:31:22, 431.23s/it]\u001b[A\n",
      "  3%|▊                                 | 62/2478 [1:41:53<205:43:43, 306.55s/it]\u001b[A\n",
      "  3%|▊                                 | 63/2478 [1:42:08<147:00:40, 219.15s/it]\u001b[A\n",
      "  3%|▉                                 | 64/2478 [1:42:24<105:57:27, 158.01s/it]\u001b[A\n",
      "  3%|▉                                  | 65/2478 [1:42:39<77:16:18, 115.28s/it]\u001b[A\n",
      "  3%|▉                                 | 66/2478 [1:58:00<239:12:28, 357.03s/it]\u001b[A\n",
      "  3%|▉                                 | 67/2478 [1:58:17<170:41:15, 254.86s/it]\u001b[A\n",
      "  3%|▉                                 | 68/2478 [1:58:33<122:46:31, 183.40s/it]\u001b[A\n",
      "  3%|▉                                  | 69/2478 [1:58:50<89:19:02, 133.48s/it]\u001b[A\n",
      "  3%|▉                                 | 70/2478 [2:14:17<248:26:08, 371.42s/it]\u001b[A\n",
      "  3%|▉                                 | 71/2478 [2:14:35<177:21:46, 265.27s/it]\u001b[A\n",
      "  3%|▉                                 | 72/2478 [2:14:52<127:38:51, 190.99s/it]\u001b[A\n",
      "  3%|█                                 | 73/2478 [2:31:27<288:41:54, 432.15s/it]\u001b[A\n",
      "  3%|█                                 | 74/2478 [2:31:45<205:39:01, 307.96s/it]\u001b[A\n",
      "  3%|█                                 | 75/2478 [2:47:51<337:16:41, 505.29s/it]\u001b[A\n",
      "  3%|█                                 | 76/2478 [2:48:10<239:47:26, 359.39s/it]\u001b[A\n",
      "  3%|█                                 | 77/2478 [2:48:28<171:30:01, 257.14s/it]\u001b[A\n",
      "  3%|█                                 | 78/2478 [2:48:47<123:43:56, 185.60s/it]\u001b[A\n",
      "  3%|█                                  | 79/2478 [2:49:06<90:21:35, 135.60s/it]\u001b[A\n",
      "  3%|█                                 | 80/2478 [3:05:27<259:14:33, 389.19s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█                                 | 81/2478 [3:05:47<185:20:50, 278.37s/it]\u001b[A\n",
      "  3%|█▏                                | 82/2478 [3:06:06<133:37:32, 200.77s/it]\u001b[A\n",
      "  3%|█▏                                 | 83/2478 [3:06:26<97:29:45, 146.55s/it]\u001b[A\n",
      "  3%|█▏                                | 84/2478 [3:21:57<253:53:51, 381.80s/it]\u001b[A\n",
      "  3%|█▏                                | 85/2478 [3:22:18<181:50:07, 273.55s/it]\u001b[A\n",
      "  3%|█▏                                | 86/2478 [3:22:39<131:26:15, 197.82s/it]\u001b[A\n",
      "  4%|█▏                                | 87/2478 [3:39:48<296:56:55, 447.10s/it]\u001b[A\n",
      "  4%|█▏                                | 88/2478 [3:40:10<212:03:10, 319.41s/it]\u001b[A\n",
      "  4%|█▏                                | 89/2478 [3:40:31<152:42:28, 230.12s/it]\u001b[A\n",
      "  4%|█▏                                | 90/2478 [3:40:53<111:14:16, 167.70s/it]\u001b[A\n",
      "  4%|█▎                                 | 91/2478 [3:41:16<82:17:04, 124.10s/it]\u001b[A\n",
      "  4%|█▎                                  | 92/2478 [3:41:38<62:04:24, 93.66s/it]\u001b[A\n",
      "  4%|█▎                                | 93/2478 [3:57:16<229:42:27, 346.73s/it]\u001b[A\n",
      "  4%|█▎                                | 94/2478 [3:57:39<165:17:58, 249.61s/it]\u001b[A\n",
      "  4%|█▎                                | 95/2478 [3:58:02<120:20:20, 181.80s/it]\u001b[A\n",
      "  4%|█▎                                | 96/2478 [4:15:49<296:00:58, 447.38s/it]\u001b[A\n",
      "  4%|█▎                                | 97/2478 [4:16:13<211:51:38, 320.33s/it]\u001b[A\n",
      "  4%|█▎                                | 98/2478 [4:25:28<258:13:48, 390.60s/it]\u001b[A\n",
      "  4%|█▎                                | 99/2478 [4:25:52<185:29:18, 280.69s/it]\u001b[A\n",
      "  4%|█▎                               | 100/2478 [4:41:36<316:54:37, 479.76s/it]\u001b[A\n",
      "  4%|█▎                               | 101/2478 [4:42:01<226:34:56, 343.16s/it]\u001b[A\n",
      "  4%|█▎                               | 102/2478 [4:42:25<163:24:14, 247.58s/it]\u001b[A\n",
      "  4%|█▎                               | 103/2478 [4:43:26<126:23:04, 191.57s/it]\u001b[A\n",
      "  4%|█▍                                | 104/2478 [4:43:52<93:31:33, 141.83s/it]\u001b[A\n",
      "  4%|█▍                               | 105/2478 [4:59:17<248:29:52, 376.99s/it]\u001b[A\n",
      "  4%|█▍                               | 106/2478 [4:59:44<179:10:49, 271.94s/it]\u001b[A\n",
      "  4%|█▍                               | 107/2478 [5:00:10<130:33:05, 198.22s/it]\u001b[A\n",
      "  4%|█▍                               | 108/2478 [5:16:33<285:23:23, 433.50s/it]\u001b[A\n",
      "  4%|█▍                               | 109/2478 [5:16:59<204:51:19, 311.30s/it]\u001b[A\n",
      "  4%|█▍                               | 110/2478 [5:17:26<148:35:26, 225.90s/it]\u001b[A\n",
      "  4%|█▍                               | 111/2478 [5:33:14<290:55:15, 442.47s/it]\u001b[A\n",
      "  5%|█▍                               | 112/2478 [5:33:43<209:17:43, 318.45s/it]\u001b[A\n",
      "  5%|█▌                               | 113/2478 [5:34:10<151:53:52, 231.22s/it]\u001b[A\n",
      "  5%|█▌                               | 114/2478 [5:34:38<111:48:27, 170.27s/it]\u001b[A\n",
      "  5%|█▌                                | 115/2478 [5:35:06<83:42:22, 127.53s/it]\u001b[A\n",
      "  5%|█▌                               | 116/2478 [5:50:41<242:37:59, 369.81s/it]\u001b[A\n",
      "  5%|█▌                               | 117/2478 [6:07:24<367:02:51, 559.67s/it]\u001b[A\n",
      "  5%|█▌                               | 118/2478 [6:07:53<262:36:43, 400.59s/it]\u001b[A\n",
      "  5%|█▌                               | 119/2478 [6:08:22<189:22:29, 289.00s/it]\u001b[A\n",
      "  5%|█▌                               | 120/2478 [6:25:07<330:03:30, 503.91s/it]\u001b[A\n",
      "  5%|█▌                               | 121/2478 [6:40:42<414:30:35, 633.11s/it]\u001b[A\n",
      "  5%|█▌                               | 122/2478 [6:41:12<295:51:07, 452.07s/it]\u001b[A\n",
      "  5%|█▋                               | 123/2478 [6:41:41<212:44:11, 325.20s/it]\u001b[A\n",
      "  5%|█▋                               | 124/2478 [6:54:07<295:10:56, 451.43s/it]\u001b[A\n",
      "  5%|█▋                               | 125/2478 [7:09:44<390:13:52, 597.04s/it]\u001b[A\n",
      "  5%|█▋                               | 126/2478 [7:10:14<279:02:57, 427.12s/it]\u001b[A\n",
      "  5%|█▋                               | 127/2478 [7:26:28<385:59:40, 591.06s/it]\u001b[A\n",
      "  5%|█▋                               | 128/2478 [7:44:04<476:57:42, 730.66s/it]\u001b[A\n",
      "  5%|█▋                               | 129/2478 [7:44:36<339:54:35, 520.93s/it]\u001b[A\n",
      "  5%|█▋                               | 130/2478 [8:00:49<428:14:10, 656.58s/it]\u001b[A\n",
      "  5%|█▋                               | 131/2478 [8:01:21<305:51:28, 469.15s/it]\u001b[A\n",
      "  5%|█▊                               | 132/2478 [8:13:24<355:20:43, 545.29s/it]\u001b[A\n",
      "  5%|█▊                               | 133/2478 [8:13:56<254:55:48, 391.36s/it]\u001b[A\n",
      "  5%|█▊                               | 134/2478 [8:17:04<215:09:18, 330.44s/it]\u001b[A\n",
      "  5%|█▊                               | 135/2478 [8:18:40<169:14:08, 260.03s/it]\u001b[A\n",
      "  5%|█▊                               | 136/2478 [8:19:13<124:53:38, 191.98s/it]\u001b[A\n",
      "  6%|█▉                                | 137/2478 [8:20:02<96:59:39, 149.16s/it]\u001b[A\n",
      "  6%|█▊                               | 138/2478 [8:33:24<224:15:10, 345.00s/it]\u001b[A\n",
      "  6%|█▊                               | 139/2478 [8:33:58<163:31:21, 251.68s/it]\u001b[A\n",
      "  6%|█▊                               | 140/2478 [8:38:25<166:29:50, 256.37s/it]\u001b[A\n",
      "  6%|█▉                               | 141/2478 [8:38:59<123:02:29, 189.54s/it]\u001b[A\n",
      "  6%|█▉                               | 142/2478 [8:54:45<270:19:43, 416.60s/it]\u001b[A\n",
      "  6%|█▉                               | 143/2478 [9:11:38<386:12:58, 595.45s/it]\u001b[A\n",
      "  6%|█▉                               | 144/2478 [9:19:35<363:02:29, 559.96s/it]\u001b[A\n",
      "  6%|█▉                               | 145/2478 [9:20:11<260:52:52, 402.56s/it]\u001b[A\n",
      "  6%|█▉                               | 146/2478 [9:37:20<382:30:59, 590.51s/it]\u001b[A\n",
      "  6%|█▉                               | 147/2478 [9:52:27<443:57:44, 685.66s/it]\u001b[A\n",
      "  6%|█▉                               | 148/2478 [9:53:10<318:59:51, 492.87s/it]\u001b[A\n",
      "  6%|█▉                               | 149/2478 [9:53:47<230:15:53, 355.93s/it]\u001b[A\n",
      "  6%|█▉                              | 150/2478 [10:09:37<345:28:14, 534.23s/it]\u001b[A\n",
      "  6%|█▉                              | 151/2478 [10:10:14<248:52:29, 385.02s/it]\u001b[A\n",
      "  6%|█▉                              | 152/2478 [10:16:33<247:35:07, 383.19s/it]\u001b[A\n",
      "  6%|█▉                              | 153/2478 [10:19:45<210:25:59, 325.83s/it]\u001b[A\n",
      "  6%|█▉                              | 154/2478 [10:28:32<249:21:22, 386.27s/it]\u001b[A\n",
      "  6%|██                              | 155/2478 [10:29:10<181:46:16, 281.69s/it]\u001b[A\n",
      "  6%|██                              | 156/2478 [10:30:32<143:02:58, 221.78s/it]\u001b[A\n",
      "  6%|██                              | 157/2478 [10:31:13<107:59:10, 167.49s/it]\u001b[A\n",
      "  6%|██                              | 158/2478 [10:31:59<154:39:46, 239.99s/it]\u001b[A\n",
      "  0%|                                                 | 0/25 [10:31:59<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_to_ml, test_ready_to_ml \u001b[38;5;241m=\u001b[39m \u001b[43mrename_tokenized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst_party\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msecond_party\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 124\u001b[0m, in \u001b[0;36mrename_tokenized\u001b[0;34m(df_1, df_2, column_1, column_2, column_3)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m df_list:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m column_list:\n\u001b[0;32m--> 124\u001b[0m         df_berted \u001b[38;5;241m=\u001b[39m \u001b[43mauto_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df_berted, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    127\u001b[0m             column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_berted_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(df_berted\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n",
      "Cell \u001b[0;32mIn[15], line 104\u001b[0m, in \u001b[0;36mauto_tokenizer\u001b[0;34m(df, column_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m encoded_input \u001b[38;5;129;01min\u001b[39;00m encoded_input_list:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 104\u001b[0m         model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     sentence_embeddings \u001b[38;5;241m=\u001b[39m mean_pooling(model_output, encoded_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    107\u001b[0m     sentence_embeddings \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(sentence_embeddings, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1758\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1770\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1772\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    534\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    535\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 537\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/transformers/pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:549\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 549\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:449\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 449\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_to_ml, test_ready_to_ml = rename_tokenized(train, test, 'first_party', 'second_party', 'facts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_to_ml).to_csv(\"./train_correlations.csv\", index=False)\n",
    "pd.DataFrame(test_ready_to_ml).to_csv('./test_correlations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3639c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
