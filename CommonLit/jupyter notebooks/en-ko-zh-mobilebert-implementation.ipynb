{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import HTML\nhtmlCodeHIDE = \"\"\"\n<style>\n    .button0 {\n        background-color: #3c3b6e;\n        border: none;\n        color: white;\n        padding: 8px 22px;\n        text-align: center;\n        text-decoration: none;\n        display: inline-block;\n        font-size: 16px;\n        margin: 0;\n        cursor: pointer;\n    }\n</style>\n<script>\n    function toggleInput0() {\n        var i;\n        var textCellsToHide0 = [0];\n        for (i = 0; i < document.getElementsByClassName(\"input\").length; i++) {\n            var divTag = document.getElementsByClassName(\"input\")[0]\n            var displaySetting = divTag.style.display;\n            if (displaySetting == 'none') { \n                divTag.style.display = '';\n            }\n            else { \n                divTag.style.display = 'none';\n            }\n        }\n        for (i = 0; i < document.getElementsByClassName(\"text_cell\").length; i++) {\n            var divTag = document.getElementsByClassName(\"text_cell\")[i];\n            var displaySetting = divTag.style.display;\n            if (textCellsToHide0.includes(i)) {\n                if (displaySetting == 'none') {\n                    divTag.style.display = '';\n                } else {\n                    divTag.style.display = 'none';\n                }\n            }\n        }\n        \n    }\n    function setInitialDisplay0() {\n        var textCells0 = document.getElementsByClassName(\"text_cell\");\n        {\n            textCells0[0].style.display = 'none';\n        }\n    }\n</script>\n<button onclick=\"javascript:toggleInput0()\" class=\"button0\"> Click here for the explanation </button>\n\"\"\"\nHTML(htmlCodeHIDE)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-26T08:51:53.441175Z","iopub.execute_input":"2023-09-26T08:51:53.441638Z","iopub.status.idle":"2023-09-26T08:51:53.455875Z","shell.execute_reply.started":"2023-09-26T08:51:53.441599Z","shell.execute_reply":"2023-09-26T08:51:53.454356Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    .button0 {\n        background-color: #3c3b6e;\n        border: none;\n        color: white;\n        padding: 8px 22px;\n        text-align: center;\n        text-decoration: none;\n        display: inline-block;\n        font-size: 16px;\n        margin: 0;\n        cursor: pointer;\n    }\n</style>\n<script>\n    function toggleInput0() {\n        var i;\n        var textCellsToHide0 = [0];\n        for (i = 0; i < document.getElementsByClassName(\"input\").length; i++) {\n            var divTag = document.getElementsByClassName(\"input\")[0]\n            var displaySetting = divTag.style.display;\n            if (displaySetting == 'none') { \n                divTag.style.display = '';\n            }\n            else { \n                divTag.style.display = 'none';\n            }\n        }\n        for (i = 0; i < document.getElementsByClassName(\"text_cell\").length; i++) {\n            var divTag = document.getElementsByClassName(\"text_cell\")[i];\n            var displaySetting = divTag.style.display;\n            if (textCellsToHide0.includes(i)) {\n                if (displaySetting == 'none') {\n                    divTag.style.display = '';\n                } else {\n                    divTag.style.display = 'none';\n                }\n            }\n        }\n        \n    }\n    function setInitialDisplay0() {\n        var textCells0 = document.getElementsByClassName(\"text_cell\");\n        {\n            textCells0[0].style.display = 'none';\n        }\n    }\n</script>\n<button onclick=\"javascript:toggleInput0()\" class=\"button0\"> Click here for the explanation </button>\n"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import HTML\nhtmlCodeHide = \"\"\"\n<style>\n    .button1 {\n        background: #CD2E3A;\n        background: -moz-linear-gradient(-45deg, #CD2E3A 0%, #CD2E3A 50%, #0F64CD 51%, #0F64CD 100%);\n        background: -webkit-linear-gradient(-45deg, #CD2E3A 0%,#CD2E3A 50%,#0F64CD 51%,#0F64CD 100%);\n        background: linear-gradient(135deg, #CD2E3A 0%,#CD2E3A 50%,#0F64CD 51%,#0F64CD 100%);\n        filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#CD2E3A', endColorstr='#0F64CD',GradientType=1 );\n        border: none;\n        color: white;\n        padding: 8px 22px;\n        text-align: center;\n        text-decoration: none;\n        display: inline-block;\n        font-size: 16px;\n        margin: 0;\n        cursor: pointer;\n    }\n\n</style>\n<script>\n    function toggleInput1() {\n        var i;\n        var textCellsToHide1 = [1];\n        for (i = 0; i < document.getElementsByClassName(\"input\").length; i++) {\n            var divTag = document.getElementsByClassName(\"input\")[1]\n            var displaySetting = divTag.style.display;\n            if (displaySetting == 'none') { \n                divTag.style.display = '';\n            }\n            else { \n                divTag.style.display = 'none';\n            }\n        }\n        for (i = 0; i < document.getElementsByClassName(\"text_cell\").length; i++) {\n            var divTag = document.getElementsByClassName(\"text_cell\")[i];\n            var displaySetting = divTag.style.display;\n            if (textCellsToHide1.includes(i)) {\n                if (displaySetting == 'none') {\n                    divTag.style.display = '';\n                } else {\n                    divTag.style.display = 'none';\n                }\n            }\n        }\n    }\n    function setInitialDisplay1() {\n        var textCells1 = document.getElementsByClassName(\"text_cell\");\n        {\n            textCells1[1].style.display = 'none';\n        }\n    }    \n</script>\n<button onclick=\"javascript:toggleInput1()\" class=\"button1\">한국어로 설명을 읽고 싶으시다면 여기를 눌러주세요</button>\n\"\"\"\nHTML(htmlCodeHide)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-26T08:51:53.458888Z","iopub.execute_input":"2023-09-26T08:51:53.459845Z","iopub.status.idle":"2023-09-26T08:51:53.480488Z","shell.execute_reply.started":"2023-09-26T08:51:53.459794Z","shell.execute_reply":"2023-09-26T08:51:53.478308Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    .button1 {\n        background: #CD2E3A;\n        background: -moz-linear-gradient(-45deg, #CD2E3A 0%, #CD2E3A 50%, #0F64CD 51%, #0F64CD 100%);\n        background: -webkit-linear-gradient(-45deg, #CD2E3A 0%,#CD2E3A 50%,#0F64CD 51%,#0F64CD 100%);\n        background: linear-gradient(135deg, #CD2E3A 0%,#CD2E3A 50%,#0F64CD 51%,#0F64CD 100%);\n        filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#CD2E3A', endColorstr='#0F64CD',GradientType=1 );\n        border: none;\n        color: white;\n        padding: 8px 22px;\n        text-align: center;\n        text-decoration: none;\n        display: inline-block;\n        font-size: 16px;\n        margin: 0;\n        cursor: pointer;\n    }\n\n</style>\n<script>\n    function toggleInput1() {\n        var i;\n        var textCellsToHide1 = [1];\n        for (i = 0; i < document.getElementsByClassName(\"input\").length; i++) {\n            var divTag = document.getElementsByClassName(\"input\")[1]\n            var displaySetting = divTag.style.display;\n            if (displaySetting == 'none') { \n                divTag.style.display = '';\n            }\n            else { \n                divTag.style.display = 'none';\n            }\n        }\n        for (i = 0; i < document.getElementsByClassName(\"text_cell\").length; i++) {\n            var divTag = document.getElementsByClassName(\"text_cell\")[i];\n            var displaySetting = divTag.style.display;\n            if (textCellsToHide1.includes(i)) {\n                if (displaySetting == 'none') {\n                    divTag.style.display = '';\n                } else {\n                    divTag.style.display = 'none';\n                }\n            }\n        }\n    }\n    function setInitialDisplay1() {\n        var textCells1 = document.getElementsByClassName(\"text_cell\");\n        {\n            textCells1[1].style.display = 'none';\n        }\n    }    \n</script>\n<button onclick=\"javascript:toggleInput1()\" class=\"button1\">한국어로 설명을 읽고 싶으시다면 여기를 눌러주세요</button>\n"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import HTML\nhtmlCodeHIDE = \"\"\"\n<style>\n    .button2 {\n        background-color: #EE1C25;\n        border: none;\n        color: white;\n        padding: 8px 22px;\n        text-align: center;\n        text-decoration: none;\n        display: inline-block;\n        font-size: 16px;\n        margin: 0;\n        cursor: pointer;\n    }\n</style>\n<script>\n    function toggleInput2() {\n        var i;\n        var textCellsToHide2 = [2];\n        for (i = 0; i < document.getElementsByClassName(\"input\").length; i++) {\n            var divTag = document.getElementsByClassName(\"input\")[2]\n            var displaySetting = divTag.style.display;\n            if (displaySetting == 'none') { \n                divTag.style.display = '';\n            }\n            else { \n                divTag.style.display = 'none';\n            }\n        }\n        for (i = 0; i < document.getElementsByClassName(\"text_cell\").length; i++) {\n            var divTag = document.getElementsByClassName(\"text_cell\")[i];\n            var displaySetting = divTag.style.display;\n            if (textCellsToHide2.includes(i)) {\n                if (displaySetting == 'none') {\n                    divTag.style.display = '';\n                } else {\n                    divTag.style.display = 'none';\n                }\n            }\n        }\n        \n    }\n    function setInitialDisplay2() {\n        var textCells2 = document.getElementsByClassName(\"text_cell\");\n        {\n            textCells2[2].style.display = 'none';\n        }\n    }\n    window.onload = () => {\n    setInitialDisplay0();\n    setInitialDisplay1();\n    setInitialDisplay2();\n    }\n</script>\n<button onclick=\"javascript:toggleInput2()\" class=\"button2\"> 请按这里看中文解释 </button>\n\"\"\"\nHTML(htmlCodeHIDE)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-26T08:51:53.482867Z","iopub.execute_input":"2023-09-26T08:51:53.483774Z","iopub.status.idle":"2023-09-26T08:51:53.500686Z","shell.execute_reply.started":"2023-09-26T08:51:53.483725Z","shell.execute_reply":"2023-09-26T08:51:53.499848Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    .button2 {\n        background-color: #EE1C25;\n        border: none;\n        color: white;\n        padding: 8px 22px;\n        text-align: center;\n        text-decoration: none;\n        display: inline-block;\n        font-size: 16px;\n        margin: 0;\n        cursor: pointer;\n    }\n</style>\n<script>\n    function toggleInput2() {\n        var i;\n        var textCellsToHide2 = [2];\n        for (i = 0; i < document.getElementsByClassName(\"input\").length; i++) {\n            var divTag = document.getElementsByClassName(\"input\")[2]\n            var displaySetting = divTag.style.display;\n            if (displaySetting == 'none') { \n                divTag.style.display = '';\n            }\n            else { \n                divTag.style.display = 'none';\n            }\n        }\n        for (i = 0; i < document.getElementsByClassName(\"text_cell\").length; i++) {\n            var divTag = document.getElementsByClassName(\"text_cell\")[i];\n            var displaySetting = divTag.style.display;\n            if (textCellsToHide2.includes(i)) {\n                if (displaySetting == 'none') {\n                    divTag.style.display = '';\n                } else {\n                    divTag.style.display = 'none';\n                }\n            }\n        }\n        \n    }\n    function setInitialDisplay2() {\n        var textCells2 = document.getElementsByClassName(\"text_cell\");\n        {\n            textCells2[2].style.display = 'none';\n        }\n    }\n    window.onload = () => {\n    setInitialDisplay0();\n    setInitialDisplay1();\n    setInitialDisplay2();\n    }\n</script>\n<button onclick=\"javascript:toggleInput2()\" class=\"button2\"> 请按这里看中文解释 </button>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Intro\nI would say this is the first notebook I am trying hard with post-EDA process or post-preprocessing.<br>\nI may suffer, but it will only make me stronger.<br>\n\nIf you are interested, please check out my [SpellCheck Comparison notebook](https://www.kaggle.com/code/jasonheesanglee/nah-nevermind-eda-spellcheck-comparison) which became a basis for the preprocessing part of this notebook.<br>\n<br>\n## I have implemented MobileBERT.\nAs the purpose of this competition is to enhance the teaching and scoring experience of the teachers, I believe making this model light-weighted is the key.<br>\nBy implementing MobieBERT, the model would run on individual mobile devices, teachers at isolated locations with poor or no internet connection could also use this.<br>\nIt would not need simultaneous connection to the internet.<br><br>\n\nThe children in such area shouldn't be neglected.<br>\n\n### However\nit seems like MobileBERT is not the correct model for this task..<br>\nThe LB score using this model was around 5000 ~ 6000.<br>\n\n### OR MAYBE\nI haven't applied the model correctly.<br>\nIt was weird to see such scores.<br>\nAs expected the way I implemented MobileBERT seemed wrong based on the [research paper](https://arxiv.org/pdf/2004.02984.pdf).<br>\nI had to teach the teacher model IB-BERT, then apply knowledge transfer to MobileBERT.<br>\nI will research more and apply the technique later on.<br>\n<br>\n\n## Separating DataFrames\nAlso, another main update of this notebook is that I have separated the scoring for content and wording.<br>\nMetric is basically the same, but content score is computed with misspell checked DataFrame, and wording score is computed with the original one (with some preprocessing).<br>\n\n#### Sep 19 update: I am working on Google Colab to see if this works.\n#### Still the same day, it worked on Colab, so I tried scoring\nThis worked but the score was exceeding 1000. (Which still is an improvement from my last attempt with MobileBERT.<br>\nI probably have done something wrong, or... maybe this is just not the right model.<br>\n<br>\nWait... I got Public Score 52771.88 for Version 143...<br>\nSomething is definitely wrong here...<br>","metadata":{}},{"cell_type":"markdown","source":"## 서론\n전처리 단계 이후 모델링 단계에서 단순히 Baseline을 따라쳐보며 파라미터 조율을 해보는 것이 아닌, 다른 분들이 사용해보지 않은 모델을 사용하려는 시도를 처음 해보는 중입니다.<br>\n아직은 미숙한 실력이기에 제대로 작동하지 않을 수도 있고, 제대로 작동한다 하여도 점수가 좋지 않을 수도 있습니다.<br>\n모델링 연습 겸 시도 겸 작성해보는 노트북으로 생각해 주시면 감사하겠습니다!<br>\n\n제가 어떻게 symspellpy라는 오타처리 툴을 메인 툴로 선택하게 되었는지 궁금하시다면 이 [노트북](https://www.kaggle.com/code/jasonheesanglee/nah-nevermind-eda-spellcheck-comparison) 참고하여 주시면 감사하겠습니다!<br>\n<br>\n## MobileBERT 구현\n이 대회의 목적은, 사람이 직접하고 있는 Summarization scoring을 AI scoring으로 대체하여 채점자 혹은 선생님들의 직업 경험을 증진시키는 것에 있다고 파악하였습니다.<br>\n따라서, 가벼운 모델을 활용하여 모바일에서도 간편하고 빠르게 돌아가는 것에 중점을 두어야 한다고 생각하였습니다.<br>\n`MobileBERT`를 활용하게 되면, 모바일 기기 자체에서 모델을 돌릴 수 있게되고, 그렇게 되면 인터넷이 잘 구축되어 있지 않은 환경에서도 무리없이 해당 서비스를 활용 할 수 있게 될 것입니다.<br>그러한 환경에서 공부하는 학생들도 타 지역의 학생들과 동일한 (혹은 유사한) 교육 서비스를 받는 것에 초점을 두고 이 시도를 진행해 보게 되었습니다.<br>\n<br>\n\n초기엔 [@bulivington](https://www.kaggle.com/bulivington)님의 [Baseline코드](https://www.kaggle.com/code/bulivington/transformers-predictions-base)로 시작을 하였고, 그 위에 `MobileBERT`를 그대로 얹어보려고 하였습니다.<br>그렇게 하여 구동은 되었으나, 6000점 대의 LB점수가 나오고 충격을 받아 이 모델은 아닌가 싶어 잠시 방치를 해 두었습니다..<br>\n<br>\n하지만, 이대로 끝내기엔 아쉬워 `MobileBERT` 논문을 찾아보게 되었고, 해당 모델은 다른 모델들과 달리, `IB-BERT`라는 Teacher 모델을 사용하여 선제적으로 학습을 시킨 후, 다시 `MobileBERT`를 학습시키는 방식으로 구현이 된다는 것을 알게 되었습니다.<br>\n<br>\n해당 방식으로 구현을 마쳐 둔 상태이며 LB점수가 많이 좋아지긴 하였으나 아직 1000점 대에 머물러 있습니다.<br>\n<br>\n기존에 Teacher 모델으로 사용한 것은 `bert-base-uncased`이었기에 본 대회에서 많은 분들이 사용하고 계신 `DeBERTa-v3-base`로 변경하여 현재 점수 산출을 기다리고 있습니다.<br>\n(제발 1000점보다 나은 점수가 나오면 좋겠습니다...)<br>\n<br>\n궁금하신 점이 있으시거나, 조언해 주시고 싶은 사항이 있으시다면 편히 말씀 부탁드리겠습니다!<br>\n감사합니다.","metadata":{}},{"cell_type":"markdown","source":"## Intro\n这是我在EDA流程之后努力尝试的第一次Notebook。<br>\n在此之前，我一直在专注于EDA和前期流程。<br>\n因此我还不太增长，所以这个Notebook可能不会成功，即使成功了，也可能得分不高。<br>\n请算这个Notebook当做一个建模练习的Notebook。<br>\n\n如果您感兴趣的话，请查看我的 [SpellCheck Tool Comparison Notebook](https://www.kaggle.com/code/jasonheesanglee/nah-nevermind-eda-spellcheck-comparison)，它是一个Notebook前期流程部分的基础。<br><br>\n\n## MobileBERT\n由于本次比赛的不敌视提高教师的教学和评分经验，我认为使这个一模式将量化是关键。<br>\n通过这个MobileBERT，该模型将在个人移动设备上运行，网路链接不畅或没有网路链接的偏远地区的教师也可以使用该模式。<br>\n它不需要同时链接互联网。<br>\n\n我相信这些地区的学生不应被忽视。<br>\n\n**但是**<br>\n看起来对这个比赛，MobileBERT不是个很好的模型。<br>\n使用该模型的LB分数约为5000～6000多<br>\n\n\n**可能**<br>\n我没学好这个模型。<br>\n很奇怪的道5000多分。<br>\n怪不得我实施MobileBERT 的方法似乎是错误的。<br>\n根据研究论文，我得先教教师模型（IB-BERT）然后让它教学生模型（Mobile BERT）\n我多多学好这个模型然后在实施。<br>\n\n**分开DataFrame**<br>\n此外，本Notebook的另外一个主要更新是，我将Content和Wording评分分开了。<br>\n衡量标准基本相同，但Content得分是用经过拼写错误检查的 DataFrame 计算的，而Wording得分是用原始 DataFrame（经过一些预处理）计算的。<br>","metadata":{}},{"cell_type":"markdown","source":"# Base Configuration","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\")\n\nimport os\nos.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport logging\nlogging.disable(logging.ERROR)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-26T08:51:53.503473Z","iopub.execute_input":"2023-09-26T08:51:53.503696Z","iopub.status.idle":"2023-09-26T08:51:53.516836Z","shell.execute_reply.started":"2023-09-26T08:51:53.503667Z","shell.execute_reply":"2023-09-26T08:51:53.515692Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Installing Spell Checking modules\nInstalling pyspellchecker from [@dmitriygakh](https://www.kaggle.com/dmitriygakh)'s [dataset](https://www.kaggle.com/datasets/dmitriygakh/nlp-set) and symspellpy from my [dataset](https://www.kaggle.com/datasets/jasonheesanglee/symspell-677).","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/nlp-set/nlp_set/whl/pyspellchecker-0.7.2-py3-none-any.whl\n!pip install /kaggle/input/symspell-677/editdistpy-0.1.3-cp310-cp310-linux_x86_64.whl\n!pip install /kaggle/input/symspell-677/symspellpy-6.7.7-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:51:53.521343Z","iopub.execute_input":"2023-09-26T08:51:53.522459Z","iopub.status.idle":"2023-09-26T08:53:28.573672Z","shell.execute_reply.started":"2023-09-26T08:51:53.522423Z","shell.execute_reply":"2023-09-26T08:53:28.572565Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/nlp-set/nlp_set/whl/pyspellchecker-0.7.2-py3-none-any.whl\npyspellchecker is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nProcessing /kaggle/input/symspell-677/editdistpy-0.1.3-cp310-cp310-linux_x86_64.whl\neditdistpy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nProcessing /kaggle/input/symspell-677/symspellpy-6.7.7-py3-none-any.whl\nRequirement already satisfied: editdistpy>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from symspellpy==6.7.7) (0.1.3)\nsymspellpy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"***Checking GPU connection.***","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:28.575972Z","iopub.execute_input":"2023-09-26T08:53:28.576279Z","iopub.status.idle":"2023-09-26T08:53:29.736233Z","shell.execute_reply.started":"2023-09-26T08:53:28.576236Z","shell.execute_reply":"2023-09-26T08:53:29.735162Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Tue Sep 26 08:53:29 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   40C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   40C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"from datasets import load_metric, disable_progress_bar\nfrom datasets import Dataset,load_dataset, load_from_disk\ndisable_progress_bar()\n\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport itertools\nfrom itertools import islice\nimport pkg_resources\nimport unidecode\nfrom spellchecker import SpellChecker\nfrom symspellpy import SymSpell, Verbosity\nfrom stop_words import get_stop_words\nimport json\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport pkg_resources\nimport scipy as sp\nimport shutil\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom spellchecker import SpellChecker\nfrom stop_words import get_stop_words\nfrom symspellpy import SymSpell, Verbosity\nimport torch\nimport transformers\nimport tensorflow as tf\nfrom transformers import pipeline\nfrom transformers import T5Tokenizer, \nfrom transformers import MobileBertTokenizer, MobileBertModel\nfrom transformers import DataCollatorWithPadding\nfrom transformers import TrainingArguments, Trainer\n\nfrom transformers import MobileBertForSequenceClassification\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom tqdm.auto import tqdm\nimport unidecode\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:29.738213Z","iopub.execute_input":"2023-09-26T08:53:29.738464Z","iopub.status.idle":"2023-09-26T08:53:49.798420Z","shell.execute_reply.started":"2023-09-26T08:53:29.738433Z","shell.execute_reply":"2023-09-26T08:53:49.797413Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n[nltk_data]     Temporary failure in name resolution>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Scoring Metrics & NLP Base","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    rmse = mean_squared_error(labels, predictions, squared=False)\n    return {\"rmse\": rmse}\n\n\ndef compute_mcrmse(eval_pred):\n    \"\"\"\n    Calculates mean columnwise root mean squared error\n    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n    \"\"\"\n    preds, labels = eval_pred\n\n    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n    mcrmse = np.mean(col_rmse)\n\n    return {\n        \"content_rmse\": col_rmse[0],\n        \"wording_rmse\": col_rmse[1],\n        \"mcrmse\": mcrmse,\n    }\n\n\n\ndef compt_score(content_true, content_pred, wording_true, wording_pred):\n    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n    return (content_score + wording_score)/2\n\ndef seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nstop_words = stopwords.words('english')\n_stop_words = list(get_stop_words('en'))\nstop_words.extend(_stop_words)\n\n\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:49.802250Z","iopub.execute_input":"2023-09-26T08:53:49.802496Z","iopub.status.idle":"2023-09-26T08:53:49.817592Z","shell.execute_reply.started":"2023-09-26T08:53:49.802463Z","shell.execute_reply":"2023-09-26T08:53:49.816587Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# NLP Models\nAs I am a very lazy person, I hated moving back and forth between the notebooks for each model.<br>\nSo I decided to add all models and change models here.","metadata":{}},{"cell_type":"code","source":"files = ['berttiny', # files[0]\n         'commonlit-evaluate-student-summaries', # files[1]\n         'deberta-v3-large', # files[2]\n         'debertav3base', # files[3]\n         'debertav3small', # files[4]\n         'electrabasediscriminator', # files[5]\n         'roberta-base', # files[6]\n         'mobilebert-uncased', # files[7]\n         'transformers/t5-large', # files[8]\n         'transformers/t5-base' # files[9]\n         ]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:49.819136Z","iopub.execute_input":"2023-09-26T08:53:49.819546Z","iopub.status.idle":"2023-09-26T08:53:49.833674Z","shell.execute_reply.started":"2023-09-26T08:53:49.819514Z","shell.execute_reply":"2023-09-26T08:53:49.832757Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"markdown","source":"model_name is selected from above model names.","metadata":{}},{"cell_type":"markdown","source":"model_name의 경우, 상단의 files에서 선택해 사용합니다.","metadata":{}},{"cell_type":"code","source":"IS_DEBUG = False\n\nclass CFG:\n    model_name=f\"/kaggle/input/{files[2]}\"\n    learning_rate=1.5e-5\n    weight_decay=0.02\n#     warmup_ratio=0.02\n    hidden_dropout_prob=0.02\n    attention_probs_dropout_prob=0.02\n    num_train_epochs= 1 if IS_DEBUG else 4\n    n_splits = 2 if IS_DEBUG else 2\n    batch_size = 1 if IS_DEBUG else 4\n    random_seed = 42\n    save_steps = 2000 if IS_DEBUG else 100\n    max_length = 5 if IS_DEBUG else 384","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:49.835372Z","iopub.execute_input":"2023-09-26T08:53:49.835732Z","iopub.status.idle":"2023-09-26T08:53:49.844458Z","shell.execute_reply.started":"2023-09-26T08:53:49.835682Z","shell.execute_reply":"2023-09-26T08:53:49.843366Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Pre-preprocessing\nSimply merging separated dataset","metadata":{}},{"cell_type":"code","source":"prompts_train = pd.read_csv(f\"/kaggle/input/{files[1]}/prompts_train.csv\")\nprompts_test = pd.read_csv(f\"/kaggle/input/{files[1]}/prompts_test.csv\")\n\nsummaries_train = pd.read_csv(f\"/kaggle/input/{files[1]}/summaries_train.csv\")\nsummaries_test = pd.read_csv(f\"/kaggle/input/{files[1]}/summaries_test.csv\")\n\nsample_submission = pd.read_csv(f\"/kaggle/input/{files[1]}/sample_submission.csv\")\n\ndisplay(prompts_train)\ndisplay(summaries_train)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-26T08:53:49.845970Z","iopub.execute_input":"2023-09-26T08:53:49.846260Z","iopub.status.idle":"2023-09-26T08:53:49.940529Z","shell.execute_reply.started":"2023-09-26T08:53:49.846225Z","shell.execute_reply":"2023-09-26T08:53:49.939695Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"  prompt_id                                    prompt_question  \\\n0    39c16e  Summarize at least 3 elements of an ideal trag...   \n1    3b9047  In complete sentences, summarize the structure...   \n2    814d6b  Summarize how the Third Wave developed over su...   \n3    ebad26  Summarize the various ways the factory would u...   \n\n                prompt_title  \\\n0                 On Tragedy   \n1  Egyptian Social Structure   \n2             The Third Wave   \n3    Excerpt from The Jungle   \n\n                                         prompt_text  \n0  Chapter 13 \\r\\nAs the sequel to what has alrea...  \n1  Egyptian society was structured like a pyramid...  \n2  Background \\r\\nThe Third Wave experiment took ...  \n3  With one member trimming beef in a cannery, an...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt_id</th>\n      <th>prompt_question</th>\n      <th>prompt_title</th>\n      <th>prompt_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39c16e</td>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3b9047</td>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>Egyptian society was structured like a pyramid...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>814d6b</td>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>The Third Wave</td>\n      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ebad26</td>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>Excerpt from The Jungle</td>\n      <td>With one member trimming beef in a cannery, an...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"        student_id prompt_id  \\\n0     000e8c3c7ddb    814d6b   \n1     0020ae56ffbf    ebad26   \n2     004e978e639e    3b9047   \n3     005ab0199905    3b9047   \n4     0070c9e7af47    814d6b   \n...            ...       ...   \n7160  ff7c7e70df07    ebad26   \n7161  ffc34d056498    3b9047   \n7162  ffd1576d2e1b    3b9047   \n7163  ffe4a98093b2    39c16e   \n7164  fffbccfd8a08    ebad26   \n\n                                                   text   content   wording  \n0     The third wave was an experimentto see how peo...  0.205683  0.380538  \n1     They would rub it up with soda to make the sme... -0.548304  0.506755  \n2     In Egypt, there were many occupations and soci...  3.128928  4.231226  \n3     The highest class was Pharaohs these people we... -0.210614 -0.471415  \n4     The Third Wave developed  rapidly because the ...  3.272894  3.219757  \n...                                                 ...       ...       ...  \n7160  They used all sorts of chemical concoctions to...  0.205683  0.380538  \n7161  The lowest classes are slaves and farmers slav... -0.308448  0.048171  \n7162             they sorta made people start workin... -1.408180 -0.493603  \n7163  An ideal tragety has three elements that make ... -0.393310  0.627128  \n7164  The meat would smell sour but the would \"rub i...  1.771596  0.547742  \n\n[7165 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>content</th>\n      <th>wording</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000e8c3c7ddb</td>\n      <td>814d6b</td>\n      <td>The third wave was an experimentto see how peo...</td>\n      <td>0.205683</td>\n      <td>0.380538</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0020ae56ffbf</td>\n      <td>ebad26</td>\n      <td>They would rub it up with soda to make the sme...</td>\n      <td>-0.548304</td>\n      <td>0.506755</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>004e978e639e</td>\n      <td>3b9047</td>\n      <td>In Egypt, there were many occupations and soci...</td>\n      <td>3.128928</td>\n      <td>4.231226</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>005ab0199905</td>\n      <td>3b9047</td>\n      <td>The highest class was Pharaohs these people we...</td>\n      <td>-0.210614</td>\n      <td>-0.471415</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0070c9e7af47</td>\n      <td>814d6b</td>\n      <td>The Third Wave developed  rapidly because the ...</td>\n      <td>3.272894</td>\n      <td>3.219757</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7160</th>\n      <td>ff7c7e70df07</td>\n      <td>ebad26</td>\n      <td>They used all sorts of chemical concoctions to...</td>\n      <td>0.205683</td>\n      <td>0.380538</td>\n    </tr>\n    <tr>\n      <th>7161</th>\n      <td>ffc34d056498</td>\n      <td>3b9047</td>\n      <td>The lowest classes are slaves and farmers slav...</td>\n      <td>-0.308448</td>\n      <td>0.048171</td>\n    </tr>\n    <tr>\n      <th>7162</th>\n      <td>ffd1576d2e1b</td>\n      <td>3b9047</td>\n      <td>they sorta made people start workin...</td>\n      <td>-1.408180</td>\n      <td>-0.493603</td>\n    </tr>\n    <tr>\n      <th>7163</th>\n      <td>ffe4a98093b2</td>\n      <td>39c16e</td>\n      <td>An ideal tragety has three elements that make ...</td>\n      <td>-0.393310</td>\n      <td>0.627128</td>\n    </tr>\n    <tr>\n      <th>7164</th>\n      <td>fffbccfd8a08</td>\n      <td>ebad26</td>\n      <td>The meat would smell sour but the would \"rub i...</td>\n      <td>1.771596</td>\n      <td>0.547742</td>\n    </tr>\n  </tbody>\n</table>\n<p>7165 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"prompts_train['prompt_text'] = prompts_train['prompt_text'].str.replace('\\r',' ').str.replace('\\n', ' ')\nprompts_test['prompt_text'] = prompts_test['prompt_text'].str.replace('\\r',' ').str.replace('\\n', ' ')","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:49.942128Z","iopub.execute_input":"2023-09-26T08:53:49.942455Z","iopub.status.idle":"2023-09-26T08:53:49.950239Z","shell.execute_reply.started":"2023-09-26T08:53:49.942423Z","shell.execute_reply":"2023-09-26T08:53:49.948975Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train = pd.merge(prompts_train, summaries_train, how=\"left\", on=\"prompt_id\")\ntest = pd.merge(prompts_test, summaries_test, how=\"left\", on=\"prompt_id\")","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:49.951888Z","iopub.execute_input":"2023-09-26T08:53:49.952137Z","iopub.status.idle":"2023-09-26T08:53:49.970784Z","shell.execute_reply.started":"2023-09-26T08:53:49.952107Z","shell.execute_reply":"2023-09-26T08:53:49.969884Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing configuration","metadata":{}},{"cell_type":"markdown","source":"I have implemented all possible combinations of preprocessing methods and can be altered by simple yes/no modification.","metadata":{}},{"cell_type":"markdown","source":"전처리 자동화(라고 부르는 수작업으로 구성된 무언가)<br>\n단순한 yes / no 스위치로 적용되는 전처리 기법을 적용합니다.<br>","metadata":{}},{"cell_type":"code","source":"method = ['py_and_sym', 'pyspell_only', 'symspell_only']\n\nfreq_dict_list = [\"/kaggle/input/symspell-677/symspell_freq_dict.txt\",\n            \"/kaggle/input/symspell-677/frequency_dictionary_en_82_765.txt\",\n            \"/kaggle/input/symspell-677/frequency_bigramdictionary_en_243_342.txt\"]\n\nyes = True\nno = False\n\npreprocess = no ####### FIX AS YES #######\nbasic_preprocess = no\nnt_preprocess = no\nstop_words_switch = no ####### FIX AS NO #######\n\nmanage_misspelled_words = no\nmisspelled_word_method = method[0]\n\nkeyword_masking = no\nfreq_word_masking = no","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:49.972146Z","iopub.execute_input":"2023-09-26T08:53:49.972483Z","iopub.status.idle":"2023-09-26T08:53:49.987798Z","shell.execute_reply.started":"2023-09-26T08:53:49.972448Z","shell.execute_reply":"2023-09-26T08:53:49.986451Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"if (manage_misspelled_words==yes) & (misspelled_word_method == 'py_and_sym'):\n    pyspell_detector = yes\n    symspell_corrector = yes\n    misspell_counter = 0\n\nif (manage_misspelled_words==yes) & (misspelled_word_method == 'pyspell_only'):\n    pyspell_detector = yes\n    symspell_corrector = no\n    misspell_counter = 1\n    \nif (manage_misspelled_words==yes) & (misspelled_word_method == 'symspell_only'):\n    pyspell_detector = no\n    symspell_corrector = yes\n    freq_dict = freq_dict_list[0]\n    misspell_counter = 2\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:49.989316Z","iopub.execute_input":"2023-09-26T08:53:49.989864Z","iopub.status.idle":"2023-09-26T08:53:49.999037Z","shell.execute_reply.started":"2023-09-26T08:53:49.989826Z","shell.execute_reply":"2023-09-26T08:53:49.998019Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# Text Processor","metadata":{}},{"cell_type":"markdown","source":"This module came from my previous competition in DACON (Kaggle-like platform in Korea).<br>\nI have modified the module a lot here (adding nt_preprocess, stopwords)<br>\nMore modifications are still needed.","metadata":{}},{"cell_type":"markdown","source":"제가 데이콘에서 활용했던 definition을 가져와서 더 구체화 시켰습니다.","metadata":{}},{"cell_type":"code","source":"def text_processor_2(s, stop_words, nt_preprocess):\n    \"\"\"\n    문장을 담고있는 variable을 넣어주면\n    알파벳을 제외한 문장의 모든 기호, 숫자를 제거합니다.\n\n    :param s: 문장을 담고있는 variable\n    :return: 새로운 DataFrame안에 담긴 text_processor가 적용된 column\n    \"\"\"\n    if basic_preprocess:\n        pattern = r'\\([^)]*\\)'  # ()\n        s = re.sub(pattern=pattern, repl='', string=s)\n        pattern = r'\\[[^)]*\\]'  # []\n        s = re.sub(pattern=pattern, repl='', string=s)\n        pattern = r'\\<[^)]*\\>'  # <>\n        s = re.sub(pattern=pattern, repl='', string=s)\n        pattern = r'\\{[^)]*\\}'  # {}\n        s = re.sub(pattern=pattern, repl='', string=s)\n        \n#         pattern = r'[^a-zA-Z0-9]'\n#         s = re.sub(pattern=pattern, repl=' ', string=s)\n        s = s.replace(\"'\", ' ')\n#         s = s.replace(',', ' ')\n    \n        if nt_preprocess:\n            s_split = s.split()\n\n            not_list0 = ['isnt', 'arent', 'wasnt','werent',\n                         'didnt', 'dont', 'doesnt',\n                         'hasnt', 'havent', 'hadnt',\n                         'neednt', 'darent',\n                         'oughtnt', 'mustnt',\n                         'wouldnt', 'couldnt', 'wouldnt', 'shouldnt']\n            not_list1 = ['cant', 'wont']\n\n            s_list = []\n            for word in s_split:\n                if len(word) != 1:\n                    if word in not_list0:\n                        s_list.append(word[:-2])\n                        s_list.append('not')\n                    elif word == not_list1[0]:\n                        s_list.append('cannot')\n                    elif word == not_list1[1]:\n                        s_list.append('will')\n                        s_list.append('not')\n                    else:\n                        s_list.append(word)\n\n            s = \" \".join(s_list)\n        \n    if nt_preprocess and not basic_preprocess:\n        s_split = s.split()\n        \n        not_list0 = [\"isn't\", \"aren't\", \"wasn't\",\"weren't\",\n                     \"didn't\", \"don't\", \"doesn't\",\n                     \"hasn't\", \"haven't\", \"hadn't\",\n                     \"needn't\", \"daren't\",\n                     \"oughtn't\", \"mustn't\",\n                     \"wouldn't\", \"couldn't\", \"wouldn't\", \"shouldn't\"]\n        not_list1 = [\"can't\", \"won't\"]\n\n        s_list = []\n        for word in s_split:\n            if len(word) != 1:\n                if word in not_list0:\n                    s_list.append(word[:-3])\n                    s_list.append('not')\n                elif word == not_list1[0]:\n                    s_list.append('cannot')\n                elif word == not_list1[1]:\n                    s_list.append('will')\n                    s_list.append('not')\n                else:\n                    s_list.append(word)\n\n        s = \" \".join(s_list)\n        \n    if stop_words_switch:\n        s_split = s.split()\n        \n        s_list = []\n        for word in s_split:\n            if word in stop_words:\n                continue\n            else:\n                s_list.append(word)\n        \n        s = \" \".join(s_list)\n        \n    return s\n\ndef preprocessing(df, column, stop_words, deep_preprocess):\n    df[column] = df[column].map(unidecode.unidecode)\n    df[column] = df[column].str.lower()\n    df[column] = df[column].str.replace('\\r', ' ').str.replace('\\n', ' ')\n    if column == 'text':\n        temp = []\n        for row_num in range(df.shape[0]):\n            text_row = text_processor_2(df[column][row_num], stop_words, nt_preprocess)\n            temp.append(text_row)\n        df[column] = pd.Series(temp)\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.000283Z","iopub.execute_input":"2023-09-26T08:53:50.000534Z","iopub.status.idle":"2023-09-26T08:53:50.022749Z","shell.execute_reply.started":"2023-09-26T08:53:50.000501Z","shell.execute_reply":"2023-09-26T08:53:50.021437Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"if preprocess:\n    train = preprocessing(train, 'text', stop_words, nt_preprocess)\n    test = preprocessing(test, 'text', stop_words, nt_preprocess)\nprint('test.head(5) :\\n')\ndisplay(train.head(5))\nprint('test.head(5) :\\n')\ndisplay(test.head(5))","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.025466Z","iopub.execute_input":"2023-09-26T08:53:50.025743Z","iopub.status.idle":"2023-09-26T08:53:50.056219Z","shell.execute_reply.started":"2023-09-26T08:53:50.025689Z","shell.execute_reply":"2023-09-26T08:53:50.055277Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"test.head(5) :\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  prompt_id                                    prompt_question prompt_title  \\\n0    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n1    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n2    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n3    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n4    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n\n                                         prompt_text    student_id  \\\n0  Chapter 13   As the sequel to what has already...  00791789cc1f   \n1  Chapter 13   As the sequel to what has already...  0086ef22de8f   \n2  Chapter 13   As the sequel to what has already...  0094589c7a22   \n3  Chapter 13   As the sequel to what has already...  00cd5736026a   \n4  Chapter 13   As the sequel to what has already...  00d98b8ff756   \n\n                                                text   content   wording  \n0  1 element of an ideal tragedy is that it shoul... -0.210614 -0.471415  \n1  The three elements of an ideal tragedy are:  H... -0.970237 -0.417058  \n2  Aristotle states that an ideal tragedy should ... -0.387791 -0.584181  \n3  One element of an Ideal tragedy is having a co...  0.088882 -0.594710  \n4  The 3 ideal of tragedy is how complex you need... -0.687288 -0.460886  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt_id</th>\n      <th>prompt_question</th>\n      <th>prompt_title</th>\n      <th>prompt_text</th>\n      <th>student_id</th>\n      <th>text</th>\n      <th>content</th>\n      <th>wording</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39c16e</td>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>Chapter 13   As the sequel to what has already...</td>\n      <td>00791789cc1f</td>\n      <td>1 element of an ideal tragedy is that it shoul...</td>\n      <td>-0.210614</td>\n      <td>-0.471415</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39c16e</td>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>Chapter 13   As the sequel to what has already...</td>\n      <td>0086ef22de8f</td>\n      <td>The three elements of an ideal tragedy are:  H...</td>\n      <td>-0.970237</td>\n      <td>-0.417058</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>39c16e</td>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>Chapter 13   As the sequel to what has already...</td>\n      <td>0094589c7a22</td>\n      <td>Aristotle states that an ideal tragedy should ...</td>\n      <td>-0.387791</td>\n      <td>-0.584181</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>39c16e</td>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>Chapter 13   As the sequel to what has already...</td>\n      <td>00cd5736026a</td>\n      <td>One element of an Ideal tragedy is having a co...</td>\n      <td>0.088882</td>\n      <td>-0.594710</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>39c16e</td>\n      <td>Summarize at least 3 elements of an ideal trag...</td>\n      <td>On Tragedy</td>\n      <td>Chapter 13   As the sequel to what has already...</td>\n      <td>00d98b8ff756</td>\n      <td>The 3 ideal of tragedy is how complex you need...</td>\n      <td>-0.687288</td>\n      <td>-0.460886</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"test.head(5) :\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  prompt_id prompt_question     prompt_title      prompt_text    student_id  \\\n0    abc123    Summarize...  Example Title 1  Heading Text...  000000ffffff   \n1    abc123    Summarize...  Example Title 1  Heading Text...  222222cccccc   \n2    def789    Summarize...  Example Title 2  Heading Text...  111111eeeeee   \n3    def789    Summarize...  Example Title 2  Heading Text...  333333dddddd   \n\n             text  \n0  Example text 1  \n1  Example text 3  \n2  Example text 2  \n3  Example text 4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt_id</th>\n      <th>prompt_question</th>\n      <th>prompt_title</th>\n      <th>prompt_text</th>\n      <th>student_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abc123</td>\n      <td>Summarize...</td>\n      <td>Example Title 1</td>\n      <td>Heading Text...</td>\n      <td>000000ffffff</td>\n      <td>Example text 1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>abc123</td>\n      <td>Summarize...</td>\n      <td>Example Title 1</td>\n      <td>Heading Text...</td>\n      <td>222222cccccc</td>\n      <td>Example text 3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>def789</td>\n      <td>Summarize...</td>\n      <td>Example Title 2</td>\n      <td>Heading Text...</td>\n      <td>111111eeeeee</td>\n      <td>Example text 2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>def789</td>\n      <td>Summarize...</td>\n      <td>Example Title 2</td>\n      <td>Heading Text...</td>\n      <td>333333dddddd</td>\n      <td>Example text 4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Spell checking\nAs per my [SpellCheck Tool Comparison notebook](https://www.kaggle.com/code/jasonheesanglee/spellcheck-tool-comparison), pyspellchecker and symspellpy were selected.<br>\nCharacteristics of pyspellchecker : As per the [notebook](https://www.kaggle.com/code/dmitriygakh/my-nlp-set-examples/notebook) of [@dmitriygakh](https://www.kaggle.com/dmitriygakh), this module has comparitively better performance.<br> \nBased on my EDA notebook, this was the fastest and most accurate (tie with simspellpy) misspell detection rate.<br>\nTherefore, I have selected this model to just use as a detector.<br><br>\nsimspellpy had the best accuracy and the result of the misspell correction was the best amongst all the other spellcheckers.<br>\nHowever, detecting with symspellpy took the longest time, and that is why I have separated the detector model and correction model.<br><br>\nI have added each of the full models here, so that I can try to use it by modifying the preprocess configuration.\n\n[SpellCheck Tool Comparison notebook](https://www.kaggle.com/code/jasonheesanglee/spellcheck-tool-comparison)에서 진행한 WordCloud EDA를 통해 다양한 오타가 있는 것을 확인하였습니다.<br>\n`Pharaoh` 등 문맥상 중요할 것으로 판단되는 단어들에도 `Pharoh`, `Pharoah` 등 다양하고 신박한 오타들을 발견하여 오타 처리는 꼭 해야한다고 판단을 했습니다.<br><br>\n따라서, 동일 노트북에서 4개의 오타처리 관련 라이브러리의 (pyspellchecker, TextBlob, AutoCorrect, Symspellpy) 성능을 비교하였고, 본 Task에서는 symspellpy가 가장 좋은 성능을 보이는 것으로 파악하였습니다.<br><br>\n하지만... 주최 측의 [Discussion Post](https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/discussion/424402)에 따르면 오타처리는 굳이 할 필요가 없는 것 같습니다...<br>\n> Spelling and grammar mistakes are not part of the rubric used to generate the scores and raters were instructed to ignore these mistakes when grading the summaries\n","metadata":{}},{"cell_type":"code","source":"def pyspellchecker_detector(sentence):\n    sentence = re.sub(r'[^\\w\\s]','',sentence)\n    spell = SpellChecker()\n    tokens = sentence.split(' ')\n    mis_tokens = []\n    for token in spell.unknown(tokens):\n        if token.isalpha():\n            mis_tokens.append(token)\n    return mis_tokens\n\ndef symspellpy_corrector(mis_tokens):\n    try:\n        sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n        freq_dict = pkg_resources.resource_filename(\"symspellpy\", \"symspell_freq_dict.txt\")\n        sym_spell.load_dictionary(freq_dict, term_index=0, count_index=1)\n        corrected_token = {}\n        for token in tqdm(mis_tokens):\n            terms = sym_spell.lookup_compound(token, \n                                              max_edit_distance=2) \n            if token not in corrected_token.keys():\n                corrected_token[token] = terms[0].term\n        return corrected_token\n\n    except UnicodeDecodeError:\n        return mis_tokens\n\ndef py_sym_checker(df, column):\n    try:\n        mis_tokens = []\n        for row_num in tqdm(range(df.shape[0])):\n            sentence = df[column][row_num]\n            for word in pyspellchecker_detector(sentence):\n                mis_tokens.append(word)\n        \n        mis_token_rep = symspellpy_corrector(mis_tokens)\n        \n        temp = []\n        for row_num in tqdm(range(df.shape[0])):\n            sentence = df[column][row_num]\n            tokens = sentence.split(' ')\n            temp_str = ''\n            for token in tokens:\n                if token in mis_token_rep.keys():\n                    temp_str = temp_str + \" \" + mis_token_rep.get(token)\n                else:\n                    temp_str = temp_str + \" \" + token\n            temp.append(temp_str)\n            \n        df[column] = pd.Series(temp)\n        return df\n    \n    except UnicodeDecodeError:\n        return df\n    \ndef symspellpy_correction(df, column, freq_dict):\n    try:        \n        temp = []\n        for row_num in tqdm(range(df.shape[0])):\n            sentence = df[column][row_num]\n            sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=5)\n            freq_dict = freq_dict\n            sym_spell.load_dictionary(freq_dict, term_index=0, count_index=1)\n            terms = sym_spell.lookup_compound(sentence, \n                                              max_edit_distance=2) \n\n            corrected_sentence = terms[0].term\n            temp.append(corrected_sentence)\n            \n        df[column] = pd.Series(temp)\n        return df\n    \n    except UnicodeDecodeError:\n        return df\n    \ndef pyspell_correction(df, column):\n    try:        \n        temp_total = []\n        for row_num in tqdm(range(df.shape[0])):\n            sentence = df[column][row_num]\n            \n            spell = SpellChecker()\n            tokens = nltk.word_tokenize(sentence)\n            text_length = len(tokens)\n\n            mis_tokens = [token for token in spell.unknown(tokens) if token.isalpha()]\n            temp = []\n            corrected_words = []\n            for word in tqdm(mis_tokens):\n                corrected_word = spell.correction(word)\n                temp.append({word : corrected_word})\n                corrected_words.append(corrected_word)\n\n            temp_1 = []\n            for word in tokens:\n                for set_ in temp:\n                    if list(set_.keys())[0] == word:\n                        word = list(set_.values())[0]\n                        if word in temp_1:\n                            continue\n                        else:\n                            temp_1.append(word)\n                if word in temp_1:\n                    continue\n                else:\n                    temp_1.append(word)\n\n            corrected_sentence = ''\n            for word in temp_1:\n                if word.isalpha() or word.isnumeric() == True:\n                    corrected_sentence = corrected_sentence + word + ' '\n                elif word in [',', '.', '\"', \"'\", '(', ')', '[', ']', '{', '}']:\n                    corrected_sentence = corrected_sentence + word\n                    \n            corrected_sentence = corrected_sentence.replace('  ', ' ').strip()\n            temp_total.append(corrected_sentence)\n            \n        df[column] = pd.Series(temp_total)\n        return df\n    except UnicodeDecodeError:\n        return df","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.061055Z","iopub.execute_input":"2023-09-26T08:53:50.061584Z","iopub.status.idle":"2023-09-26T08:53:50.083435Z","shell.execute_reply.started":"2023-09-26T08:53:50.061550Z","shell.execute_reply":"2023-09-26T08:53:50.082485Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"마찬가지로, 노트북 상단의 전처리 자동화 코드와 동기화 되도록 구성하였습니다.","metadata":{}},{"cell_type":"code","source":"train_processed = train\ntest_processed = test\nif manage_misspelled_words:\n    if misspell_counter == 0:\n        train_misspell_checked = py_sym_checker(train, 'text')\n        print(f\"py & sym train_processed['text'][0] =\\n{train_misspell_checked['text'][0]}\")\n        print()\n        test_misspell_checked = py_sym_checker(test, 'text')\n        print(f\"py & sym test_processed['text'][0] =\\n{test_misspell_checked['text'][0]}\")\n    elif misspell_counter == 1:\n        train_misspell_checked = pyspell_correction(train, 'text')\n        print(f\"py only train_processed['text'][0] =\\n{train_misspell_checked['text'][0]}\")\n        print()\n        test_misspell_checked = pyspell_correction(test, 'text')\n        print(f\"py only test_processed['text'][0] =\\n{test_misspell_checked['text'][0]}\")\n    elif misspell_counter == 2:\n        train_misspell_checked = symspellpy_correction(train, 'text', freq_dict)\n        print(f\"sym only train_processed['text'][0] =\\n{train_misspell_checked['text'][0]}\")\n        print()\n        test_misspell_checked = symspellpy_correction(test, 'text', freq_dict)\n        print(f\"sym only test_processed['text'][0] =\\n{test_misspell_checked['text'][0]}\")\nelse:\n    train_misspell_checked = train\n    test_misspell_checked = test\n    print('no change')","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.085398Z","iopub.execute_input":"2023-09-26T08:53:50.085935Z","iopub.status.idle":"2023-09-26T08:53:50.098393Z","shell.execute_reply.started":"2023-09-26T08:53:50.085895Z","shell.execute_reply":"2023-09-26T08:53:50.097569Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"no change\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Masking\nI am still not sure if I have correctly understood how masking works.<br>\nI am sure that this is needed but not sure of where this process should be taken place.<br>\nIs it really needed to be done during the preprocess process? or is it needed to be done during the training process.<br>\nLet me research & understand more and use it.<br>\n<br>\n마스킹을 하려 했으나, 이렇게 하는 것이 아니라는 피드백을 받아 이 부분은 공부하여 본 다음 다시 도전해 보겠습니다!","metadata":{}},{"cell_type":"code","source":"def keyword_masking_debertav3base(df, base_column, target_column, stop_words):\n    temp_1 = []\n\n    for row in tqdm(range(df.shape[0])):\n        sentence = df[target_column][row]\n        keywords = [word.replace(',', '') for word in df[base_column][row].lower().split() if len(word) > 3 and word not in stop_words]\n        words = [word.strip() for word in sentence.split(' ') if word != '']\n        temp = []\n        for word in words:\n            if word.endswith(','):\n                temp.append(word.replace(',', ''))\n                temp.append('[SEP]')\n                continue\n            elif word.endswith('.'):\n                temp.append(word.replace('.', ''))\n                temp.append('[SEP]')\n                continue\n            elif word.endswith(';'):\n                temp.append(word.replace(';', ''))\n                temp.append('[SEP]')\n                continue\n            if word in keywords:\n                temp.append('[MASK]')\n            else:\n                temp.append(word)\n        sentence = ' '.join(temp)\n        temp_1.append(sentence)\n    temp_Series = pd.Series(temp_1)\n    df[target_column] = temp_Series\n\n    return df\n\ndef unique_list(list_):\n    unique_list = []\n    for x in list_:\n        if x not in unique_list:\n            unique_list.append(x)\n    return unique_list\n\ndef freq_word_masking_debertav3base(df, target_column, stop_words):\n    temp_1 = []\n    \n    \n    for row in tqdm(range(df.shape[0])):\n        sentence = df[target_column][row]\n        words = [word for word in sentence.split(' ')]\n\n        temp_dict = {}\n        for word in unique_list(words):\n            temp_dict[word] = words.count(word)\n\n        mask_list = []\n        for word in temp_dict.keys():\n            if len(word) > 3:\n                if temp_dict.get(word) > 4:\n                    if word not in stop_words:\n                        mask_list.append(word)\n\n        temp = []\n        for word in words:\n            if word in mask_list:\n                word = '[MASK]'\n                temp.append(word)\n            else:\n                temp.append(word)\n\n        sentence = ' '.join(temp)\n        temp_1.append(sentence)\n\n    temp_Series = pd.Series(temp_1)\n    df[target_column] = temp_Series\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.099978Z","iopub.execute_input":"2023-09-26T08:53:50.100545Z","iopub.status.idle":"2023-09-26T08:53:50.116245Z","shell.execute_reply.started":"2023-09-26T08:53:50.100514Z","shell.execute_reply":"2023-09-26T08:53:50.115254Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if keyword_masking == yes:\n    # if CFG.model_name == f\"/kaggle/input/{files[3]}\":\n    train_processed = keyword_masking_debertav3base(train_processed, 'prompt_question', 'text', stop_words)\n    test_processed = keyword_masking_debertav3base(test_processed, 'prompt_question', 'text', stop_words)\n    train_misspell_checked = keyword_masking_debertav3base(train_misspell_checked, 'prompt_question', 'text', stop_words)\n    test_misspell_checked = keyword_masking_debertav3base(test_misspell_checked, 'prompt_question', 'text', stop_words)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.119051Z","iopub.execute_input":"2023-09-26T08:53:50.119263Z","iopub.status.idle":"2023-09-26T08:53:50.130876Z","shell.execute_reply.started":"2023-09-26T08:53:50.119237Z","shell.execute_reply":"2023-09-26T08:53:50.129904Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"if freq_word_masking == yes:\n    # if CFG.model_name == f'/contents/{files[3]}':\n    train_processed = freq_word_masking_debertav3base(train_processed, 'text', stop_words)\n    test_processed = freq_word_masking_debertav3base(test_processed, 'text', stop_words)\n    train_misspell_checked = freq_word_masking_debertav3base(train_misspell_checked, 'text', stop_words)\n    test_misspell_checked = freq_word_masking_debertav3base(test_misspell_checked, 'text', stop_words)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.132322Z","iopub.execute_input":"2023-09-26T08:53:50.132740Z","iopub.status.idle":"2023-09-26T08:53:50.144543Z","shell.execute_reply.started":"2023-09-26T08:53:50.132695Z","shell.execute_reply":"2023-09-26T08:53:50.143540Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"train_processed.text","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.146166Z","iopub.execute_input":"2023-09-26T08:53:50.146686Z","iopub.status.idle":"2023-09-26T08:53:50.160049Z","shell.execute_reply.started":"2023-09-26T08:53:50.146654Z","shell.execute_reply":"2023-09-26T08:53:50.159221Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"0       1 element of an ideal tragedy is that it shoul...\n1       The three elements of an ideal tragedy are:  H...\n2       Aristotle states that an ideal tragedy should ...\n3       One element of an Ideal tragedy is having a co...\n4       The 3 ideal of tragedy is how complex you need...\n                              ...                        \n7160    In paragraph two, they would use pickle meat a...\n7161    in the first paragraph  it says \"either can it...\n7162    They would have piles of filthy meat on the fl...\n7163    They used all sorts of chemical concoctions to...\n7164    The meat would smell sour but the would \"rub i...\nName: text, Length: 7165, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"# Train & Infer\nThis is the copy of the [baseline code](https://www.kaggle.com/code/bulivington/transformers-predictions-base) of [@bulivington](https://www.kaggle.com/bulivington).<br>\n## What have I modified :\nI have separated the inner loop into two, by receiving two different dataset generated above.<br>\nI believe the data should be scored separatedly for each 'content' and 'wording' score.\n<br><br>\n하단은 Train & Infer 코드로 [@bulivington](https://www.kaggle.com/bulivington)님의 [baseline code](https://www.kaggle.com/code/bulivington/transformers-predictions-base)를 활용하였습니다.<br>\n해당 코드에서 제가 변경한 사항은 \n1. 오타처리 여부에 따라 content / wording에 대한 training / inference가 다르게 작동하도록 하였습니다.\n2. `MobileBERT` 모델을 추가하다 보니 해당 모델에 맞는 코드를 새로 짜게 되었습니다.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset as ds\nfrom transformers import MobileBertTokenizer, MobileBertForSequenceClassification, MobileBertConfig, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.161785Z","iopub.execute_input":"2023-09-26T08:53:50.162312Z","iopub.status.idle":"2023-09-26T08:53:50.168497Z","shell.execute_reply.started":"2023-09-26T08:53:50.162280Z","shell.execute_reply":"2023-09-26T08:53:50.167557Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"if manage_misspelled_words:\n    def train_n_infer(train,\n                      train_m,\n                      val,\n                      val_m,\n                      test,\n                      test_m,\n                      model_name,\n                      batch_size,\n                      learning_rate,\n                      weight_decay,\n                      hidden_dropout_prob,\n                      attention_probs_dropout_prob,\n                      num_train_epochs,\n                      save_steps,\n                      random_seed,\n                      max_length,\n                      model_dir,\n                      model_dir_m\n                      ):\n\n        is_mobilebert = model_name == f\"/kaggle/input/{files[7]}\"\n\n        train_content = train[[\"text\", \"content\", \"wording\"]]\n        train_m_content = train_m[[\"text\", \"content\", \"wording\"]]\n        val_content = val[[\"text\", \"content\", \"wording\"]]\n        val_m_content = val_m[[\"text\", \"content\", \"wording\"]]\n\n        test_ = test[[\"text\"]]\n        test_m_ = test_m[['text']]\n\n    # ------------------------------------------------------------------------------------------------------------------\n\n        if is_mobilebert:\n\n            student_model_name = model_name # 'mobilebert-uncased'\n            tokenizer = MobileBertTokenizer.from_pretrained(student_model_name)\n\n#             teacher_model_name = f\"/kaggle/input/bert-base-uncased\" # bert-base-uncased\n            teacher_model_name = f\"/kaggle/input/{files[3]}\" # debertav3base\n#             teacher_model_name = f\"/kaggle/input/{files[6]}\" # robertav3base\n            teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n\n            def tokenizer_text(text):\n                return tokenizer(text, padding='max_length', truncation=True, max_length=CFG.max_length, return_tensors='pt')\n\n            def student_tokenizer_text(examples):\n                labels = [examples[\"content\"], examples[\"wording\"]]\n                tokenized = tokenizer(examples[\"text\"],\n                                padding=False,\n                                truncation=True,\n                                max_length=max_length)\n                return {\n                    **tokenized,\n                    \"labels\": labels,\n                    }\n\n            def student_tokenize_function_test(examples):\n                tokenized = tokenizer(examples[\"text\"],\n                                              padding=False,\n                                              truncation=True,\n                                              max_length=max_length)\n                return tokenized\n\n            def teacher_tokenize_function_test(examples):\n                tokenized = teacher_tokenizer(examples[\"text\"],\n                                              padding=False,\n                                              truncation=True,\n                                              max_length=max_length)\n                return tokenized\n\n\n            train_tokens = train_content['text'].apply(tokenizer_text)\n            val_tokens = val_content['text'].apply(tokenizer_text)\n            train_m_tokens = train_m_content['text'].apply(tokenizer_text)\n            val_m_tokens = val_m_content['text'].apply(tokenizer_text)\n            test_tokens = test_['text'].apply(tokenizer_text)\n            test_m_tokens = test_m_['text'].apply(tokenizer_text)\n            \n            \n            teacher_model_config = AutoConfig.from_pretrained(teacher_model_name)\n            teacher_model_config.update({\n                \"hidden_dropout_prob\": hidden_dropout_prob, # 0.0\n                \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n                \"num_labels\": 2,\n                \"problem_type\": \"regression\",\n            })\n            teacher_model_content = AutoModelForSequenceClassification.from_pretrained(teacher_model_name, config=teacher_model_config)\n\n\n            student_model_config = MobileBertConfig.from_pretrained(student_model_name)\n            student_model_config.update({\n                \"hidden_dropout_prob\": hidden_dropout_prob, # 0.0\n                \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n                \"num_labels\": 2,\n                \"problem_type\": \"regression\",\n            })\n\n            model_content = MobileBertForSequenceClassification.from_pretrained(student_model_name, config=student_model_config)\n\n            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            teacher_model_content.to(device)\n            model_content.to(device)\n\n            optimizer = torch.optim.AdamW(model_content.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n            loss_fn = torch.nn.MSELoss()\n\n            teacher_logits_key = 'logits'\n            student_logits_key = 'logits'\n\n            for epoch in range(CFG.num_train_epochs):\n                model_content.train()\n                for batch in tqdm(train_tokens):\n                    inputs = {k: v.to(device) for k, v in batch.items()}\n\n                    with torch.no_grad():\n                        teacher_outputs = teacher_model_content(**inputs)\n\n                    optimizer.zero_grad()\n                    student_outputs = model_content(**inputs)\n\n                    # print(\"Teacher Model Output Keys:\", teacher_outputs.keys())\n                    # print(\"Student Model Output Keys:\", student_outputs.keys())\n\n                    teacher_logits = teacher_outputs[teacher_logits_key]\n                    student_logits = student_outputs[student_logits_key]\n\n\n                    loss = loss_fn(student_logits.view(-1), teacher_logits.view(-1))\n                    loss.backward()\n                    optimizer.step()\n                print(f'Epoch {epoch + 1}/{CFG.num_train_epochs}, Loss: {loss.item():.4f}')  # , Val Loss: {val_loss:.4f}')\n\n          #### Use model_content later on.\n    # ---------------------------------------------Missing Word---------------------------------------------------------------------\n\n            teacher_tokenizer_m = AutoTokenizer.from_pretrained(teacher_model_name)\n            teacher_model_config_m = AutoConfig.from_pretrained(teacher_model_name)\n            teacher_model_config_m.update({\n                \"hidden_dropout_prob\": hidden_dropout_prob, # 0.0\n                \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n                \"num_labels\": 2,\n                \"problem_type\": \"regression\",\n            })\n            teacher_model_content_m = AutoModelForSequenceClassification.from_pretrained(teacher_model_name, config=teacher_model_config_m)\n            tokenizer_m = MobileBertTokenizer.from_pretrained(student_model_name)\n            student_model_config_m = MobileBertConfig.from_pretrained(student_model_name)\n            student_model_config_m.update({\n                \"hidden_dropout_prob\": hidden_dropout_prob, # 0.0\n                \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n                \"num_labels\": 2,\n                \"problem_type\": \"regression\",\n            })\n\n            model_content_m = MobileBertForSequenceClassification.from_pretrained(student_model_name, config=student_model_config_m)\n\n            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            teacher_model_content_m.to(device)\n            model_content_m.to(device)\n\n            optimizer = torch.optim.AdamW(model_content_m.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n            loss_fn = torch.nn.MSELoss()\n\n            teacher_logits_key = 'logits'\n            student_logits_key = 'logits'\n\n            for epoch in range(CFG.num_train_epochs):\n                model_content_m.train()\n                for batch in tqdm(train_m_tokens):\n                    inputs = {k: v.to(device) for k, v in batch.items()}\n\n                    with torch.no_grad():\n                        teacher_outputs = teacher_model_content_m(**inputs)\n\n                    optimizer.zero_grad()\n                    student_outputs = model_content_m(**inputs)\n\n                    teacher_logits = teacher_outputs[teacher_logits_key]\n                    student_logits = student_outputs[student_logits_key]\n\n\n                    loss = loss_fn(student_logits.view(-1), teacher_logits.view(-1))\n                    loss.backward()\n                    optimizer.step()\n                print(f'Epoch {epoch + 1}/{CFG.num_train_epochs}, Loss: {loss.item():.4f}')  # , Val Loss: {val_loss:.4f}')\n\n          #### Use model_content_m later on.\n\n\n        else:\n\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n            tokenizer_m = AutoTokenizer.from_pretrained(model_name)\n\n            def tokenize_function(examples):\n                labels = [examples[\"content\"], examples[\"wording\"]]\n                tokenized = tokenizer(examples[\"text\"],\n                                padding=False,\n                                truncation=True,\n                                max_length=max_length)\n                return {\n                    **tokenized,\n                    \"labels\": labels,\n                }\n\n            def tokenize_m_function(examples):\n                labels = [examples[\"content\"], examples[\"wording\"]]\n                tokenized = tokenizer_m(examples[\"text\"],\n                                padding=False,\n                                truncation=True,\n                                max_length=max_length)\n                return {\n                    **tokenized,\n                    \"labels\": labels,\n                }\n\n            def tokenize_function_test(examples):\n                tokenized = tokenizer(examples[\"text\"],\n                                padding=False,\n                                truncation=True,\n                                max_length=max_length)\n                return tokenized\n\n            model_config = AutoConfig.from_pretrained(model_name)\n            model_config.update({\n                \"hidden_dropout_prob\": hidden_dropout_prob, # 0.0\n                \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n                \"num_labels\": 2,\n                \"problem_type\": \"regression\",\n            })\n            model_content = AutoModelForSequenceClassification.from_pretrained(model_name, config=model_config)\n\n\n            model_config_m = AutoConfig.from_pretrained(model_name)\n            model_config_m.update({\n                \"hidden_dropout_prob\": hidden_dropout_prob, # 0.0\n                \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n                \"num_labels\": 2,\n                \"problem_type\": \"regression\",\n            })\n            model_content_m = AutoModelForSequenceClassification.from_pretrained(model_name, config=model_config_m)\n\n    # ------------------------------------------------------------------------------------------------------------------\n\n        train_dataset_content = Dataset.from_pandas(train_content, preserve_index=False)\n        val_dataset_content = Dataset.from_pandas(val_content, preserve_index=False)\n\n        train_m_dataset_content = Dataset.from_pandas(train_m_content, preserve_index=False)\n        val_m_dataset_content = Dataset.from_pandas(val_m_content, preserve_index=False)\n\n        seed_everything(seed=42)\n\n    # ------------------------------------------------------------------------------------------------------------------\n        if is_mobilebert:\n            train_tokenized_datasets_content = train_dataset_content.map(student_tokenizer_text, batched=False)\n            val_tokenized_datasets_content = val_dataset_content.map(student_tokenizer_text, batched=False)\n\n            train_m_tokenized_datasets_content = train_m_dataset_content.map(student_tokenizer_text, batched=False)\n            val_m_tokenized_datasets_content = val_m_dataset_content.map(student_tokenizer_text, batched=False)\n\n            test_dataset = Dataset.from_pandas(test_, preserve_index=False)\n            test_tokenized_dataset = test_dataset.map(student_tokenize_function_test, batched=False)\n\n            test_m_dataset = Dataset.from_pandas(test_m_, preserve_index=False)\n            test_m_tokenized_dataset = test_m_dataset.map(student_tokenize_function_test, batched=False)\n\n    # ------------------------------------------------------------------------------------------------------------------\n        else:\n            train_tokenized_datasets_content = train_dataset_content.map(tokenize_function, batched=False)\n            val_tokenized_datasets_content = val_dataset_content.map(tokenize_function, batched=False)\n\n            train_m_tokenized_datasets_content = train_m_dataset_content.map(tokenize_m_function, batched=False)\n            val_m_tokenized_datasets_content = val_m_dataset_content.map(tokenize_m_function, batched=False)\n\n            test_dataset = Dataset.from_pandas(test_, preserve_index=False)\n            test_tokenized_dataset = test_dataset.map(tokenize_function_test, batched=False)\n\n            test_m_dataset = Dataset.from_pandas(test_m_, preserve_index=False)\n            test_m_tokenized_dataset = test_m_dataset.map(tokenize_function_test, batched=False)\n\n    # ------------------------------------------------------------------------------------------------------------------\n\n        data_collator = DataCollatorWithPadding(\n            tokenizer=tokenizer\n            )\n        data_collator_m = DataCollatorWithPadding(\n            tokenizer=tokenizer_m\n            )\n\n    # ------------------------------------------------------------------------------------------------------------------\n\n\n        model_dir = model_dir\n        training_args = TrainingArguments(\n            output_dir=model_dir,\n            load_best_model_at_end=True,\n            learning_rate=learning_rate,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=8,\n            num_train_epochs=num_train_epochs,\n            weight_decay=weight_decay,\n            report_to='none',\n            greater_is_better=False,\n            save_strategy=\"steps\",\n            evaluation_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            metric_for_best_model=\"mcrmse\",\n            save_total_limit=1\n        )\n\n        model_dir_m = model_dir_m\n        training_m_args = TrainingArguments(\n            output_dir=model_dir_m,\n            load_best_model_at_end=True,\n            learning_rate=learning_rate,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=8,\n            num_train_epochs=num_train_epochs,\n            weight_decay=weight_decay,\n            report_to='none',\n            greater_is_better=False,\n            save_strategy=\"steps\",\n            evaluation_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            metric_for_best_model=\"mcrmse\",\n            save_total_limit=1\n        )\n\n    # ------------------------------------------------------------------------------------------------------------------\n\n        trainer_content = Trainer(\n            model=model_content,\n            args=training_args,\n            train_dataset=train_tokenized_datasets_content,\n            eval_dataset=val_tokenized_datasets_content,\n            tokenizer=tokenizer,\n            compute_metrics=compute_mcrmse,\n            data_collator=data_collator\n        )\n\n        trainer_m_content = Trainer(\n            model=model_content_m,\n            args=training_m_args,\n            train_dataset=train_m_tokenized_datasets_content,\n            eval_dataset=val_m_tokenized_datasets_content,\n            tokenizer=tokenizer_m,\n            compute_metrics=compute_mcrmse,\n            data_collator=data_collator_m\n        )\n\n    # ------------------------------------------------------------------------------------------------------------------\n\n        trainer_content.train()\n        trainer_m_content.train()\n\n        best_check__ = os.listdir(model_dir)\n        best_check_m__ = os.listdir(model_dir_m)\n\n        checkpoint_files = [filename for filename in best_check__ if filename.startswith(\"checkpoint\")]\n        checkpoint_files_m = [filename for filename in best_check_m__ if filename.startswith(\"checkpoint\")]\n\n        best_check = checkpoint_files[0]\n        best_check_m = checkpoint_files_m[0]\n\n    # ------------------------------------------------------------------------------------------------------------------\n\n        if is_mobilebert:\n            model_content = MobileBertForSequenceClassification.from_pretrained(f\"{model_dir}/{best_check}\")\n            model_content.eval()\n\n            model_m_content = MobileBertForSequenceClassification.from_pretrained(f\"{model_dir_m}/{best_check_m}\")\n            model_m_content.eval()\n\n\n        else:\n            model_content = AutoModelForSequenceClassification.from_pretrained(f\"{model_dir}/{best_check}\")\n            model_content.eval()\n\n            model_m_content = AutoModelForSequenceClassification.from_pretrained(f\"{model_dir_m}/{best_check_m}\")\n            model_m_content.eval()\n\n\n        test_args = TrainingArguments(\n            output_dir=model_dir,\n            do_train = False,\n            do_predict = True,\n            per_device_eval_batch_size = 4,\n            dataloader_drop_last = False,\n        )\n\n        test_m_args = TrainingArguments(\n            output_dir=model_dir_m,\n            do_train = False,\n            do_predict = True,\n            per_device_eval_batch_size = 4,\n            dataloader_drop_last = False,\n        )\n\n        # init trainer\n        infer_content = Trainer(\n                      model = model_content,\n                      tokenizer=tokenizer,\n                      data_collator=data_collator,\n                      args = test_args)\n\n        infer_content_m = Trainer(\n                      model = model_content_m,\n                      tokenizer=tokenizer_m,\n                      data_collator=data_collator_m,\n                      args = test_m_args)\n\n        val_results_content = infer_content.predict(val_tokenized_datasets_content)[0]\n        test_results_content = infer_content.predict(test_tokenized_dataset)[0]\n\n        val_m_results_content = infer_content_m.predict(val_m_tokenized_datasets_content)[0]\n        test_m_results_content = infer_content_m.predict(test_m_tokenized_dataset)[0]\n\n        model_content.save_pretrained(model_dir)\n        tokenizer.save_pretrained(model_dir)\n        shutil.rmtree(f\"{model_dir}/{best_check}\")\n\n        model_m_content.save_pretrained(model_dir_m)\n        tokenizer.save_pretrained(model_dir_m)\n        shutil.rmtree(f\"{model_dir_m}/{best_check_m}\")\n\n        return val_results_content, test_results_content, val_m_results_content, test_m_results_content\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.170284Z","iopub.execute_input":"2023-09-26T08:53:50.170889Z","iopub.status.idle":"2023-09-26T08:53:50.227513Z","shell.execute_reply.started":"2023-09-26T08:53:50.170854Z","shell.execute_reply":"2023-09-26T08:53:50.226427Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"if manage_misspelled_words:\n    def get_oof_pred_n_test(train,\n                            train_m,\n                            test,\n                            test_m,\n                            model_name,\n                            n_splits,\n                            batch_size,\n                            learning_rate,\n#                             warmup_ratio,\n                            hidden_dropout_prob,\n                            attention_probs_dropout_prob,\n                            weight_decay,\n                            num_train_epochs,\n                            random_seed,\n                            save_steps,\n                            max_length\n                           ):\n        kfl = KFold(n_splits=n_splits, random_state=random_seed, shuffle=True)\n        oof_content = np.zeros((len(train), 2))\n        oof_m_content = np.zeros((len(train_m), 2))\n        test_pred_content = np.zeros((len(test), 2))\n        test_m_pred_content = np.zeros((len(test_m), 2))\n\n        if CFG.model_name == f'kaggle/input/{files[2]}':\n            model_name = f'kaggle/input/{files[2]}/{files[2]}'\n            print('changed')\n\n        model_name_ = model_name.split(\"/\")[-1] + \"_\"\n        model_name_m_ = model_name.split(\"/\")[-1] + \"_m\"\n        # if os.path.exists(model_name_):\n            # shutil.rmtree(model_name_)\n        # os.mkdir(model_name_)\n\n\n        for i, (train_indx, val_indx) in enumerate(kfl.split(train, groups=train[\"prompt_id\"])):\n            print(f\"fold {i}:\")\n            train_ = train.iloc[train_indx]\n            val_ = train.iloc[val_indx]\n            train_m_ = train_m.iloc[train_indx]\n            val_m_ = train_m.iloc[val_indx]\n            val_res_content, test_res_content, val_m_res_content, test_m_res_content = train_n_infer(train_,\n                                                                                                    train_m_,\n                                                                                                    val_,\n                                                                                                    val_m_,\n                                                                                                    test,\n                                                                                                    test_m,\n                                                                                                    model_name=model_name,\n                                                                                                    batch_size=batch_size,\n                                                                                                    learning_rate=learning_rate,\n#                                                                                                     warmup_ratio=warmup_ratio,\n                                                                                                    hidden_dropout_prob=hidden_dropout_prob,\n                                                                                                    attention_probs_dropout_prob=attention_probs_dropout_prob,\n                                                                                                    weight_decay=weight_decay,\n                                                                                                    num_train_epochs=num_train_epochs,\n                                                                                                    save_steps=save_steps,\n                                                                                                    max_length=max_length,\n                                                                                                    random_seed=random_seed,\n                                                                                                    model_dir=f\"{model_name_}/fold_{i}\",\n                                                                                                    model_dir_m=f\"{model_name_m_}/fold_{i}\"\n                                                                                                    )\n\n            oof_content[val_indx] = val_res_content\n            oof_m_content[val_indx] = val_m_res_content\n            test_pred_content += test_res_content/n_splits\n            test_m_pred_content += test_m_res_content/n_splits\n\n        oof_train = pd.DataFrame(oof_content, columns=[f\"content_pred_{model_name_}\", f\"wording_pred_{model_name_}\"])\n        oof_m_train = pd.DataFrame(oof_m_content, columns=[f\"content_pred_{model_name_m_}\", f\"wording_pred_{model_name_m_}\"])\n        test_pred = pd.DataFrame(test_pred_content, columns=[f\"content_pred_{model_name_}\", f\"wording_pred_{model_name_}\"])\n        test_m_pred = pd.DataFrame(test_m_pred_content, columns=[f\"content_pred_{model_name_m_}\", f\"wording_pred_{model_name_m_}\"])\n        print(f'\\n\\noof_train =\\n{oof_train}\\n')\n        print(f'oof_mtrain =\\n{oof_m_train}\\n')\n        print(f'test_pred =\\n{test_pred}\\n')\n        print(f'test_m_pred =\\n{test_m_pred}\\n')\n\n\n        cv_metric = compute_mcrmse((oof_train.values, train[[\"content\", \"wording\"]]))\n        cv_m_metric = compute_mcrmse((oof_m_train.values, train_m[[\"content\", \"wording\"]]))\n\n        print(f\"cv mcrmse: {cv_metric}, cv_m mcrmse: {cv_m_metric}\")\n        with open(f\"{model_name_}/cv_metric.json\", \"w\") as outfile:\n            json.dump(cv_metric, outfile)\n        with open(f\"{model_name_m_}/cv_m_metric.json\", \"w\") as outfile:\n            json.dump(cv_m_metric, outfile)\n        oof_train.to_csv(f\"{model_name_}/oof_train.csv\", index=False)\n        oof_m_train.to_csv(f\"{model_name_m_}/oof_m_train.csv\", index=False)\n\n        return oof_train, oof_m_train, test_pred, test_m_pred","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.229194Z","iopub.execute_input":"2023-09-26T08:53:50.229460Z","iopub.status.idle":"2023-09-26T08:53:50.250735Z","shell.execute_reply.started":"2023-09-26T08:53:50.229427Z","shell.execute_reply":"2023-09-26T08:53:50.249604Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"if manage_misspelled_words:\n    oof_train, oof_m_train, test_pred, test_m_pred  = get_oof_pred_n_test(train_processed,\n                                                                          train_misspell_checked,\n                                                                          test_processed,\n                                                                          test_misspell_checked,\n                                                                          model_name=CFG.model_name,\n                                                                          learning_rate=CFG.learning_rate,\n#                                                                           warmup_ratio=CFG.warmup_ratio,\n                                                                          hidden_dropout_prob=CFG.hidden_dropout_prob,\n                                                                          attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n                                                                          weight_decay=CFG.weight_decay,\n                                                                          num_train_epochs=CFG.num_train_epochs,\n                                                                          n_splits=CFG.n_splits,\n                                                                          batch_size=CFG.batch_size,\n                                                                          random_seed=CFG.random_seed,\n                                                                          save_steps=CFG.save_steps,\n                                                                          max_length=CFG.max_length\n                                                                         )","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.252196Z","iopub.execute_input":"2023-09-26T08:53:50.252568Z","iopub.status.idle":"2023-09-26T08:53:50.267293Z","shell.execute_reply.started":"2023-09-26T08:53:50.252530Z","shell.execute_reply":"2023-09-26T08:53:50.266250Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"if not manage_misspelled_words:\n    def train_n_infer(train,\n                      val,\n                      test,\n                      model_name,\n                      batch_size,\n                      learning_rate,\n                      weight_decay,\n                      hidden_dropout_prob,\n                      attention_probs_dropout_prob,\n                      num_train_epochs,\n                      save_steps,\n                      random_seed,\n                      max_length,\n                      model_dir,\n                      ):\n\n        is_mobilebert = model_name == f\"/kaggle/input/{files[7]}\"\n\n        train_content = train[[\"text\", \"content\", \"wording\"]]\n        val_content = val[[\"text\", \"content\", \"wording\"]]\n\n        test_ = test[[\"text\"]]\n\n    # ------------------------------------------------------------------------------------------------------------------\n\n        if is_mobilebert:\n\n            student_model_name = model_name # 'mobilebert-uncased'\n            tokenizer = MobileBertTokenizer.from_pretrained(student_model_name)\n\n#             teacher_model_name = f\"/kaggle/input/bert-base-uncased\" # bert-base-uncased\n            teacher_model_name = f\"/kaggle/input/{files[3]}\" # debertav3base\n#             teacher_model_name = f\"/kaggle/input/{files[6]}\" # robertav3base\n\n            teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n\n            def tokenizer_text(text):\n                return tokenizer(text, padding='max_length', truncation=True, max_length=CFG.max_length, return_tensors='pt')\n\n            def student_tokenizer_text(examples):\n                labels = [examples[\"content\"], examples[\"wording\"]]\n                tokenized = tokenizer(examples[\"text\"],\n                                padding=False,\n                                truncation=True,\n                                max_length=max_length)\n                return {\n                    **tokenized,\n                    \"labels\": labels,\n                    }\n\n            def student_tokenize_function_test(examples):\n                tokenized = tokenizer(examples[\"text\"],\n                                              padding=False,\n                                              truncation=True,\n                                              max_length=max_length)\n                return tokenized\n\n            def teacher_tokenize_function_test(examples):\n                tokenized = teacher_tokenizer(examples[\"text\"],\n                                              padding=False,\n                                              truncation=True,\n                                              max_length=max_length)\n                return tokenized\n\n\n            train_tokens = train_content['text'].apply(tokenizer_text)\n            val_tokens = val_content['text'].apply(tokenizer_text)\n            test_tokens = test_['text'].apply(tokenizer_text)\n            \n            teacher_model_config = AutoConfig.from_pretrained(teacher_model_name)\n            teacher_model_config.update({\n                \"hidden_dropout_prob\": hidden_dropout_prob, # 0.0\n                \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n                \"num_labels\": 2,\n                \"problem_type\": \"regression\",\n            })\n            teacher_model_content = AutoModelForSequenceClassification.from_pretrained(teacher_model_name, config=teacher_model_config)\n\n\n            student_model_config = MobileBertConfig.from_pretrained(student_model_name)\n            student_model_config.update({\n                \"hidden_dropout_prob\": hidden_dropout_prob, # 0.0\n                \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n                \"num_labels\": 2,\n                \"problem_type\": \"regression\",\n            })\n\n            model_content = MobileBertForSequenceClassification.from_pretrained(student_model_name, config=student_model_config)\n\n            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            teacher_model_content.to(device)\n            model_content.to(device)\n\n            optimizer = torch.optim.AdamW(model_content.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n            loss_fn = torch.nn.MSELoss()\n\n            teacher_logits_key = 'logits'\n            student_logits_key = 'logits'\n\n            for epoch in range(CFG.num_train_epochs):\n                model_content.train()\n                for batch in tqdm(train_tokens):\n                    inputs = {k: v.to(device) for k, v in batch.items()}\n\n                    with torch.no_grad():\n                        teacher_outputs = teacher_model_content(**inputs)\n\n                    optimizer.zero_grad()\n                    student_outputs = model_content(**inputs)\n\n                    # print(\"Teacher Model Output Keys:\", teacher_outputs.keys())\n                    # print(\"Student Model Output Keys:\", student_outputs.keys())\n\n                    teacher_logits = teacher_outputs[teacher_logits_key]\n                    student_logits = student_outputs[student_logits_key]\n\n\n                    loss = loss_fn(student_logits.view(-1), teacher_logits.view(-1))\n                    loss.backward()\n                    optimizer.step()\n                print(f'Epoch {epoch + 1}/{CFG.num_train_epochs}, Loss: {loss.item():.4f}')  # , Val Loss: {val_loss:.4f}')\n\n        else:\n\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n            def tokenize_function(examples):\n                labels = [examples[\"content\"], examples[\"wording\"]]\n                tokenized = tokenizer(examples[\"text\"],\n                                padding=False,\n                                truncation=True,\n                                max_length=max_length)\n                return {\n                    **tokenized,\n                    \"labels\": labels,\n                }\n\n\n            def tokenize_function_test(examples):\n                tokenized = tokenizer(examples[\"text\"],\n                                padding=False,\n                                truncation=True,\n                                max_length=max_length)\n                return tokenized\n\n            model_config = AutoConfig.from_pretrained(model_name)\n            model_config.update({\n                \"hidden_dropout_prob\": hidden_dropout_prob, # 0.0\n                \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n                \"num_labels\": 2,\n                \"problem_type\": \"regression\",\n            })\n            model_content = AutoModelForSequenceClassification.from_pretrained(model_name, config=model_config)\n\n            \n        train_dataset_content = Dataset.from_pandas(train_content, preserve_index=False)\n        val_dataset_content = Dataset.from_pandas(val_content, preserve_index=False)\n\n        seed_everything(seed=42)\n\n        if is_mobilebert:\n            train_tokenized_datasets_content = train_dataset_content.map(student_tokenizer_text, batched=False)\n            val_tokenized_datasets_content = val_dataset_content.map(student_tokenizer_text, batched=False)\n\n            test_dataset = Dataset.from_pandas(test_, preserve_index=False)\n            test_tokenized_dataset = test_dataset.map(student_tokenize_function_test, batched=False)\n\n        else:\n            train_tokenized_datasets_content = train_dataset_content.map(tokenize_function, batched=False)\n            val_tokenized_datasets_content = val_dataset_content.map(tokenize_function, batched=False)\n\n            test_dataset = Dataset.from_pandas(test_, preserve_index=False)\n            test_tokenized_dataset = test_dataset.map(tokenize_function_test, batched=False)\n\n    # ------------------------------------------------------------------------------------------------------------------\n\n        data_collator = DataCollatorWithPadding(\n            tokenizer=tokenizer\n            )\n        \n    # ------------------------------------------------------------------------------------------------------------------\n\n\n        model_dir = model_dir\n        training_args = TrainingArguments(\n            output_dir=model_dir,\n            load_best_model_at_end=True,\n            learning_rate=learning_rate,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=8,\n            num_train_epochs=num_train_epochs,\n            weight_decay=weight_decay,\n            report_to='none',\n            greater_is_better=False,\n            save_strategy=\"steps\",\n            evaluation_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            metric_for_best_model=\"mcrmse\",\n            save_total_limit=1\n        )\n\n    # ------------------------------------------------------------------------------------------------------------------\n\n        trainer_content = Trainer(\n            model=model_content,\n            args=training_args,\n            train_dataset=train_tokenized_datasets_content,\n            eval_dataset=val_tokenized_datasets_content,\n            tokenizer=tokenizer,\n            compute_metrics=compute_mcrmse,\n            data_collator=data_collator\n        )\n\n    # ------------------------------------------------------------------------------------------------------------------\n\n        trainer_content.train()\n\n        best_check__ = os.listdir(model_dir)\n\n        checkpoint_files = [filename for filename in best_check__ if filename.startswith(\"checkpoint\")]\n\n        best_check = checkpoint_files[0]\n\n    # ------------------------------------------------------------------------------------------------------------------\n\n        if is_mobilebert:\n            model_content = MobileBertForSequenceClassification.from_pretrained(f\"{model_dir}/{best_check}\")\n            model_content.eval()\n\n        else:\n            model_content = AutoModelForSequenceClassification.from_pretrained(f\"{model_dir}/{best_check}\")\n            model_content.eval()\n\n        test_args = TrainingArguments(\n            output_dir=model_dir,\n            do_train = False,\n            do_predict = True,\n            per_device_eval_batch_size = 4,\n            dataloader_drop_last = False,\n        )\n\n        # init trainer\n        infer_content = Trainer(\n                      model = model_content,\n                      tokenizer=tokenizer,\n                      data_collator=data_collator,\n                      args = test_args)\n\n        val_results_content = infer_content.predict(val_tokenized_datasets_content)[0]\n        test_results_content = infer_content.predict(test_tokenized_dataset)[0]\n\n        model_content.save_pretrained(model_dir)\n        tokenizer.save_pretrained(model_dir)\n        shutil.rmtree(f\"{model_dir}/{best_check}\")\n\n        return val_results_content, test_results_content","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.269023Z","iopub.execute_input":"2023-09-26T08:53:50.269699Z","iopub.status.idle":"2023-09-26T08:53:50.306534Z","shell.execute_reply.started":"2023-09-26T08:53:50.269659Z","shell.execute_reply":"2023-09-26T08:53:50.305504Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"if not manage_misspelled_words:\n    def get_oof_pred_n_test(train,\n                            test,\n                            model_name,\n                            n_splits,\n                            batch_size,\n                            learning_rate,\n#                             warmup_ratio,\n                            hidden_dropout_prob,\n                            attention_probs_dropout_prob,\n                            weight_decay,\n                            num_train_epochs,\n                            random_seed,\n                            save_steps,\n                            max_length\n                           ):\n        kfl = KFold(n_splits=n_splits, random_state=random_seed, shuffle=True)\n        oof_content = np.zeros((len(train), 2))\n        test_pred_content = np.zeros((len(test), 2))\n\n        if CFG.model_name == f'kaggle/input/{files[2]}':\n            model_name = f'kaggle/input/{files[2]}/{files[2]}'\n            print('changed')\n\n        model_name_ = model_name.split(\"/\")[-1] + \"_\"\n        # if os.path.exists(model_name_):\n            # shutil.rmtree(model_name_)\n        # os.mkdir(model_name_)\n\n\n        for i, (train_indx, val_indx) in enumerate(kfl.split(train, groups=train[\"prompt_id\"])):\n            print(f\"fold {i}:\")\n            train_ = train.iloc[train_indx]\n            val_ = train.iloc[val_indx]\n            val_res_content, test_res_content = train_n_infer(train_,\n                                                              val_,\n                                                              test,\n                                                              model_name=model_name,\n                                                              batch_size=batch_size,\n                                                              learning_rate=learning_rate,\n#                                                               warmup_ratio=warmup_ratio,\n                                                              hidden_dropout_prob=hidden_dropout_prob,\n                                                              attention_probs_dropout_prob=attention_probs_dropout_prob,\n                                                              weight_decay=weight_decay,\n                                                              num_train_epochs=num_train_epochs,\n                                                              save_steps=save_steps,\n                                                              max_length=max_length,\n                                                              random_seed=random_seed,\n                                                              model_dir=f\"{model_name_}/fold_{i}\",\n                                                             )\n\n            oof_content[val_indx] = val_res_content\n            test_pred_content += test_res_content/n_splits\n\n        oof_train = pd.DataFrame(oof_content, columns=[f\"content_pred_{model_name_}\", f\"wording_pred_{model_name_}\"])\n        test_pred = pd.DataFrame(test_pred_content, columns=[f\"content_pred_{model_name_}\", f\"wording_pred_{model_name_}\"])\n        print(f'\\n\\noof_train =\\n{oof_train}\\n')\n        print(f'test_pred =\\n{test_pred}\\n')\n\n\n        cv_metric = compute_mcrmse((oof_train.values, train[[\"content\", \"wording\"]]))\n\n        print(f\"cv mcrmse: {cv_metric}\")\n        with open(f\"{model_name_}/cv_metric.json\", \"w\") as outfile:\n            json.dump(cv_metric, outfile)\n        oof_train.to_csv(f\"{model_name_}/oof_train.csv\", index=False)\n\n        return oof_train, test_pred\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.310254Z","iopub.execute_input":"2023-09-26T08:53:50.310497Z","iopub.status.idle":"2023-09-26T08:53:50.325536Z","shell.execute_reply.started":"2023-09-26T08:53:50.310468Z","shell.execute_reply":"2023-09-26T08:53:50.324563Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"if not manage_misspelled_words:\n    oof_train, test_pred  = get_oof_pred_n_test(train_processed,\n                                                test_processed,\n                                                model_name=CFG.model_name,\n                                                learning_rate=CFG.learning_rate,\n#                                                 warmup_ratio=CFG.warmup_ratio,\n                                                hidden_dropout_prob=CFG.hidden_dropout_prob,\n                                                attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n                                                weight_decay=CFG.weight_decay,\n                                                num_train_epochs=CFG.num_train_epochs,\n                                                n_splits=CFG.n_splits,\n                                                batch_size=CFG.batch_size,\n                                                random_seed=CFG.random_seed,\n                                                save_steps=CFG.save_steps,\n                                                max_length=CFG.max_length\n                                               )\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:50.327910Z","iopub.execute_input":"2023-09-26T08:53:50.328466Z","iopub.status.idle":"2023-09-26T08:53:51.833228Z","shell.execute_reply.started":"2023-09-26T08:53:50.328432Z","shell.execute_reply":"2023-09-26T08:53:51.831732Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"fold 0:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 1 \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m manage_misspelled_words:                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 2 \u001b[2m│   \u001b[0moof_train, test_pred  = get_oof_pred_n_test(train_processed,                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   \u001b[0mtest_processed,                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   \u001b[0mmodel_name=CFG.model_name,                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   \u001b[0mlearning_rate=CFG.learning_rate,            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92mget_oof_pred_n_test\u001b[0m:\u001b[94m35\u001b[0m                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m32 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mfold \u001b[0m\u001b[33m{\u001b[0mi\u001b[33m}\u001b[0m\u001b[33m:\u001b[0m\u001b[33m\"\u001b[0m)                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrain_ = train.iloc[train_indx]                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m34 \u001b[0m\u001b[2m│   │   │   \u001b[0mval_ = train.iloc[val_indx]                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m35 \u001b[2m│   │   │   \u001b[0mval_res_content, test_res_content = train_n_infer(train_,                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m36 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   │   │   │     \u001b[0mval_,                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m37 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   │   │   │     \u001b[0mtest,                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m38 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   │   │   │     \u001b[0mmodel_name=model_name,        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92mtrain_n_infer\u001b[0m:\u001b[94m154\u001b[0m                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mnum_labels\u001b[0m\u001b[33m\"\u001b[0m: \u001b[94m2\u001b[0m,                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mproblem_type\u001b[0m\u001b[33m\"\u001b[0m: \u001b[33m\"\u001b[0m\u001b[33mregression\u001b[0m\u001b[33m\"\u001b[0m,                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m153 \u001b[0m\u001b[2m│   │   │   \u001b[0m})                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m154 \u001b[2m│   │   │   \u001b[0mmodel_content = AutoModelForSequenceClassification.from_pretrained(model_nam   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m155 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m157 \u001b[0m\u001b[2m│   │   \u001b[0mtrain_dataset_content = Dataset.from_pandas(train_content, preserve_index=\u001b[94mFalse\u001b[0m)   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/\u001b[0m\u001b[1;33mauto_factory.py\u001b[0m:\u001b[94m487\u001b[0m in          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m484 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m model_class.from_pretrained(                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m487 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mUnrecognized configuration class \u001b[0m\u001b[33m{\u001b[0mconfig.\u001b[91m__class__\u001b[0m\u001b[33m}\u001b[0m\u001b[33m for this kind of AutoM\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m489 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mModel type should be one of \u001b[0m\u001b[33m{\u001b[0m\u001b[33m'\u001b[0m\u001b[33m, \u001b[0m\u001b[33m'\u001b[0m.join(c.\u001b[91m__name__\u001b[0m\u001b[90m \u001b[0m\u001b[94mfor\u001b[0m\u001b[90m \u001b[0mc\u001b[90m \u001b[0m\u001b[95min\u001b[0m\u001b[90m \u001b[0m\u001b[96mcls\u001b[0m._model_mapp   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m490 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mValueError: \u001b[0mUnrecognized configuration class \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'transformers.models.t5.configuration_t5.T5Config'\u001b[0m\u001b[1m>\u001b[0m for this \nkind of AutoModel: AutoModelForSequenceClassification.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, \nBioGptConfig, BloomConfig, CamembertConfig, CanineConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig, \nDebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, \nFlaubertConfig, FNetConfig, FunnelConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, \nGPTJConfig, IBertConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig, \nLongformerConfig, LukeConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, \nMPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, \nPerceiverConfig, PLBartConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, \nRobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, TransfoXLConfig, \nXLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig.\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> manage_misspelled_words:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 2 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>oof_train, test_pred  = get_oof_pred_n_test(train_processed,                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   │   │   </span>test_processed,                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   │   │   </span>model_name=CFG.model_name,                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   │   │   </span>learning_rate=CFG.learning_rate,            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_oof_pred_n_test</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">35</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"fold {</span>i<span style=\"color: #808000; text-decoration-color: #808000\">}:\"</span>)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>train_ = train.iloc[train_indx]                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>val_ = train.iloc[val_indx]                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>35 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>val_res_content, test_res_content = train_n_infer(train_,                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">36 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   │   │   │   │   │     </span>val_,                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">37 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   │   │   │   │   │     </span>test,                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   │   │   │   │   │     </span>model_name=model_name,        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_n_infer</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">154</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">151 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"num_labels\"</span>: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">152 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"problem_type\"</span>: <span style=\"color: #808000; text-decoration-color: #808000\">\"regression\"</span>,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">153 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>})                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>154 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model_content = AutoModelForSequenceClassification.from_pretrained(model_nam   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">155 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">156 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">157 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>train_dataset_content = Dataset.from_pandas(train_content, preserve_index=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/auto/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">auto_factory.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> model_class.from_pretrained(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">488 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Unrecognized configuration class {</span>config.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__class__</span><span style=\"color: #808000; text-decoration-color: #808000\">} for this kind of AutoM</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Model type should be one of {', '</span>.join(c.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span>c<span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._model_mapp   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">490 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Unrecognized configuration class <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.t5.configuration_t5.T5Config'</span><span style=\"font-weight: bold\">&gt;</span> for this \nkind of AutoModel: AutoModelForSequenceClassification.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, \nBioGptConfig, BloomConfig, CamembertConfig, CanineConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig, \nDebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, \nFlaubertConfig, FNetConfig, FunnelConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, \nGPTJConfig, IBertConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig, \nLongformerConfig, LukeConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, \nMPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, \nPerceiverConfig, PLBartConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, \nRobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, TransfoXLConfig, \nXLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig.\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"if manage_misspelled_words:\n\n    sample_submission[\"content\"] = test_m_pred.values[: ,0]\n    sample_submission[\"wording\"] = test_pred.values[:, 1]\n\n    display(sample_submission)\n\nelse:\n\n    sample_submission[\"content\"] = test_pred.values[: ,0]\n    sample_submission[\"wording\"] = test_pred.values[:, 1]\n\n    display(sample_submission)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:51.834978Z","iopub.status.idle":"2023-09-26T08:53:51.835771Z","shell.execute_reply.started":"2023-09-26T08:53:51.835473Z","shell.execute_reply":"2023-09-26T08:53:51.835501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if CFG.model_name!=f\"/kaggle/input/{files[7]}\":\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T08:53:51.837266Z","iopub.status.idle":"2023-09-26T08:53:51.837972Z","shell.execute_reply.started":"2023-09-26T08:53:51.837726Z","shell.execute_reply":"2023-09-26T08:53:51.837750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}