{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### This notebook originates from the [work](https://www.kaggle.com/code/nhttinnguynbch/commonlit-ess-lgbm-autocorrect-deberta-v3-tuned) of [@nhttinnguynbch](https://www.kaggle.com/nhttinnguynbch)<br>Which originates from the [work](https://www.kaggle.com/code/siddhvr/commonlit-ess-lgbm-autocorrect-deberta-v3-tuned) of [@siddhvr](https://www.kaggle.com/siddhvr)\n\nWith this notebook, I tried to perform the preprocessing part separately from the submission notebook.<br>\nSo that I could save some time when submitting the actual notebook I was working on.<br>\n<br>\nHalf of this notebook is generated by [@siddhvr](https://www.kaggle.com/siddhvr) and the rest is by myself, based on this [notebook](https://www.kaggle.com/code/jasonheesanglee/spellcheck-tool-comparison) for comparing spellchecking tools.<br>\nHowever, now I found out that this wasn't the key to achieve high marks on this competition.","metadata":{}},{"cell_type":"code","source":"# !pip install -q \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\"\n# !pip install -q \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"\n# !pip install -q '/kaggle/input/wheel-downloader/wheelhouse/NLP/editdistpy-0.1.3.tar.gz'\n# !pip install -q '/kaggle/input/symspell-677/editdistpy-0.1.3-cp310-cp310-linux_x86_64.whl'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-11T06:09:58.248598Z","iopub.execute_input":"2023-10-11T06:09:58.249161Z","iopub.status.idle":"2023-10-11T06:09:58.257880Z","shell.execute_reply.started":"2023-10-11T06:09:58.249108Z","shell.execute_reply":"2023-10-11T06:09:58.256350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q autocorrect pyspellchecker symspellpy","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:09:58.266070Z","iopub.execute_input":"2023-10-11T06:09:58.266735Z","iopub.status.idle":"2023-10-11T06:10:09.053593Z","shell.execute_reply.started":"2023-10-11T06:09:58.266704Z","shell.execute_reply":"2023-10-11T06:10:09.051845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport logging\nimport os\nimport shutil\nimport json\nimport transformers\nimport sentencepiece\nimport pkg_resources\n\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\n# from transformers import T5ForConditionalGeneration, T5TokenizerFast, T5Config\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nimport spacy\nimport re\nfrom autocorrect import Speller\nfrom spellchecker import SpellChecker\nfrom symspellpy import SymSpell, Verbosity\nimport lightgbm as lgb\nimport optuna\n\n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:10:09.055958Z","iopub.execute_input":"2023-10-11T06:10:09.056470Z","iopub.status.idle":"2023-10-11T06:10:19.351218Z","shell.execute_reply.started":"2023-10-11T06:10:09.056422Z","shell.execute_reply":"2023-10-11T06:10:19.349962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:10:19.355466Z","iopub.execute_input":"2023-10-11T06:10:19.355885Z","iopub.status.idle":"2023-10-11T06:10:19.366160Z","shell.execute_reply.started":"2023-10-11T06:10:19.355835Z","shell.execute_reply":"2023-10-11T06:10:19.365044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files = ['debertav3base', # files[0]\n         'albert-large-v2', # files[1]\n         'bert-base-uncased', # files[2]\n         'bert-large-uncased', # files[3]\n         'distilroberta-base', # files[4]\n         'distilbert-base-uncased', # files[5]\n         'google-electra-base-discriminator', # files[6]\n         'facebook-bart-base', # files[7] # Not working\n         'facebook-bart-large', # files[8]\n         'funnel-transformer-small', # files[9]\n         'funnel-transformer-large', # files[10]\n         'roberta-base', # files[11]\n         'roberta-large', # files[12]\n         't5-base', # files[13] # don't use\n         't5-large', # files[14] # don't use\n         'xlnet-base-cased', # files[15]\n         'xlnet-large-cased' # files[16]\n         ]","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:10:19.368670Z","iopub.execute_input":"2023-10-11T06:10:19.369056Z","iopub.status.idle":"2023-10-11T06:10:19.383994Z","shell.execute_reply.started":"2023-10-11T06:10:19.369006Z","shell.execute_reply":"2023-10-11T06:10:19.382632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IS_DEBUG = True\nOPTUNA = False\nFOLD = 'G_FOLD' # 'G_FOLD' or 'S_FOLD'\n\nSEP_TKN = ' #### ' # ' [SEP]' or ' #### '\n\nif SEP_TKN == ' #### ':\n    CLS_TKN = ''\nelse:\n    CLS_TKN = ' [CLS] '\n    \nclass CFG:\n    model_name=files[0]\n    learning_rate=0.000016   #0.000015\n    weight_decay=0.007        #0.02\n    hidden_dropout_prob=0.007\n    attention_probs_dropout_mprob=0.007\n    num_train_epochs=1 if IS_DEBUG else 4\n    n_splits= 2 if IS_DEBUG else 4\n    batch_size= 2 if IS_DEBUG else 8\n    random_seed=42\n    save_steps=1000 if IS_DEBUG else 100\n    max_length= 10 if IS_DEBUG else 512","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:10:19.386038Z","iopub.execute_input":"2023-10-11T06:10:19.386775Z","iopub.status.idle":"2023-10-11T06:10:19.401779Z","shell.execute_reply.started":"2023-10-11T06:10:19.386731Z","shell.execute_reply":"2023-10-11T06:10:19.400497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"method = ['py_and_sym', 'pyspell_only', 'symspell_only']\n\nfreq_dict_list = [\"/content/symspell-677/symspell_freq_dict.txt\",\n            \"/content/symspell-677/frequency_dictionary_en_82_765.txt\",\n            \"/content/symspell-677/frequency_bigramdictionary_en_243_342.txt\"]\n\nyes = True\nno = False\n\nmanage_misspelled_words = yes\nmisspelled_word_method = method[0]\nfreq_dict = freq_dict_list[1].split('/')[-1]\nfreq_dict\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:22:31.376253Z","iopub.execute_input":"2023-10-11T06:22:31.376596Z","iopub.status.idle":"2023-10-11T06:22:31.386066Z","shell.execute_reply.started":"2023-10-11T06:22:31.376564Z","shell.execute_reply":"2023-10-11T06:22:31.384816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (manage_misspelled_words==yes) & (misspelled_word_method == 'py_and_sym'):\n    pyspell_detector = yes\n    symspell_corrector = yes\n    misspell_counter = 0\n\nif (manage_misspelled_words==yes) & (misspelled_word_method == 'pyspell_only'):\n    pyspell_detector = yes\n    symspell_corrector = no\n    misspell_counter = 1\n    \nif (manage_misspelled_words==yes) & (misspelled_word_method == 'symspell_only'):\n    pyspell_detector = no\n    symspell_corrector = yes\n    freq_dict = freq_dict_list[0]\n    misspell_counter = 2","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:10:19.420808Z","iopub.execute_input":"2023-10-11T06:10:19.421109Z","iopub.status.idle":"2023-10-11T06:10:19.433110Z","shell.execute_reply.started":"2023-10-11T06:10:19.421081Z","shell.execute_reply":"2023-10-11T06:10:19.432295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n\nprompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\nprompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\nsummaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\nsummaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\nsample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:10:19.435116Z","iopub.execute_input":"2023-10-11T06:10:19.435414Z","iopub.status.idle":"2023-10-11T06:10:19.532170Z","shell.execute_reply.started":"2023-10-11T06:10:19.435375Z","shell.execute_reply":"2023-10-11T06:10:19.530894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self, \n                model_name: str,\n                ) -> None:\n        if model_name == files[0]:\n            self.tokenizer = AutoTokenizer.from_pretrained(f'/kaggle/input/{model_name}')\n        elif model_name == (files[13] or files[14]):\n            self.tokenizer = T5TokenizerFast.from_pretrained(f'/kaggle/input/transformers/{model_name}')\n        else:\n            self.tokenizer = AutoTokenizer.from_pretrained(f'/kaggle/input/transformers/{model_name}')\n        \n        self.twd = TreebankWordDetokenizer()\n        self.STOP_WORDS = set(stopwords.words('english'))\n        \n        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n        self.speller = Speller(lang='en')\n        self.spellchecker = SpellChecker() \n        \n    def word_overlap_count(self, row):\n        \"\"\" intersection(prompt_text, text) \"\"\"        \n        def check_is_stop_word(word):\n            return word in self.STOP_WORDS\n        \n        prompt_words = row['prompt_tokens']\n        summary_words = row['summary_tokens']\n        if self.STOP_WORDS:\n            prompt_words = list(filter(check_is_stop_word, prompt_words))\n            summary_words = list(filter(check_is_stop_word, summary_words))\n        return len(set(prompt_words).intersection(set(summary_words)))\n            \n    def ngrams(self, token, n):\n        # Use the zip function to help us generate n-grams\n        # Concatentate the tokens into ngrams and return\n        ngrams = zip(*[token[i:] for i in range(n)])\n        return [\" \".join(ngram) for ngram in ngrams]\n\n    def ngram_co_occurrence(self, row, n: int) -> int:\n        # Tokenize the original text and summary into words\n        original_tokens = row['prompt_tokens']\n        summary_tokens = row['summary_tokens']\n\n        # Generate n-grams for the original text and summary\n        original_ngrams = set(self.ngrams(original_tokens, n))\n        summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n        # Calculate the number of common n-grams\n        common_ngrams = original_ngrams.intersection(summary_ngrams)\n        return len(common_ngrams)\n    \n    def ner_overlap_count(self, row, mode:str):\n        model = self.spacy_ner_model\n        def clean_ners(ner_list):\n            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n        prompt = model(row['prompt_text'])\n        summary = model(row['text'])\n\n        if \"spacy\" in str(model):\n            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n        elif \"stanza\" in str(model):\n            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n            summary_ner = set([(token.text, token.type) for token in summary.ents])\n        else:\n            raise Exception(\"Model not supported\")\n\n        prompt_ner = clean_ners(prompt_ner)\n        summary_ner = clean_ners(summary_ner)\n\n        intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n        if mode == \"train\":\n            return ner_dict\n        elif mode == \"test\":\n            return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n    def quotes_count(self, row):\n        summary = row['text']\n        text = row['prompt_text']\n        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n        if len(quotes_from_summary)>0:\n            return [quote in text for quote in quotes_from_summary].count(True)\n        else:\n            return 0\n\n    def spelling(self, text):\n        \n        wordlist=text.split()\n        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n\n        return amount_miss\n    \n    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n        self.spellchecker.word_frequency.load_words(tokens)\n        self.speller.nlp_data.update({token:1000 for token in tokens})\n    \n    def run(self, \n            prompts: pd.DataFrame,\n            summaries:pd.DataFrame,\n            mode:str\n        ) -> pd.DataFrame:\n        \n        # before merge preprocess\n        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n\n        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n        \n        # Add prompt tokens into spelling checker dictionary\n        prompts[\"prompt_tokens\"].apply(\n            lambda x: self.add_spelling_dictionary(x)\n        )\n        \n#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n        # fix misspelling\n        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n            lambda x: self.speller(x)\n        )\n        \n        # count misspelling\n        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n        \n        # merge prompts and summaries\n        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n\n        # after merge preprocess\n        # input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n        \n        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n        input_df['bigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence,args=(2,), axis=1 \n        )\n        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n        \n        input_df['trigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence, args=(3,), axis=1\n        )\n        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n        \n        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        \n        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n    \npreprocessor = Preprocessor(model_name=CFG.model_name)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:10:19.534225Z","iopub.execute_input":"2023-10-11T06:10:19.534554Z","iopub.status.idle":"2023-10-11T06:10:21.852654Z","shell.execute_reply.started":"2023-10-11T06:10:19.534497Z","shell.execute_reply":"2023-10-11T06:10:21.851428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\ntest = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:10:21.854600Z","iopub.execute_input":"2023-10-11T06:10:21.854829Z","iopub.status.idle":"2023-10-11T06:19:39.020965Z","shell.execute_reply.started":"2023-10-11T06:10:21.854803Z","shell.execute_reply":"2023-10-11T06:19:39.019029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pyspellchecker_detector(sentence):\n    sentence = re.sub(r'[^\\w\\s]','',sentence)\n    spell = SpellChecker()\n    tokens = sentence.split(' ')\n    mis_tokens = []\n    for token in spell.unknown(tokens):\n        if token.isalpha():\n            mis_tokens.append(token)\n    return mis_tokens\n\ndef symspellpy_corrector(mis_tokens, freq_dict_):\n    try:\n        sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n        freq_dict = pkg_resources.resource_filename(\"symspellpy\", freq_dict_)\n        sym_spell.load_dictionary(freq_dict, term_index=0, count_index=1)\n        corrected_token = {}\n        for token in tqdm(mis_tokens):\n            terms = sym_spell.lookup_compound(token, \n                                              max_edit_distance=2) \n            if token not in corrected_token.keys():\n                corrected_token[token] = terms[0].term\n        return corrected_token\n\n    except UnicodeDecodeError:\n        return mis_tokens\n\ndef py_sym_checker(df, column, new_col, freq_dict_):\n    try:\n        mis_tokens = []\n        for row_num in tqdm(range(df.shape[0])):\n            sentence = df[column][row_num]\n            for word in pyspellchecker_detector(sentence):\n                mis_tokens.append(word)\n        \n        mis_token_rep = symspellpy_corrector(mis_tokens, freq_dict_)\n        \n        temp = []\n        for row_num in tqdm(range(df.shape[0])):\n            sentence = df[column][row_num]\n            tokens = sentence.split(' ')\n            temp_str = ''\n            for token in tokens:\n                if token in mis_token_rep.keys():\n                    temp_str = temp_str + \" \" + mis_token_rep.get(token)\n                else:\n                    temp_str = temp_str + \" \" + token\n            temp.append(temp_str)\n            \n        # df[column] = pd.Series(temp)\n        return pd.Series(temp)\n    \n    except UnicodeDecodeError:\n        return df[new_col]\n    \ndef symspellpy_correction(df, column, new_col, freq_dict=freq_dict):\n    try:        \n        temp = []\n        for row_num in tqdm(range(df.shape[0])):\n            sentence = df[column][row_num]\n            sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=5)\n            freq_dict = freq_dict\n            sym_spell.load_dictionary(freq_dict, term_index=0, count_index=1)\n            terms = sym_spell.lookup_compound(sentence, \n                                              max_edit_distance=2) \n\n            corrected_sentence = terms[0].term\n            temp.append(corrected_sentence)\n            \n        # df[column] = pd.Series(temp)\n        return pd.Series(temp)\n    \n    except UnicodeDecodeError:\n        return df[new_col]\n    \ndef pyspell_correction(df, column, new_col):\n    try:        \n        temp_total = []\n        for row_num in tqdm(range(df.shape[0])):\n            sentence = df[column][row_num]\n            \n            spell = SpellChecker()\n            tokens = nltk.word_tokenize(sentence)\n            text_length = len(tokens)\n\n            mis_tokens = [token for token in spell.unknown(tokens) if token.isalpha()]\n            temp = []\n            corrected_words = []\n            for word in mis_tokens:\n                corrected_word = spell.correction(word)\n                temp.append({word : corrected_word})\n                corrected_words.append(corrected_word)\n\n            temp_1 = []\n            for word in tokens:\n                for set_ in temp:\n                    if list(set_.keys())[0] == word:\n                        word = list(set_.values())[0]\n                        if word in temp_1:\n                            continue\n                        else:\n                            temp_1.append(word)\n                if word in temp_1:\n                    continue\n                else:\n                    temp_1.append(word)\n\n            corrected_sentence = ''\n            for word in temp_1:\n                try:\n                    if (word.isalpha() or word.isnumeric()) == True:\n                        corrected_sentence = corrected_sentence + word + ' '\n                    elif word in [',', '.', '\"', \"'\", '(', ')', '[', ']', '{', '}']:\n                        corrected_sentence = corrected_sentence + word\n                    else:\n                        corrected_sentence = corrected_sentence + word\n                except:\n                    continue\n            corrected_sentence = corrected_sentence.replace('  ', ' ').strip()\n            temp_total.append(corrected_sentence)\n            \n        # df[column] = pd.Series(temp_total)\n        return pd.Series(temp_total)\n    except UnicodeDecodeError:\n        return df[new_col]","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:19:39.022869Z","iopub.execute_input":"2023-10-11T06:19:39.023250Z","iopub.status.idle":"2023-10-11T06:19:39.045619Z","shell.execute_reply.started":"2023-10-11T06:19:39.023215Z","shell.execute_reply":"2023-10-11T06:19:39.043968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if manage_misspelled_words:\n    train_0 = train.copy()\n    test_0 = test.copy()\n    train_0['symspell_corr'] = py_sym_checker(train, 'text', 'symspell_corr',freq_dict)\n    print(f\"py & sym train_processed['text'][0] =\\n{train_0['symspell_corr'][0]}\")\n    print()\n    test_0['symspell_corr'] = py_sym_checker(test, 'text', 'symspell_corr',freq_dict)\n    print(f\"py & sym test_processed['text'][0] =\\n{test_0['symspell_corr'][0]}\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:22:37.522555Z","iopub.execute_input":"2023-10-11T06:22:37.522846Z","iopub.status.idle":"2023-10-11T06:32:26.385831Z","shell.execute_reply.started":"2023-10-11T06:22:37.522819Z","shell.execute_reply":"2023-10-11T06:32:26.384779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if manage_misspelled_words:\n\n    train_1 = train.copy()\n    test_1 = test.copy()\n    train_1['symspell_corr'] = pyspell_correction(train, 'text', 'symspell_corr')\n    print(f\"py only train_processed['text'][0] =\\n{train_1['symspell_corr'][0]}\")\n    print()\n    test_1['symspell_corr'] = pyspell_correction(test, 'text', 'symspell_corr')\n    print(f\"py only test_processed['text'][0] =\\n{test_1['symspell_corr'][0]}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:32:26.388326Z","iopub.execute_input":"2023-10-11T06:32:26.388634Z","iopub.status.idle":"2023-10-11T06:50:42.316371Z","shell.execute_reply.started":"2023-10-11T06:32:26.388602Z","shell.execute_reply":"2023-10-11T06:50:42.314688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if manage_misspelled_words:\n\n    train_2 = train.copy()\n    test_2 = test.copy()\n    train_2['symspell_corr'] = symspellpy_correction(train, 'text', 'symspell_corr', freq_dict)\n    print(f\"sym only train_processed['text'][0] =\\n{train_2['symspell_corr'][0]}\")\n    print()\n    test_2['symspell_corr'] = symspellpy_correction(test, 'text', 'symspell_corr', freq_dict)\n    print(f\"sym only test_processed['text'][0] =\\n{test_2['symspell_corr'][0]}\")\n    \nelse:\n    \n    print('no change')","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:50:42.318379Z","iopub.execute_input":"2023-10-11T06:50:42.318680Z","iopub.status.idle":"2023-10-11T06:50:57.934467Z","shell.execute_reply.started":"2023-10-11T06:50:42.318651Z","shell.execute_reply":"2023-10-11T06:50:57.932982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:50:57.938375Z","iopub.execute_input":"2023-10-11T06:50:57.939151Z","iopub.status.idle":"2023-10-11T06:50:57.948380Z","shell.execute_reply.started":"2023-10-11T06:50:57.939104Z","shell.execute_reply":"2023-10-11T06:50:57.946849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_0.head(5))\ndisplay(test_0.head(5))","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:50:57.950105Z","iopub.execute_input":"2023-10-11T06:50:57.950425Z","iopub.status.idle":"2023-10-11T06:50:57.990262Z","shell.execute_reply.started":"2023-10-11T06:50:57.950394Z","shell.execute_reply":"2023-10-11T06:50:57.989484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_1.head(5))\ndisplay(test_1.head(5))","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:50:57.991616Z","iopub.execute_input":"2023-10-11T06:50:57.992414Z","iopub.status.idle":"2023-10-11T06:50:58.035261Z","shell.execute_reply.started":"2023-10-11T06:50:57.992384Z","shell.execute_reply":"2023-10-11T06:50:58.033521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_2.head(5))\ndisplay(test_2.head(5))","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:50:58.037021Z","iopub.execute_input":"2023-10-11T06:50:58.037343Z","iopub.status.idle":"2023-10-11T06:50:58.075215Z","shell.execute_reply.started":"2023-10-11T06:50:58.037299Z","shell.execute_reply":"2023-10-11T06:50:58.073846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.shape)\ndisplay(test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:50:58.077154Z","iopub.execute_input":"2023-10-11T06:50:58.077479Z","iopub.status.idle":"2023-10-11T06:50:58.090310Z","shell.execute_reply.started":"2023-10-11T06:50:58.077439Z","shell.execute_reply":"2023-10-11T06:50:58.088879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.to_csv('original_train.csv', index=False)\ntest.to_csv('original_test.csv', index=False)\n\ntrain_0.to_csv('py_sym_train.csv', index=False)\ntest_0.to_csv('py_sym_test.csv', index=False)\ntrain_1.to_csv('py_only_train.csv', index=False)\ntest_1.to_csv('py_only_test.csv', index=False)\ntrain_2.to_csv('sym_only_train.csv', index=False)\ntest_2.to_csv('sym_only_test.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T06:50:58.091843Z","iopub.execute_input":"2023-10-11T06:50:58.092111Z","iopub.status.idle":"2023-10-11T06:51:01.746854Z","shell.execute_reply.started":"2023-10-11T06:50:58.092083Z","shell.execute_reply":"2023-10-11T06:51:01.745735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}