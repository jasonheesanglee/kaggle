{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c53e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from custom_dataset import CustomDataSet\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoModelForAudioClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from custom_dataset import CustomDataSet\n",
    "'''\n",
    "                            speech_file_to_array_fn as sfaf,\n",
    "                            collate_fn,\n",
    "                            create_data_loader,\n",
    "                            validation,\n",
    "                            train'''\n",
    "from transformers import (Wav2Vec2FeatureExtractor,\n",
    "                          Wav2Vec2Model,\n",
    "                          Wav2Vec2Config,\n",
    "                          Wav2Vec2ConformerForSequenceClassification,\n",
    "                          AutoModelForAudioClassification)\n",
    "import evaluate\n",
    "import librosa\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset, Audio\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680ed990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "import librosa\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datasets import load_dataset, Audio\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb79e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'SR':16000,\n",
    "    'N_MFCC':32, # Melspectrogram 벡터를 추출할 개수\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'SR':16_000,\n",
    "    'SEED':42,\n",
    "    'BATCH_SIZE':8, # out of Memory가 발생하면 줄여주세요\n",
    "    'TOTAL_BATCH_SIZE':32, # 원하는 batch size\n",
    "    'EPOCHS':1,\n",
    "    'LR':1e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/wav2vec2-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc611872",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(train_df, test_size=0.2, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a45d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(df):\n",
    "    feature = []\n",
    "    for path in tqdm(df['path']):\n",
    "        speech_array, _ = librosa.load(path, sr=CFG['SR'])\n",
    "        feature.append(speech_array)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd1de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = speech_file_to_array_fn(train_df)\n",
    "valid_x = speech_file_to_array_fn(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801016c4",
   "metadata": {},
   "source": [
    "class CustomDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y, processor):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_values = self.processor(self.x[idx], sampling_rate=CFG['SR'], return_tensors=\"pt\", padding=True).input_values\n",
    "        if self.y is not None:\n",
    "            return input_values.squeeze(), self.y[idx]\n",
    "        else:\n",
    "            return input_values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x, y = zip(*batch)\n",
    "    x = pad_sequence([torch.tensor(xi) for xi in x], batch_first=True)\n",
    "    y = pad_sequence([torch.tensor([yi]) for yi in y], batch_first=True)  # Convert scalar targets to 1D tensors\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee28b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(dataset, batch_size, shuffle, collate_fn, num_workers=0):\n",
    "    return DataLoader(dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      collate_fn=collate_fn,\n",
    "                      num_workers=num_workers\n",
    "                      )\n",
    "\n",
    "train_dataset = CustomDataSet(train_x, train_df['label'], processor)\n",
    "valid_dataset = CustomDataSet(valid_x, valid_df['label'], processor)\n",
    "\n",
    "train_loader = create_data_loader(train_dataset, CFG['BATCH_SIZE'], False, collate_fn, 16)\n",
    "valid_loader = create_data_loader(valid_dataset, CFG['BATCH_SIZE'], False, collate_fn, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675950e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model = AutoModelForAudioClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f952c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.model = audio_model\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.classifier = nn.Linear(256, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        output = self.classifier(output.logits)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ef0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, creterion):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(iter(valid_loader)):\n",
    "            x = x.to(device)\n",
    "            y = y.flatten().to(device)\n",
    "\n",
    "            output = model(x)\n",
    "            loss = creterion(output, y)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).cpu().sum()\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    avg_loss = np.mean(val_loss)\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, optimizer, scheduler):\n",
    "    accumulation_step = int(CFG['TOTAL_BATCH_SIZE'] / CFG['BATCH_SIZE'])\n",
    "    model.to(device)\n",
    "    creterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        for i, (x, y) in enumerate(tqdm(train_loader)):\n",
    "            x = x.to(device)\n",
    "            y = y.flatten().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(x)\n",
    "            loss = creterion(output, y)\n",
    "            loss.backward()\n",
    "\n",
    "            if (i+1) % accumulation_step == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        avg_loss = np.mean(train_loss)\n",
    "        valid_loss, valid_acc = validation(model, valid_loader, creterion)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_acc)\n",
    "\n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_model = model\n",
    "\n",
    "        print(f'epoch:[{epoch}] train loss:[{avg_loss:.5f}] valid_loss:[{valid_loss:.5f}] valid_acc:[{valid_acc:.5f}]')\n",
    "    \n",
    "    print(f'best_acc:{best_acc:.5f}')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa68446",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CFG['LR'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "infer_model = train(model, train_loader, valid_loader, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbde5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_test(batch):\n",
    "    x = pad_sequence([torch.tensor(xi) for xi in batch], batch_first=True)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a23dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = speech_file_to_array_fn(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f283c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataSet(test_x, y=None, processor=processor)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, collate_fn=collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef907f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x in tqdm(iter(test_loader)):\n",
    "            x = x.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "\n",
    "            preds += output.argmax(-1).detach().cpu().numpy().tolist()\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = inference(infer_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission['label'] = preds\n",
    "submission.to_csv('./baseline_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827f09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dcf1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87851bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff3f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938189e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5001 entries, 0 to 5000\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      5001 non-null   object\n",
      " 1   path    5001 non-null   object\n",
      " 2   label   5001 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 117.3+ KB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1881 entries, 0 to 1880\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      1881 non-null   object\n",
      " 1   path    1881 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 29.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./train.csv')\n",
    "print(train_df.info())\n",
    "print()\n",
    "test_df = pd.read_csv('./test.csv')\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfbe00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Locations\n",
    "dataset = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65c378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_feature(df):\n",
    "    features = []\n",
    "    for path in tqdm(df['path']):\n",
    "        # librosa패키지를 사용하여 wav 파일 load\n",
    "        y, sr = librosa.load(path, sr=CFG['SR'])\n",
    "        # librosa패키지를 사용하여 mfcc 추출\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CFG['N_MFCC'])\n",
    "        y_feature = []\n",
    "        # 추출된 MFCC들의 평균을 Feature로 사용\n",
    "        for e in mfcc:\n",
    "            y_feature.append(np.mean(e))\n",
    "        features.append(y_feature)\n",
    "\n",
    "    mfcc_df = pd.DataFrame(features, columns=['mfcc_'+str(x) for x in range(1,CFG['N_MFCC']+1)])\n",
    "    return mfcc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c277e927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e042feec5fe4e72a1f71a8cc37c819b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          mfcc_1      mfcc_2     mfcc_3     mfcc_4     mfcc_5    mfcc_6  \\\n",
      "0    -414.755737  110.100639  46.699074  23.939814  14.766221  4.820827   \n",
      "1    -399.769531   83.051300  55.473316  31.782587  22.040754  0.985082   \n",
      "2    -341.145081   97.399071  38.274349  19.811539   0.731027  0.838704   \n",
      "3    -376.963715  118.961670  34.490349  24.178417  -1.065604 -1.613391   \n",
      "4    -352.863220  117.553337  29.948687  31.094315   5.406391 -5.591998   \n",
      "...          ...         ...        ...        ...        ...       ...   \n",
      "4996 -416.181305  112.938484  47.294231  23.111433  16.048231  6.795750   \n",
      "4997 -237.811432   72.207787   3.371584  14.646128 -10.546066  7.671333   \n",
      "4998 -368.228119  103.887871  40.081417  21.763754  14.120455  5.778781   \n",
      "4999 -407.114288  103.880676  44.980690  23.265039  13.279220  6.904113   \n",
      "5000 -359.754608   89.241714  20.517361  24.043827  16.926319 -7.636394   \n",
      "\n",
      "        mfcc_7     mfcc_8    mfcc_9    mfcc_10  ...   mfcc_23   mfcc_24  \\\n",
      "0     1.445079  -0.926153  2.892509   4.465990  ... -2.615444 -1.620636   \n",
      "1     3.712760  -2.359319 -0.026561   0.393656  ...  1.074466  1.944394   \n",
      "2    -1.911201 -10.645283 -1.595291  -2.305276  ... -1.521030  1.525717   \n",
      "3     2.888371  -7.865876 -2.260846  -5.609926  ... -1.049496 -3.562966   \n",
      "4    -4.809399 -11.500415 -6.894947   2.946273  ... -0.323433  0.386383   \n",
      "...        ...        ...       ...        ...  ...       ...       ...   \n",
      "4996  0.194628  -3.153772 -5.016596  -7.414401  ... -1.217834 -2.810278   \n",
      "4997 -1.676355  -8.311659  5.224692  -3.619729  ...  0.276754  1.673036   \n",
      "4998 -3.776188  -4.852784 -3.689436  -0.837069  ...  0.427912  1.294595   \n",
      "4999 -1.792801   3.526099  2.991546  -1.412130  ...  0.797132  1.962759   \n",
      "5000 -6.266701  -0.879193 -1.824082 -10.422786  ... -2.481820 -5.475562   \n",
      "\n",
      "       mfcc_25   mfcc_26   mfcc_27   mfcc_28   mfcc_29   mfcc_30   mfcc_31  \\\n",
      "0    -1.285819 -1.538144 -1.629623 -0.110236  0.022208  3.912367  4.398968   \n",
      "1     0.017539 -0.097045  0.061731  1.764198  0.311808  2.248236  0.097092   \n",
      "2     0.446549 -0.879012 -0.619363  3.587360  4.218766  3.195291 -0.777671   \n",
      "3    -3.667920 -2.209875 -0.537536 -1.000548 -3.482561 -2.733938 -0.503050   \n",
      "4    -3.801968 -2.157103 -0.022309  1.282429  1.643818  2.032169 -0.533625   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "4996 -3.939800 -4.212123 -1.823620  0.406775  0.249249  0.091639 -1.120363   \n",
      "4997 -2.383266 -1.866506 -4.395771 -1.290971 -0.243260 -0.217550  0.025551   \n",
      "4998  1.419106  2.330357  1.584116  2.376555  0.746695  3.503192  1.551537   \n",
      "4999  0.478440 -1.340471  0.208459  3.158219  2.288530  1.665004 -0.770613   \n",
      "5000 -1.680754 -2.730674 -3.256532 -0.626901 -2.973576  1.013279  1.561999   \n",
      "\n",
      "       mfcc_32  \n",
      "0     2.815795  \n",
      "1    -0.008172  \n",
      "2    -0.989654  \n",
      "3    -0.844000  \n",
      "4    -1.729753  \n",
      "...        ...  \n",
      "4996 -1.970424  \n",
      "4997  0.289345  \n",
      "4998 -0.069177  \n",
      "4999  0.064017  \n",
      "5000 -2.942966  \n",
      "\n",
      "[5001 rows x 32 columns]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36f39c2a240427c92a1e0bbe843657e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          mfcc_1      mfcc_2     mfcc_3     mfcc_4     mfcc_5    mfcc_6  \\\n",
      "0    -335.757324  125.215431  22.145767  14.351713  -1.045251  0.567860   \n",
      "1    -295.973053   92.839684  24.976181  22.831310 -10.278670  7.804742   \n",
      "2    -444.395996  109.385201  55.236771  26.486050  12.487712  9.069915   \n",
      "3    -384.600220  112.463974  47.454556  17.336460  13.556947  4.677102   \n",
      "4    -273.304077   97.817047  12.370095  24.591480   3.248469 -9.987856   \n",
      "...          ...         ...        ...        ...        ...       ...   \n",
      "1876 -250.254913   69.372955  17.328987  12.005389  -2.463175 -7.652928   \n",
      "1877 -348.593842  107.067047  27.683287  17.207047   2.634121  1.812742   \n",
      "1878 -295.658112   99.606911  -9.669126   4.676853   4.752311 -5.689676   \n",
      "1879 -450.677094  122.865677  46.413559  21.616255  13.785479  5.433998   \n",
      "1880 -366.914154  113.387276  43.263287  15.806440   8.695131  6.917760   \n",
      "\n",
      "        mfcc_7     mfcc_8    mfcc_9    mfcc_10  ...   mfcc_23   mfcc_24  \\\n",
      "0    -0.666742  -8.114830 -7.136742  -0.890950  ... -0.452362 -1.410518   \n",
      "1    -7.779604 -11.835019 -9.290084  -2.127427  ...  1.778805  2.596544   \n",
      "2     2.790433  -0.485667 -2.200089  -3.334617  ... -0.988734 -3.927215   \n",
      "3    -5.572524  -1.354830 -2.386853   0.361085  ... -1.371162 -1.021333   \n",
      "4    -6.246259 -13.113685 -1.204894   0.282557  ...  2.007670 -1.133698   \n",
      "...        ...        ...       ...        ...  ...       ...       ...   \n",
      "1876 -5.683615  -2.965508  1.244487  -1.624788  ...  1.999792 -3.934587   \n",
      "1877 -1.466079  -5.889251  3.410136   1.865329  ...  0.566558  5.903645   \n",
      "1878  1.521859  -8.851722 -3.052611 -10.759281  ... -4.568923 -1.418633   \n",
      "1879 -2.776925  -0.925962 -1.236971  -5.947121  ...  2.544887 -1.345986   \n",
      "1880 -7.379910  -4.027730 -3.587318  -1.590748  ... -3.402888 -2.253915   \n",
      "\n",
      "       mfcc_25   mfcc_26   mfcc_27   mfcc_28   mfcc_29   mfcc_30   mfcc_31  \\\n",
      "0    -2.177118  0.198908  1.068798  0.964545 -2.613074  2.812386  0.901755   \n",
      "1    -1.626948  0.363650 -1.242502 -1.523350  0.161729  2.599890  0.549057   \n",
      "2    -4.373907 -1.453163 -3.116022  0.184532 -3.222616 -0.724788 -1.841568   \n",
      "3    -3.315773 -2.259278 -1.717189 -0.019883 -0.051858  0.199551  2.846354   \n",
      "4     1.177092 -0.153630 -1.755448  1.917316 -2.084666 -1.174302 -0.747732   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1876 -0.296250 -0.949789 -0.430385 -2.024839 -3.166185  3.210045  2.162896   \n",
      "1877  3.492442  1.928877  0.612623 -0.503881  0.614712  3.338864  1.078280   \n",
      "1878 -6.151373 -2.744540 -3.983426 -5.843061 -1.557026  1.383257  0.714763   \n",
      "1879 -1.827730 -2.109568 -2.891415 -0.612319 -2.212980 -1.055952 -1.289654   \n",
      "1880 -2.139054 -4.618435 -0.188326  0.892588 -4.297170 -0.811239  2.563804   \n",
      "\n",
      "       mfcc_32  \n",
      "0     1.809820  \n",
      "1    -0.346372  \n",
      "2    -4.278657  \n",
      "3     5.373621  \n",
      "4    -1.348814  \n",
      "...        ...  \n",
      "1876  1.188407  \n",
      "1877  1.294158  \n",
      "1878 -2.767536  \n",
      "1879 -1.245361  \n",
      "1880 -0.798751  \n",
      "\n",
      "[1881 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "train_x = get_mfcc_feature(train_df)\n",
    "print(train_x)\n",
    "test_x = get_mfcc_feature(test_df)\n",
    "print(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f168f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5001 entries, 0 to 5000\n",
      "Data columns (total 32 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   mfcc_1   5001 non-null   float32\n",
      " 1   mfcc_2   5001 non-null   float32\n",
      " 2   mfcc_3   5001 non-null   float32\n",
      " 3   mfcc_4   5001 non-null   float32\n",
      " 4   mfcc_5   5001 non-null   float32\n",
      " 5   mfcc_6   5001 non-null   float32\n",
      " 6   mfcc_7   5001 non-null   float32\n",
      " 7   mfcc_8   5001 non-null   float32\n",
      " 8   mfcc_9   5001 non-null   float32\n",
      " 9   mfcc_10  5001 non-null   float32\n",
      " 10  mfcc_11  5001 non-null   float32\n",
      " 11  mfcc_12  5001 non-null   float32\n",
      " 12  mfcc_13  5001 non-null   float32\n",
      " 13  mfcc_14  5001 non-null   float32\n",
      " 14  mfcc_15  5001 non-null   float32\n",
      " 15  mfcc_16  5001 non-null   float32\n",
      " 16  mfcc_17  5001 non-null   float32\n",
      " 17  mfcc_18  5001 non-null   float32\n",
      " 18  mfcc_19  5001 non-null   float32\n",
      " 19  mfcc_20  5001 non-null   float32\n",
      " 20  mfcc_21  5001 non-null   float32\n",
      " 21  mfcc_22  5001 non-null   float32\n",
      " 22  mfcc_23  5001 non-null   float32\n",
      " 23  mfcc_24  5001 non-null   float32\n",
      " 24  mfcc_25  5001 non-null   float32\n",
      " 25  mfcc_26  5001 non-null   float32\n",
      " 26  mfcc_27  5001 non-null   float32\n",
      " 27  mfcc_28  5001 non-null   float32\n",
      " 28  mfcc_29  5001 non-null   float32\n",
      " 29  mfcc_30  5001 non-null   float32\n",
      " 30  mfcc_31  5001 non-null   float32\n",
      " 31  mfcc_32  5001 non-null   float32\n",
      "dtypes: float32(32)\n",
      "memory usage: 625.2 KB\n",
      "None\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1881 entries, 0 to 1880\n",
      "Data columns (total 32 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   mfcc_1   1881 non-null   float32\n",
      " 1   mfcc_2   1881 non-null   float32\n",
      " 2   mfcc_3   1881 non-null   float32\n",
      " 3   mfcc_4   1881 non-null   float32\n",
      " 4   mfcc_5   1881 non-null   float32\n",
      " 5   mfcc_6   1881 non-null   float32\n",
      " 6   mfcc_7   1881 non-null   float32\n",
      " 7   mfcc_8   1881 non-null   float32\n",
      " 8   mfcc_9   1881 non-null   float32\n",
      " 9   mfcc_10  1881 non-null   float32\n",
      " 10  mfcc_11  1881 non-null   float32\n",
      " 11  mfcc_12  1881 non-null   float32\n",
      " 12  mfcc_13  1881 non-null   float32\n",
      " 13  mfcc_14  1881 non-null   float32\n",
      " 14  mfcc_15  1881 non-null   float32\n",
      " 15  mfcc_16  1881 non-null   float32\n",
      " 16  mfcc_17  1881 non-null   float32\n",
      " 17  mfcc_18  1881 non-null   float32\n",
      " 18  mfcc_19  1881 non-null   float32\n",
      " 19  mfcc_20  1881 non-null   float32\n",
      " 20  mfcc_21  1881 non-null   float32\n",
      " 21  mfcc_22  1881 non-null   float32\n",
      " 22  mfcc_23  1881 non-null   float32\n",
      " 23  mfcc_24  1881 non-null   float32\n",
      " 24  mfcc_25  1881 non-null   float32\n",
      " 25  mfcc_26  1881 non-null   float32\n",
      " 26  mfcc_27  1881 non-null   float32\n",
      " 27  mfcc_28  1881 non-null   float32\n",
      " 28  mfcc_29  1881 non-null   float32\n",
      " 29  mfcc_30  1881 non-null   float32\n",
      " 30  mfcc_31  1881 non-null   float32\n",
      " 31  mfcc_32  1881 non-null   float32\n",
      "dtypes: float32(32)\n",
      "memory usage: 235.2 KB\n",
      "None\n",
      "0       1\n",
      "1       2\n",
      "2       4\n",
      "3       5\n",
      "4       4\n",
      "       ..\n",
      "4996    5\n",
      "4997    0\n",
      "4998    1\n",
      "4999    1\n",
      "5000    4\n",
      "Name: label, Length: 5001, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_x.info())\n",
    "print('\\n \\n \\n')\n",
    "print(test_x.info())\n",
    "train_y = train_df['label']\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66db3670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 32) (1001, 32) (4000,) (1001,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.2, random_state=CFG['SEED'])\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e11c247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename.iloc[train_max] =         filename\n",
      "222  Results5819, \n",
      " \n",
      " train_acc.iloc[train_max] =      train_acc\n",
      "222      0.452\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1 entries, 222 to 222\n",
      "Data columns (total 1 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   filename  1 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 16.0+ bytes\n",
      "filename.iloc[val_max].info() = None\n",
      "./Results/Results5819.csv\n",
      "This is LGBM\n"
     ]
    }
   ],
   "source": [
    "result_files = glob.glob(dataset + \"Results/*.csv\")\n",
    "\n",
    "max_temp = []\n",
    "for file in result_files:\n",
    "    filename = file.split(\".\")[1]\n",
    "    filename = filename.split(\"/\")[2]\n",
    "    df = pd.read_csv(file)\n",
    "    train_acc = pd.DataFrame(df[\"0\"])\n",
    "    val_acc = pd.DataFrame(df[\"1\"])\n",
    "    train_max = train_acc.idxmax()\n",
    "    val_max = val_acc.idxmax()\n",
    "    if train_max[-1] == val_max[-1]:\n",
    "        max_temp.append([filename, train_acc.iloc[train_max[-1]][-1], val_acc.iloc[val_max[-1]][-1]])\n",
    "\n",
    "max_temp_df = pd.DataFrame(max_temp)\n",
    "max_temp_df = max_temp_df.rename(columns={0:'filename', 1:\"train_acc\", 2:\"val_acc\"})\n",
    "\n",
    "filename = pd.DataFrame(max_temp_df['filename'])\n",
    "train_acc = pd.DataFrame(max_temp_df['train_acc'])\n",
    "val_acc = pd.DataFrame(max_temp_df['val_acc'])\n",
    "train_max = train_acc.idxmax()\n",
    "val_max = val_acc.idxmax()\n",
    "if train_max[-1] == val_max[-1]:\n",
    "    print(filename.iloc[train_max], train_acc.iloc[train_max], val_acc.iloc[val_max])\n",
    "    val_max_filepath = dataset + \"Results/\" + filename.iloc[val_max].iat[0,0] +\".csv\"\n",
    "    maxfile = pd.read_csv(filepath)\n",
    "    max_max_train = pd.DataFrame(maxfile[\"0\"])\n",
    "    max_max_val= pd.DataFrame(maxfile[\"1\"])\n",
    "    train_max_max = max_max_train.idxmax()[-1]\n",
    "    val_max_max = max_max_val.idxmax()[-1]\n",
    "    if train_max_max == 0: # Random Forest\n",
    "        rf = RandomForestClassifier(max_depth=maxfile[\"max_depth\"].iat[0],\n",
    "                                    n_estimators=int(maxfile[\"n_estimators\"].iat[0]),\n",
    "                                    min_samples_split=2,\n",
    "                                    max_features=maxfile[\"max_features\"].iat[0],\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=CFG['SEED']\n",
    "                                   )\n",
    "        rf.fit(X_train, y_train)\n",
    "        X_test = pd.get_dummies(data=test_x)\n",
    "        rf_pred = rf.predict(X_test)\n",
    "        rf_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "        rf_submission['label'] = rf_pred\n",
    "        rf_submission.to_csv(dataset + \"rf_submission.csv\", index=False)\n",
    "    elif train_max_max == 1: # Decision Tree\n",
    "        dt = DecisionTreeClassifier(max_depth=maxfile[\"max_depth\"].iat[1],\n",
    "                                    min_samples_split=2,\n",
    "                                    max_features=maxfile[\"max_features\"].iat[1],\n",
    "                                    random_state=CFG['SEED']\n",
    "                                   )\n",
    "        dt.fit(X_train, y_train)\n",
    "        X_test = pd.get_dummies(data=test_x)\n",
    "        dt_pred = dt.predict(X_test)\n",
    "        dt_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "        dt_submission['label'] = dt_pred\n",
    "        dt_submission.to_csv(dataset + \"dt_submission.csv\", index=False)\n",
    "    elif train_max_max == 2: # XG Boost\n",
    "        xgboost = XGBClassifier(max_depth=maxfile['max_depth'].iat[2],\n",
    "                                n_estimators=int(maxfile[\"n_estimators\"].iat[2]),\n",
    "                                grow_policy='depthwise',\n",
    "                                n_jobs=-1,\n",
    "                                random_state=CFG['SEED'],\n",
    "                                tree_method='auto'\n",
    "                               )\n",
    "        xgboost.fit(X_train, y_train)\n",
    "        X_test = pd.get_dummies(data=test_x)\n",
    "        xgboost_pred = xgboost.predict(X_test)\n",
    "        xgboost_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "        xgboost_submission['label'] = xgboost_pred\n",
    "        xgboost_submission.to_csv(dataset + \"xgboost_submission.csv\", index=False)\n",
    "    else:\n",
    "#         lgbm = LGBMClassifier(max_depth=maxfile['max_depth'].iat[3],\n",
    "#                             n_estimators=int(maxfile[\"n_estimators\"].iat[3]),\n",
    "#                             max_features=maxfile[\"max_features\"].iat[3],\n",
    "#                             n_jobs=-1,\n",
    "#                             num_leaves=int(maxfile[\"num_leaves\"].iat[3]),\n",
    "#                             random_state=CFG['SEED']\n",
    "#                             )\n",
    "#         lgbm.fit(X_train, y_train)\n",
    "#         X_test = pd.get_dummies(data=test_x)\n",
    "#         lgbm_pred = lgbm.predict(X_test)\n",
    "#         lgbm_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "#         lgbm_submission['label'] = lgbm_pred\n",
    "#         lgbm_submission.to_csv(dataset + \"lgbm_max_submission.csv\", index=False)\n",
    "        print(\"This is LGBM\")\n",
    "else:\n",
    "    print(f\"filename.iloc[train_max] = {filename.iloc[train_max]}, \\n \\n train_acc.iloc[train_max] = {train_acc.iloc[train_max]}\")\n",
    "    print()\n",
    "    print()\n",
    "#     print(f\"filename.iloc[val_max][1] = {filename.iloc[val_max][1]}\")\n",
    "    print(f\"filename.iloc[train_max].info() = {filename.iloc[train_max].info()}\")\n",
    "    train_max_filepath = dataset + \"Results/\" + filename.iloc[train_max].iat[0,0] +\".csv\"\n",
    "    print(train_max_filepath)\n",
    "    train_maxfile = pd.read_csv(train_max_filepath)\n",
    "    val_max_train = pd.DataFrame(train_maxfile[\"0\"])\n",
    "    val_max_val= pd.DataFrame(train_maxfile[\"1\"])\n",
    "    train_max_max = val_max_train.idxmax()[-1]\n",
    "    val_max_max = val_max_val.idxmax()[-1]\n",
    "    if train_max_max == 0: # Random Forest\n",
    "        rf = RandomForestClassifier(max_depth=train_maxfile[\"max_depth\"].iat[0],\n",
    "                                    n_estimators=int(train_maxfile[\"n_estimators\"].iat[0]),\n",
    "                                    min_samples_split=2,\n",
    "                                    max_features=train_maxfile[\"max_features\"].iat[0],\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=CFG['SEED']\n",
    "                                   )\n",
    "        rf.fit(X_train, y_train)\n",
    "        X_test = pd.get_dummies(data=test_x)\n",
    "        rf_pred = rf.predict(X_test)\n",
    "        rf_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "        rf_submission['label'] = rf_pred\n",
    "        rf_submission.to_csv(dataset + \"rf_max_submission.csv\", index=False)\n",
    "    elif train_max_max == 1: # Decision Tree\n",
    "        dt = DecisionTreeClassifier(max_depth=train_maxfile[\"max_depth\"].iat[1],\n",
    "                                    min_samples_split=2,\n",
    "                                    max_features=train_maxfile[\"max_features\"].iat[1],\n",
    "                                    random_state=CFG['SEED']\n",
    "                                   )\n",
    "        dt.fit(X_train, y_train)\n",
    "        X_test = pd.get_dummies(data=test_x)\n",
    "        dt_pred = dt.predict(X_test)\n",
    "        dt_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "        dt_submission['label'] = dt_pred\n",
    "        dt_submission.to_csv(dataset + \"dt_max_submission.csv\", index=False)\n",
    "    elif train_max_max == 2: # XG Boost\n",
    "        xgboost = XGBClassifier(max_depth=train_maxfile['max_depth'].iat[2],\n",
    "                                n_estimators=int(train_maxfile[\"n_estimators\"].iat[2]),\n",
    "                                grow_policy='depthwise',\n",
    "                                n_jobs=-1,\n",
    "                                random_state=CFG['SEED'],\n",
    "                                tree_method='auto'\n",
    "                               )\n",
    "        xgboost.fit(X_train, y_train)\n",
    "        X_test = pd.get_dummies(data=test_x)\n",
    "        xgboost_pred = xgboost.predict(X_test)\n",
    "        xgboost_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "        xgboost_submission['label'] = xgboost_pred\n",
    "        xgboost_submission.to_csv(dataset + \"xgboost_max_submission.csv\", index=False)\n",
    "    else:\n",
    "#         lgbm = LGBMClassifier(max_depth=val_maxfile['max_depth'].iat[3],\n",
    "#                             n_estimators=int(val_maxfile[\"n_estimators\"].iat[3]),\n",
    "#                             max_features=val_maxfile[\"max_features\"].iat[3],\n",
    "#                             n_jobs=-1,\n",
    "#                             num_leaves=int(val_maxfile[\"num_leaves\"].iat[3]),\n",
    "#                             random_state=CFG['SEED']\n",
    "#                             )\n",
    "#         lgbm.fit(X_train, y_train)\n",
    "#         X_test = pd.get_dummies(data=test_x)\n",
    "#         lgbm_pred = lgbm.predict(X_test)\n",
    "#         lgbm_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "#         lgbm_submission['label'] = lgbm_pred\n",
    "#         lgbm_submission.to_csv(dataset + \"lgbm_max_submission.csv\", index=False)\n",
    "        print(\"This is LGBM\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2988cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       filename\n",
      "102  Results712      train_acc\n",
      "102      0.171\n"
     ]
    }
   ],
   "source": [
    "min_temp = []\n",
    "for file in result_files:\n",
    "    filename = file.split(\".\")[1]\n",
    "    filename = filename.split(\"/\")[2]\n",
    "    df = pd.read_csv(file)\n",
    "    train_acc = pd.DataFrame(df[\"0\"])\n",
    "    val_acc = pd.DataFrame(df[\"1\"])\n",
    "    train_min = train_acc.idxmin()\n",
    "    val_min = val_acc.idxmin()\n",
    "    if train_min[-1] == val_min[-1]:\n",
    "        min_temp.append([filename, train_acc.iloc[train_min[-1]][-1], val_acc.iloc[val_min[-1]][-1]])\n",
    "\n",
    "min_temp_df = pd.DataFrame(min_temp)\n",
    "min_temp_df = min_temp_df.rename(columns={0:'filename', 1:\"train_acc\", 2:\"val_acc\"})\n",
    "\n",
    "filename = pd.DataFrame(min_temp_df['filename'])\n",
    "train_acc = pd.DataFrame(min_temp_df['train_acc'])\n",
    "val_acc = pd.DataFrame(min_temp_df['val_acc'])\n",
    "train_min = train_acc.idxmin()\n",
    "val_min = val_acc.idxmin()\n",
    "if train_min[-1] == val_min[-1]:\n",
    "#     print(filename.iloc[train_min], train_acc.iloc[train_min], val_acc.iloc[val_min])\n",
    "    val_min_filepath = dataset + \"Results/\" + filename.iloc[val_min].iat[0,0] +\".csv\"\n",
    "    minfile = pd.read_csv(filepath)\n",
    "    min_min_train = pd.DataFrame(minfile[\"0\"])\n",
    "    min_min_val= pd.DataFrame(minfile[\"1\"])\n",
    "    train_min_min = min_min_train.idxmin()[-1]\n",
    "    val_min_min = min_min_val.idxmin()[-1]\n",
    "    if train_min_min == 0: # Random Forest\n",
    "        rf = RandomForestClassifier(max_depth=minfile[\"max_depth\"].iat[0],\n",
    "                                    n_estimators=int(minfile[\"n_estimators\"].iat[0]),\n",
    "                                    min_samples_split=2,\n",
    "                                    max_features=minfile[\"max_features\"].iat[0],\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=CFG['SEED']\n",
    "                                   )\n",
    "        rf.fit(X_train, y_train)\n",
    "        X_test = pd.get_dummies(data=test_x)\n",
    "        rf_pred = rf.predict(X_test)\n",
    "        rf_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "        rf_submission['label'] = rf_pred\n",
    "        rf_submission.to_csv(dataset + \"rf_min_submission.csv\", index=False)\n",
    "    elif train_min_min == 1: # Decision Tree\n",
    "        dt = DecisionTreeClassifier(max_depth=minfile[\"max_depth\"].iat[1],\n",
    "                                    min_samples_split=2,\n",
    "                                    max_features=minfile[\"max_features\"].iat[1],\n",
    "                                    random_state=CFG['SEED']\n",
    "                                   )\n",
    "        dt.fit(X_train, y_train)\n",
    "        X_test = pd.get_dummies(data=test_x)\n",
    "        dt_pred = dt.predict(X_test)\n",
    "        dt_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "        dt_submission['label'] = dt_pred\n",
    "        dt_submission.to_csv(dataset + \"dt_min_submission.csv\", index=False)\n",
    "    elif train_min_min == 2: # XG Boost\n",
    "        xgboost = XGBClassifier(max_depth=minfile['max_depth'].iat[2],\n",
    "                                n_estimators=int(minfile[\"n_estimators\"].iat[2]),\n",
    "                                grow_policy='depthwise',\n",
    "                                n_jobs=-1,\n",
    "                                random_state=CFG['SEED'],\n",
    "                                tree_method='auto'\n",
    "                               )\n",
    "        xgboost.fit(X_train, y_train)\n",
    "        X_test = pd.get_dummies(data=test_x)\n",
    "        xgboost_pred = xgboost.predict(X_test)\n",
    "        xgboost_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "        xgboost_submission['label'] = xgboost_pred\n",
    "        xgboost_submission.to_csv(dataset + \"xgboost_min_submission.csv\", index=False)\n",
    "    else:\n",
    "#         lgbm = LGBMClassifier(max_depth=minfile['max_depth'].iat[3],\n",
    "#                             n_estimators=int(minfile[\"n_estimators\"].iat[3]),\n",
    "#                             max_features=minfile[\"max_features\"].iat[3],\n",
    "#                             n_jobs=-1,\n",
    "#                             num_leaves=int(minfile[\"num_leaves\"].iat[3]),\n",
    "#                             random_state=CFG['SEED']\n",
    "#                             )\n",
    "#         lgbm.fit(X_train, y_train)\n",
    "#         X_test = pd.get_dummies(data=test_x)\n",
    "#         lgbm_pred = lgbm.predict(X_test)\n",
    "#         lgbm_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "#         lgbm_submission['label'] = lgbm_pred\n",
    "#         lgbm_submission.to_csv(dataset + \"lgbm_min_submission.csv\", index=False)\n",
    "        print(\"This is LGBM\")\n",
    "else:\n",
    "    print(filename.iloc[train_min], train_acc.iloc[train_min])\n",
    "    val_min_filepath = dataset + \"Results/\" + filename.iloc[train_min].iat[0,0] +\".csv\"\n",
    "    val_minfile = pd.read_csv(val_min_filepath)\n",
    "    val_min_train = pd.DataFrame(val_minfile[\"0\"])\n",
    "    val_min_val= pd.DataFrame(val_minfile[\"1\"])\n",
    "    train_min_min = val_min_train.idxmin()[-1]\n",
    "    val_min_min = val_min_val.idxmin()[-1]\n",
    "    if train_min_min == 0: # Random Forest\n",
    "        rf = RandomForestClassifier(max_depth=val_minfile[\"max_depth\"].iat[0],\n",
    "                                        n_estimators=int(val_minfile[\"n_estimators\"].iat[0]),\n",
    "                                    min_samples_split=2,\n",
    "                                    max_features=val_minfile[\"max_features\"].iat[0],\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=CFG['SEED']\n",
    "                                   )\n",
    "        rf.fit(X_train, y_train)\n",
    "        X_test = pd.get_dummies(data=test_x)\n",
    "        rf_pred = rf.predict(X_test)\n",
    "        rf_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "        rf_submission['label'] = rf_pred\n",
    "        rf_submission.to_csv(dataset + \"rf_min_submission.csv\", index=False)\n",
    "    elif train_min_min == 1: # Decision Tree\n",
    "        dt = DecisionTreeClassifier(max_depth=val_minfile[\"max_depth\"].iat[1],\n",
    "                                    min_samples_split=2,\n",
    "                                    max_features=val_minfile[\"max_features\"].iat[1],\n",
    "                                    random_state=CFG['SEED']\n",
    "                                   )\n",
    "        dt.fit(X_train, y_train)\n",
    "        X_test = pd.get_dummies(data=test_x)\n",
    "        dt_pred = dt.predict(X_test)\n",
    "        dt_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "        dt_submission['label'] = dt_pred\n",
    "        dt_submission.to_csv(dataset + \"dt_min_submission.csv\", index=False)\n",
    "    elif train_min_min == 2: # XG Boost\n",
    "        xgboost = XGBClassifier(max_depth=val_minfile['max_depth'].iat[2],\n",
    "                                n_estimators=int(val_minfile[\"n_estimators\"].iat[2]),\n",
    "                                grow_policy='depthwise',\n",
    "                                n_jobs=-1,\n",
    "                                random_state=CFG['SEED'],\n",
    "                                tree_method='auto'\n",
    "                               )\n",
    "        xgboost.fit(X_train, y_train)\n",
    "        X_test = pd.get_dummies(data=test_x)\n",
    "        xgboost_pred = xgboost.predict(X_test)\n",
    "        xgboost_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "        xgboost_submission['label'] = xgboost_pred\n",
    "        xgboost_submission.to_csv(dataset + \"xgboost_min_submission.csv\", index=False)\n",
    "    else:\n",
    "#         lgbm = LGBMClassifier(max_depth=val_minfile['max_depth'].iat[3],\n",
    "#                             n_estimators=int(val_minfile[\"n_estimators\"].iat[3]),\n",
    "#                             max_features=val_minfile[\"max_features\"].iat[3],\n",
    "#                             n_jobs=-1,\n",
    "#                             num_leaves=int(val_minfile[\"num_leaves\"].iat[3]),\n",
    "#                             random_state=CFG['SEED']\n",
    "#                             )\n",
    "#         lgbm.fit(X_train, y_train)\n",
    "#         X_test = pd.get_dummies(data=test_x)\n",
    "#         lgbm_pred = lgbm.predict(X_test)\n",
    "#         lgbm_submission = pd.read_csv(dataset + 'sample_submission.csv')\n",
    "#         lgbm_submission['label'] = lgbm_pred\n",
    "#         lgbm_submission.to_csv(dataset + \"lgbm_min_submission.csv\", index=False)\n",
    "        print(\"This is LGBM\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98f50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
