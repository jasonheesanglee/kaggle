{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# All Models\n\n#### This notebook originates from the [work](https://www.kaggle.com/code/nhttinnguynbch/commonlit-ess-lgbm-autocorrect-deberta-v3-tuned) of [@nhttinnguynbch](https://www.kaggle.com/nhttinnguynbch)<br> Which originates from the [work](https://www.kaggle.com/code/siddhvr/commonlit-ess-lgbm-autocorrect-deberta-v3-tuned) of [@siddhvr](https://www.kaggle.com/siddhvr)\n\nOn this notebook, I tried to try on all the other models and compare them with `debertav3base`.<br>\nHowever, the performance of other models were not as good as `debertav3base`.<br>\nTherefore, I had to move back to `debertav3base` and added symspellpy for preprocessing part (based on my research on this [notebook](https://www.kaggle.com/code/jasonheesanglee/spellcheck-tool-comparison)) and optuna for LGBM.","metadata":{}},{"cell_type":"markdown","source":"# Install, Import","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/autocorrect/autocorrect-2.6.1.tar\n!pip install /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\n!pip install /kaggle/input/symspell-677/editdistpy-0.1.3-cp310-cp310-linux_x86_64.whl\n!pip install /kaggle/input/symspell-677/symspellpy-6.7.7-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-11T08:36:42.183868Z","iopub.execute_input":"2023-10-11T08:36:42.187965Z","iopub.status.idle":"2023-10-11T08:38:58.602816Z","shell.execute_reply.started":"2023-10-11T08:36:42.187923Z","shell.execute_reply":"2023-10-11T08:38:58.601674Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/autocorrect/autocorrect-2.6.1.tar\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: autocorrect\n  Building wheel for autocorrect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=3e9bf84dac1038066f4f5f521064dac850c772b46f9c960667dbd8f865a9e613\n  Stored in directory: /root/.cache/pip/wheels/db/69/42/0fb0421d2fe70d195a04665edc760cfe5fd341d7bb8d8e0aaa\nSuccessfully built autocorrect\nInstalling collected packages: autocorrect\nSuccessfully installed autocorrect-2.6.1\nProcessing /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\nInstalling collected packages: pyspellchecker\nSuccessfully installed pyspellchecker-0.7.2\nProcessing /kaggle/input/symspell-677/editdistpy-0.1.3-cp310-cp310-linux_x86_64.whl\nInstalling collected packages: editdistpy\nSuccessfully installed editdistpy-0.1.3\nProcessing /kaggle/input/symspell-677/symspellpy-6.7.7-py3-none-any.whl\nRequirement already satisfied: editdistpy>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from symspellpy==6.7.7) (0.1.3)\nInstalling collected packages: symspellpy\nSuccessfully installed symspellpy-6.7.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\")\n\nimport os\nos.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport logging\nlogging.disable(logging.ERROR)\n\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nimport shutil\nimport json\nimport transformers\nimport sentencepiece\nimport pkg_resources\n\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\n# from transformers import T5ForConditionalGeneration, T5TokenizerFast, T5Config\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nimport spacy\nimport re\nfrom autocorrect import Speller\nfrom spellchecker import SpellChecker\nfrom symspellpy import SymSpell, Verbosity\nimport lightgbm as lgb\nimport optuna\n\n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:38:58.605388Z","iopub.execute_input":"2023-10-11T08:38:58.605818Z","iopub.status.idle":"2023-10-11T08:39:14.359966Z","shell.execute_reply.started":"2023-10-11T08:38:58.605781Z","shell.execute_reply":"2023-10-11T08:39:14.359097Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:14.361431Z","iopub.execute_input":"2023-10-11T08:39:14.361914Z","iopub.status.idle":"2023-10-11T08:39:14.372306Z","shell.execute_reply.started":"2023-10-11T08:39:14.361852Z","shell.execute_reply":"2023-10-11T08:39:14.371539Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"files = ['debertav3base', # files[0]\n         'albert-large-v2', # files[1]\n         'bert-base-uncased', # files[2]\n         'bert-large-uncased', # files[3]\n         'distilroberta-base', # files[4]\n         'distilbert-base-uncased', # files[5]\n         'google-electra-base-discriminator', # files[6]\n         'facebook-bart-base', # files[7] # Not working\n         'facebook-bart-large', # files[8]\n         'funnel-transformer-small', # files[9]\n         'funnel-transformer-large', # files[10]\n         'roberta-base', # files[11]\n         'roberta-large', # files[12]\n         't5-base', # files[13] # don't use\n         't5-large', # files[14] # don't use\n         'xlnet-base-cased', # files[15]\n         'xlnet-large-cased' # files[16]\n         ]","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:14.375437Z","iopub.execute_input":"2023-10-11T08:39:14.375654Z","iopub.status.idle":"2023-10-11T08:39:14.385248Z","shell.execute_reply.started":"2023-10-11T08:39:14.375627Z","shell.execute_reply":"2023-10-11T08:39:14.384425Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"IS_DEBUG = True\nOPTUNA = True\nFOLD = 'G_FOLD' # 'G_FOLD' or 'S_FOLD'\n\nSEP_TKN = ' #### ' # ' [SEP]' or ' #### '\n\n# if SEP_TKN == ' #### ':\nCLS_TKN = ''\n# else:\n#     CLS_TKN = ' [CLS] '\n    \nclass CFG:\n    model_name=files[0]\n    learning_rate=0.000016   #0.000015\n    weight_decay=0.007        #0.02\n    hidden_dropout_prob=0.007\n    attention_probs_dropout_mprob=0.007\n    num_train_epochs=1 if IS_DEBUG else 4\n    n_splits= 2 if IS_DEBUG else 4\n    batch_size= 2 if IS_DEBUG else 8\n    random_seed=42\n    save_steps=1000 if IS_DEBUG else 100\n    max_length= 10 if IS_DEBUG else 512","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:14.386474Z","iopub.execute_input":"2023-10-11T08:39:14.387186Z","iopub.status.idle":"2023-10-11T08:39:14.396455Z","shell.execute_reply.started":"2023-10-11T08:39:14.387152Z","shell.execute_reply":"2023-10-11T08:39:14.395637Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"method = ['py_and_sym', 'pyspell_only', 'symspell_only']\n\nfreq_dict_list = [\"/content/symspell-677/symspell_freq_dict.txt\",\n            \"/content/symspell-677/frequency_dictionary_en_82_765.txt\",\n            \"/content/symspell-677/frequency_bigramdictionary_en_243_342.txt\"]\n\nyes = True\nno = False\n\nmanage_misspelled_words = yes\nmisspelled_word_method = method[0]\nfreq_dict = freq_dict_list[1].split('/')[-1]\nfreq_dict\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:14.397798Z","iopub.execute_input":"2023-10-11T08:39:14.398193Z","iopub.status.idle":"2023-10-11T08:39:14.409227Z","shell.execute_reply.started":"2023-10-11T08:39:14.398044Z","shell.execute_reply":"2023-10-11T08:39:14.408365Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'frequency_dictionary_en_82_765.txt'"},"metadata":{}}]},{"cell_type":"code","source":"if (manage_misspelled_words==yes) & (misspelled_word_method == 'py_and_sym'):\n    pyspell_detector = yes\n    symspell_corrector = yes\n    misspell_counter = 0\n\nif (manage_misspelled_words==yes) & (misspelled_word_method == 'pyspell_only'):\n    pyspell_detector = yes\n    symspell_corrector = no\n    misspell_counter = 1\n    \nif (manage_misspelled_words==yes) & (misspelled_word_method == 'symspell_only'):\n    pyspell_detector = no\n    symspell_corrector = yes\n    freq_dict = freq_dict_list[1]\n    misspell_counter = 2","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:14.410657Z","iopub.execute_input":"2023-10-11T08:39:14.411130Z","iopub.status.idle":"2023-10-11T08:39:14.422047Z","shell.execute_reply.started":"2023-10-11T08:39:14.411101Z","shell.execute_reply":"2023-10-11T08:39:14.421230Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Dataload","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n\nprompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\nprompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\nsummaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\nsummaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\nsample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:14.423486Z","iopub.execute_input":"2023-10-11T08:39:14.424092Z","iopub.status.idle":"2023-10-11T08:39:14.539130Z","shell.execute_reply.started":"2023-10-11T08:39:14.424061Z","shell.execute_reply":"2023-10-11T08:39:14.538288Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess\n\n[Using features]\n\n- Text Length\n- Length Ratio\n- Word Overlap\n- N-grams Co-occurrence\n  - count\n  - ratio\n- Quotes Overlap\n- Grammar Check\n  - spelling: pyspellchecker\n","metadata":{}},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self, \n                model_name: str,\n                ) -> None:\n        if model_name == files[0]:\n            self.tokenizer = AutoTokenizer.from_pretrained(f'/kaggle/input/{model_name}')\n        elif model_name == (files[13] or files[14]):\n            self.tokenizer = T5TokenizerFast.from_pretrained(f'/kaggle/input/transformers/{model_name}')\n        else:\n            self.tokenizer = AutoTokenizer.from_pretrained(f'/kaggle/input/transformers/{model_name}')\n        \n        self.twd = TreebankWordDetokenizer()\n        self.STOP_WORDS = set(stopwords.words('english'))\n        \n        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n        self.speller = Speller(lang='en')\n        self.spellchecker = SpellChecker() \n        \n    def word_overlap_count(self, row):\n        \"\"\" intersection(prompt_text, text) \"\"\"        \n        def check_is_stop_word(word):\n            return word in self.STOP_WORDS\n        \n        prompt_words = row['prompt_tokens']\n        summary_words = row['summary_tokens']\n        if self.STOP_WORDS:\n            prompt_words = list(filter(check_is_stop_word, prompt_words))\n            summary_words = list(filter(check_is_stop_word, summary_words))\n        return len(set(prompt_words).intersection(set(summary_words)))\n            \n    def ngrams(self, token, n):\n        # Use the zip function to help us generate n-grams\n        # Concatentate the tokens into ngrams and return\n        ngrams = zip(*[token[i:] for i in range(n)])\n        return [\" \".join(ngram) for ngram in ngrams]\n\n    def ngram_co_occurrence(self, row, n: int) -> int:\n        # Tokenize the original text and summary into words\n        original_tokens = row['prompt_tokens']\n        summary_tokens = row['summary_tokens']\n\n        # Generate n-grams for the original text and summary\n        original_ngrams = set(self.ngrams(original_tokens, n))\n        summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n        # Calculate the number of common n-grams\n        common_ngrams = original_ngrams.intersection(summary_ngrams)\n        return len(common_ngrams)\n    \n    def ner_overlap_count(self, row, mode:str):\n        model = self.spacy_ner_model\n        def clean_ners(ner_list):\n            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n        prompt = model(row['prompt_text'])\n        summary = model(row['text'])\n\n        if \"spacy\" in str(model):\n            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n        elif \"stanza\" in str(model):\n            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n            summary_ner = set([(token.text, token.type) for token in summary.ents])\n        else:\n            raise Exception(\"Model not supported\")\n\n        prompt_ner = clean_ners(prompt_ner)\n        summary_ner = clean_ners(summary_ner)\n\n        intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n        if mode == \"train\":\n            return ner_dict\n        elif mode == \"test\":\n            return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n    def quotes_count(self, row):\n        summary = row['text']\n        text = row['prompt_text']\n        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n        if len(quotes_from_summary)>0:\n            return [quote in text for quote in quotes_from_summary].count(True)\n        else:\n            return 0\n\n    def spelling(self, text):\n        \n        wordlist=text.split()\n        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n\n        return amount_miss\n    \n    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n        self.spellchecker.word_frequency.load_words(tokens)\n        self.speller.nlp_data.update({token:1000 for token in tokens})\n    \n    def run(self, \n            prompts: pd.DataFrame,\n            summaries:pd.DataFrame,\n            mode:str\n        ) -> pd.DataFrame:\n        \n        # before merge preprocess\n        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n\n        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n        \n        # Add prompt tokens into spelling checker dictionary\n        prompts[\"prompt_tokens\"].apply(\n            lambda x: self.add_spelling_dictionary(x)\n        )\n        \n#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n        # fix misspelling\n        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n            lambda x: self.speller(x)\n        )\n        \n        # count misspelling\n        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n        \n        # merge prompts and summaries\n        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n\n        # after merge preprocess\n        # input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n        \n        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n        input_df['bigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence,args=(2,), axis=1 \n        )\n        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n        \n        input_df['trigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence, args=(3,), axis=1\n        )\n        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n        \n        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        \n        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n    \npreprocessor = Preprocessor(model_name=CFG.model_name)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:14.540662Z","iopub.execute_input":"2023-10-11T08:39:14.541148Z","iopub.status.idle":"2023-10-11T08:39:16.326292Z","shell.execute_reply.started":"2023-10-11T08:39:14.541119Z","shell.execute_reply":"2023-10-11T08:39:16.325450Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/datapreprocess/original_train.csv')\n# train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n\ntest = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:16.330339Z","iopub.execute_input":"2023-10-11T08:39:16.330881Z","iopub.status.idle":"2023-10-11T08:39:17.185703Z","shell.execute_reply.started":"2023-10-11T08:39:16.330844Z","shell.execute_reply":"2023-10-11T08:39:17.184852Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|██████████| 4/4 [00:00<00:00, 6058.94it/s]\n100%|██████████| 4/4 [00:00<00:00, 6125.31it/s]\n100%|██████████| 4/4 [00:00<00:00, 3568.86it/s]\n100%|██████████| 4/4 [00:00<00:00, 3885.41it/s]\n100%|██████████| 4/4 [00:00<00:00, 4032.02it/s]\n100%|██████████| 4/4 [00:00<00:00, 3128.33it/s]\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"     student_id prompt_id                                               text  \\\n0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n\n    content   wording  summary_length  \\\n0  0.205683  0.380538              64   \n1 -0.548304  0.506755              54   \n2  3.128928  4.231226             269   \n3 -0.210614 -0.471415              28   \n4  3.272894  3.219757             232   \n\n                                  fixed_summary_text  splling_err_num  \\\n0  The third wave was an experimental see how peo...                5   \n1  They would rub it up with soda to make the sme...                2   \n2  In Egypt, there were many occupations and soci...               32   \n3  The highest class was Pharaohs these people we...                5   \n4  The Third Wave developed  rapidly because the ...               29   \n\n                                     prompt_question  \\\n0  Summarize how the Third Wave developed over su...   \n1  Summarize the various ways the factory would u...   \n2  In complete sentences, summarize the structure...   \n3  In complete sentences, summarize the structure...   \n4  Summarize how the Third Wave developed over su...   \n\n                prompt_title  \\\n0             The Third Wave   \n1    Excerpt from The Jungle   \n2  Egyptian Social Structure   \n3  Egyptian Social Structure   \n4             The Third Wave   \n\n                                         prompt_text  prompt_length  \\\n0  Background \\r\\nThe Third Wave experiment took ...            660   \n1  With one member trimming beef in a cannery, an...           1076   \n2  Egyptian society was structured like a pyramid...            625   \n3  Egyptian society was structured like a pyramid...            625   \n4  Background \\r\\nThe Third Wave experiment took ...            660   \n\n   word_overlap_count  bigram_overlap_count  bigram_overlap_ratio  \\\n0                  14                     4              0.063492   \n1                  18                    22              0.415094   \n2                  22                    52              0.194030   \n3                   6                     6              0.222222   \n4                  23                    27              0.116883   \n\n   trigram_overlap_count  trigram_overlap_ratio  quotes_count  \n0                      0               0.000000             0  \n1                     10               0.192308             0  \n2                     23               0.086142             2  \n3                      5               0.192308             0  \n4                      5               0.021739             4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>content</th>\n      <th>wording</th>\n      <th>summary_length</th>\n      <th>fixed_summary_text</th>\n      <th>splling_err_num</th>\n      <th>prompt_question</th>\n      <th>prompt_title</th>\n      <th>prompt_text</th>\n      <th>prompt_length</th>\n      <th>word_overlap_count</th>\n      <th>bigram_overlap_count</th>\n      <th>bigram_overlap_ratio</th>\n      <th>trigram_overlap_count</th>\n      <th>trigram_overlap_ratio</th>\n      <th>quotes_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000e8c3c7ddb</td>\n      <td>814d6b</td>\n      <td>The third wave was an experimentto see how peo...</td>\n      <td>0.205683</td>\n      <td>0.380538</td>\n      <td>64</td>\n      <td>The third wave was an experimental see how peo...</td>\n      <td>5</td>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>The Third Wave</td>\n      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n      <td>660</td>\n      <td>14</td>\n      <td>4</td>\n      <td>0.063492</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0020ae56ffbf</td>\n      <td>ebad26</td>\n      <td>They would rub it up with soda to make the sme...</td>\n      <td>-0.548304</td>\n      <td>0.506755</td>\n      <td>54</td>\n      <td>They would rub it up with soda to make the sme...</td>\n      <td>2</td>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>Excerpt from The Jungle</td>\n      <td>With one member trimming beef in a cannery, an...</td>\n      <td>1076</td>\n      <td>18</td>\n      <td>22</td>\n      <td>0.415094</td>\n      <td>10</td>\n      <td>0.192308</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>004e978e639e</td>\n      <td>3b9047</td>\n      <td>In Egypt, there were many occupations and soci...</td>\n      <td>3.128928</td>\n      <td>4.231226</td>\n      <td>269</td>\n      <td>In Egypt, there were many occupations and soci...</td>\n      <td>32</td>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>Egyptian society was structured like a pyramid...</td>\n      <td>625</td>\n      <td>22</td>\n      <td>52</td>\n      <td>0.194030</td>\n      <td>23</td>\n      <td>0.086142</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>005ab0199905</td>\n      <td>3b9047</td>\n      <td>The highest class was Pharaohs these people we...</td>\n      <td>-0.210614</td>\n      <td>-0.471415</td>\n      <td>28</td>\n      <td>The highest class was Pharaohs these people we...</td>\n      <td>5</td>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>Egyptian society was structured like a pyramid...</td>\n      <td>625</td>\n      <td>6</td>\n      <td>6</td>\n      <td>0.222222</td>\n      <td>5</td>\n      <td>0.192308</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0070c9e7af47</td>\n      <td>814d6b</td>\n      <td>The Third Wave developed  rapidly because the ...</td>\n      <td>3.272894</td>\n      <td>3.219757</td>\n      <td>232</td>\n      <td>The Third Wave developed  rapidly because the ...</td>\n      <td>29</td>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>The Third Wave</td>\n      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n      <td>660</td>\n      <td>23</td>\n      <td>27</td>\n      <td>0.116883</td>\n      <td>5</td>\n      <td>0.021739</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# SymSpell & Pyspellcheck","metadata":{}},{"cell_type":"code","source":"def pyspellchecker_detector(sentence):\n    sentence = re.sub(r'[^\\w\\s]','',sentence)\n    spell = SpellChecker()\n    tokens = sentence.split(' ')\n    mis_tokens = []\n    for token in spell.unknown(tokens):\n        if token.isalpha():\n            mis_tokens.append(token)\n    return mis_tokens\n\ndef symspellpy_corrector(mis_tokens, freq_dict_):\n    try:\n        sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n        freq_dict = pkg_resources.resource_filename(\"symspellpy\", freq_dict_)\n        sym_spell.load_dictionary(freq_dict, term_index=0, count_index=1)\n        corrected_token = {}\n        for token in tqdm(mis_tokens):\n            terms = sym_spell.lookup_compound(token, \n                                              max_edit_distance=2) \n            if token not in corrected_token.keys():\n                corrected_token[token] = terms[0].term\n        return corrected_token\n\n    except UnicodeDecodeError:\n        return mis_tokens\n\ndef py_sym_checker(df, column, new_col, freq_dict_):\n    try:\n        mis_tokens = []\n        for row_num in tqdm(range(df.shape[0])):\n            sentence = df[column][row_num]\n            for word in pyspellchecker_detector(sentence):\n                mis_tokens.append(word)\n        \n        mis_token_rep = symspellpy_corrector(mis_tokens, freq_dict_)\n        \n        temp = []\n        for row_num in tqdm(range(df.shape[0])):\n            sentence = df[column][row_num]\n            tokens = sentence.split(' ')\n            temp_str = ''\n            for token in tokens:\n                if token in mis_token_rep.keys():\n                    temp_str = temp_str + \" \" + mis_token_rep.get(token)\n                else:\n                    temp_str = temp_str + \" \" + token\n            temp.append(temp_str)\n            \n        # df[column] = pd.Series(temp)\n        return pd.Series(temp)\n    \n    except UnicodeDecodeError:\n        return df[new_col]\n    \ndef symspellpy_correction(df, column, new_col, freq_dict=freq_dict):\n    try:        \n        temp = []\n        for row_num in tqdm(range(df.shape[0])):\n            sentence = df[column][row_num]\n            sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=5)\n            freq_dict = freq_dict\n            sym_spell.load_dictionary(freq_dict, term_index=0, count_index=1)\n            terms = sym_spell.lookup_compound(sentence, \n                                              max_edit_distance=2) \n\n            corrected_sentence = terms[0].term\n            temp.append(corrected_sentence)\n            \n        # df[column] = pd.Series(temp)\n        return pd.Series(temp)\n    \n    except UnicodeDecodeError:\n        return df[new_col]\n    \ndef pyspell_correction(df, column, new_col):\n    try:        \n        temp_total = []\n        for row_num in tqdm(range(df.shape[0])):\n            sentence = df[column][row_num]\n            \n            spell = SpellChecker()\n            tokens = nltk.word_tokenize(sentence)\n            text_length = len(tokens)\n\n            mis_tokens = [token for token in spell.unknown(tokens) if token.isalpha()]\n            temp = []\n            corrected_words = []\n            for word in mis_tokens:\n                corrected_word = spell.correction(word)\n                temp.append({word : corrected_word})\n                corrected_words.append(corrected_word)\n\n            temp_1 = []\n            for word in tokens:\n                for set_ in temp:\n                    if list(set_.keys())[0] == word:\n                        word = list(set_.values())[0]\n                        if word in temp_1:\n                            continue\n                        else:\n                            temp_1.append(word)\n                if word in temp_1:\n                    continue\n                else:\n                    temp_1.append(word)\n\n            corrected_sentence = ''\n            for word in temp_1:\n                try:\n                    if (word.isalpha() or word.isnumeric()) == True:\n                        corrected_sentence = corrected_sentence + word + ' '\n                    elif word in [',', '.', '\"', \"'\", '(', ')', '[', ']', '{', '}']:\n                        corrected_sentence = corrected_sentence + word\n                    else:\n                        corrected_sentence = corrected_sentence + word\n                except:\n                    continue\n            corrected_sentence = corrected_sentence.replace('  ', ' ').strip()\n            temp_total.append(corrected_sentence)\n            \n        # df[column] = pd.Series(temp_total)\n        return pd.Series(temp_total)\n    except UnicodeDecodeError:\n        return df[new_col]","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:17.187143Z","iopub.execute_input":"2023-10-11T08:39:17.187375Z","iopub.status.idle":"2023-10-11T08:39:17.203883Z","shell.execute_reply.started":"2023-10-11T08:39:17.187346Z","shell.execute_reply":"2023-10-11T08:39:17.202962Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"if manage_misspelled_words:\n    train = pd.read_csv('/kaggle/input/datapreprocess/py_sym_train.csv')\n    if misspell_counter == 0:\n        test['symspell_corr'] = py_sym_checker(test, 'text', 'symspell_corr',freq_dict)\n        print(f\"py & sym test_processed['text'][0] =\\n{test['symspell_corr'][0]}\")\n    elif misspell_counter == 1:\n        test['symspell_corr'] = pyspell_correction(test, 'text', 'symspell_corr')\n        print(f\"py only test_processed['text'][0] =\\n{test['symspell_corr'][0]}\")\n    elif misspell_counter == 2:\n        test['symspell_corr'] = symspellpy_correction(test, 'text', 'symspell_corr', freq_dict)\n        print(f\"sym only test_processed['text'][0] =\\n{test['symspell_corr'][0]}\")\nelse:\n    \n    print('no change')","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:17.205112Z","iopub.execute_input":"2023-10-11T08:39:17.205897Z","iopub.status.idle":"2023-10-11T08:39:23.937708Z","shell.execute_reply.started":"2023-10-11T08:39:17.205867Z","shell.execute_reply":"2023-10-11T08:39:23.936847Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 4/4 [00:00<00:00, 17.72it/s]\n0it [00:00, ?it/s]\n100%|██████████| 4/4 [00:00<00:00, 9742.87it/s]","output_type":"stream"},{"name":"stdout","text":"py & sym test_processed['text'][0] =\n Example text 1\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fold","metadata":{}},{"cell_type":"code","source":"if FOLD == 'S_FOLD':\n    S_kfold = StratifiedKFold(n_splits=CFG.n_splits)\n    for i, (_, val_index) in enumerate(S_kfold.split(train, y=train[\"prompt_id\"])):\n        train.loc[val_index, \"fold\"] = i\n    print(\"It's StratifiedKFold\")\n    \nif FOLD == 'G_FOLD':\n    gkf = GroupKFold(n_splits=CFG.n_splits)\n    for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n        train.loc[val_index, \"fold\"] = i\n    print(\"It's GroupKFold\")\n    \ntrain.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:23.939070Z","iopub.execute_input":"2023-10-11T08:39:23.939548Z","iopub.status.idle":"2023-10-11T08:39:23.967502Z","shell.execute_reply.started":"2023-10-11T08:39:23.939514Z","shell.execute_reply":"2023-10-11T08:39:23.966711Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"It's GroupKFold\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"     student_id prompt_id                                               text  \\\n0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n\n    content   wording  summary_length  \\\n0  0.205683  0.380538              64   \n1 -0.548304  0.506755              54   \n2  3.128928  4.231226             269   \n3 -0.210614 -0.471415              28   \n4  3.272894  3.219757             232   \n\n                                  fixed_summary_text  splling_err_num  \\\n0  The third wave was an experimental see how peo...                5   \n1  They would rub it up with soda to make the sme...                2   \n2  In Egypt, there were many occupations and soci...               32   \n3  The highest class was Pharaohs these people we...                5   \n4  The Third Wave developed  rapidly because the ...               29   \n\n                                     prompt_question  \\\n0  Summarize how the Third Wave developed over su...   \n1  Summarize the various ways the factory would u...   \n2  In complete sentences, summarize the structure...   \n3  In complete sentences, summarize the structure...   \n4  Summarize how the Third Wave developed over su...   \n\n                prompt_title  \\\n0             The Third Wave   \n1    Excerpt from The Jungle   \n2  Egyptian Social Structure   \n3  Egyptian Social Structure   \n4             The Third Wave   \n\n                                         prompt_text  prompt_length  \\\n0  Background \\r\\nThe Third Wave experiment took ...            660   \n1  With one member trimming beef in a cannery, an...           1076   \n2  Egyptian society was structured like a pyramid...            625   \n3  Egyptian society was structured like a pyramid...            625   \n4  Background \\r\\nThe Third Wave experiment took ...            660   \n\n   word_overlap_count  bigram_overlap_count  bigram_overlap_ratio  \\\n0                  14                     4              0.063492   \n1                  18                    22              0.415094   \n2                  22                    52              0.194030   \n3                   6                     6              0.222222   \n4                  23                    27              0.116883   \n\n   trigram_overlap_count  trigram_overlap_ratio  quotes_count  \\\n0                      0               0.000000             0   \n1                     10               0.192308             0   \n2                     23               0.086142             2   \n3                      5               0.192308             0   \n4                      5               0.021739             4   \n\n                                       symspell_corr  fold  \n0   The third wave was an experiment to see how p...   0.0  \n1   They would rub it up with soda to make the sm...   1.0  \n2   In Egypt, there were many occupations and soc...   1.0  \n3   The highest class was Pharaohs these people w...   1.0  \n4   The Third Wave developed  rapidly because the...   0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>content</th>\n      <th>wording</th>\n      <th>summary_length</th>\n      <th>fixed_summary_text</th>\n      <th>splling_err_num</th>\n      <th>prompt_question</th>\n      <th>prompt_title</th>\n      <th>prompt_text</th>\n      <th>prompt_length</th>\n      <th>word_overlap_count</th>\n      <th>bigram_overlap_count</th>\n      <th>bigram_overlap_ratio</th>\n      <th>trigram_overlap_count</th>\n      <th>trigram_overlap_ratio</th>\n      <th>quotes_count</th>\n      <th>symspell_corr</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000e8c3c7ddb</td>\n      <td>814d6b</td>\n      <td>The third wave was an experimentto see how peo...</td>\n      <td>0.205683</td>\n      <td>0.380538</td>\n      <td>64</td>\n      <td>The third wave was an experimental see how peo...</td>\n      <td>5</td>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>The Third Wave</td>\n      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n      <td>660</td>\n      <td>14</td>\n      <td>4</td>\n      <td>0.063492</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>The third wave was an experiment to see how p...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0020ae56ffbf</td>\n      <td>ebad26</td>\n      <td>They would rub it up with soda to make the sme...</td>\n      <td>-0.548304</td>\n      <td>0.506755</td>\n      <td>54</td>\n      <td>They would rub it up with soda to make the sme...</td>\n      <td>2</td>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>Excerpt from The Jungle</td>\n      <td>With one member trimming beef in a cannery, an...</td>\n      <td>1076</td>\n      <td>18</td>\n      <td>22</td>\n      <td>0.415094</td>\n      <td>10</td>\n      <td>0.192308</td>\n      <td>0</td>\n      <td>They would rub it up with soda to make the sm...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>004e978e639e</td>\n      <td>3b9047</td>\n      <td>In Egypt, there were many occupations and soci...</td>\n      <td>3.128928</td>\n      <td>4.231226</td>\n      <td>269</td>\n      <td>In Egypt, there were many occupations and soci...</td>\n      <td>32</td>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>Egyptian society was structured like a pyramid...</td>\n      <td>625</td>\n      <td>22</td>\n      <td>52</td>\n      <td>0.194030</td>\n      <td>23</td>\n      <td>0.086142</td>\n      <td>2</td>\n      <td>In Egypt, there were many occupations and soc...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>005ab0199905</td>\n      <td>3b9047</td>\n      <td>The highest class was Pharaohs these people we...</td>\n      <td>-0.210614</td>\n      <td>-0.471415</td>\n      <td>28</td>\n      <td>The highest class was Pharaohs these people we...</td>\n      <td>5</td>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>Egyptian Social Structure</td>\n      <td>Egyptian society was structured like a pyramid...</td>\n      <td>625</td>\n      <td>6</td>\n      <td>6</td>\n      <td>0.222222</td>\n      <td>5</td>\n      <td>0.192308</td>\n      <td>0</td>\n      <td>The highest class was Pharaohs these people w...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0070c9e7af47</td>\n      <td>814d6b</td>\n      <td>The Third Wave developed  rapidly because the ...</td>\n      <td>3.272894</td>\n      <td>3.219757</td>\n      <td>232</td>\n      <td>The Third Wave developed  rapidly because the ...</td>\n      <td>29</td>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>The Third Wave</td>\n      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n      <td>660</td>\n      <td>23</td>\n      <td>27</td>\n      <td>0.116883</td>\n      <td>5</td>\n      <td>0.021739</td>\n      <td>4</td>\n      <td>The Third Wave developed  rapidly because the...</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# train.text = train.prompt_question + ' #### ' + train.fixed_summary_text\n# train = train.drop(columns=['prompt_question', 'fixed_summary_text'])\n# display(train.head(5))\n\n# test.text = test.prompt_question + ' #### '  + test.fixed_summary_text\n# test = test.drop(columns=['prompt_question', 'fixed_summary_text'])\n# display(test.head(5))\n\n\n\ntrain.text = train.prompt_question + ' #### ' + train.fixed_summary_text + ' #### ' + train.symspell_corr\ntrain = train.drop(columns=['prompt_question', 'fixed_summary_text', 'symspell_corr'])\ndisplay(train.head(5))\n\ntest.text = test.prompt_question + ' #### '  + test.fixed_summary_text + ' #### ' + test.symspell_corr\ntest = test.drop(columns=['prompt_question', 'fixed_summary_text', 'symspell_corr'])\ndisplay(test.head(5))\n\n\n\n# train.text = train.prompt_question + ' #### ' + train.symspell_corr\n# train = train.drop(columns=['prompt_question', 'fixed_summary_text', 'symspell_corr'])\n# display(train.head(5))\n\n# test.text = test.prompt_question + ' #### '  + test.symspell_corr\n# test = test.drop(columns=['prompt_question', 'fixed_summary_text', 'symspell_corr'])\n# display(test.head(5))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:23.968968Z","iopub.execute_input":"2023-10-11T08:39:23.969717Z","iopub.status.idle":"2023-10-11T08:39:24.010057Z","shell.execute_reply.started":"2023-10-11T08:39:23.969687Z","shell.execute_reply":"2023-10-11T08:39:24.009265Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"     student_id prompt_id                                               text  \\\n0  000e8c3c7ddb    814d6b  Summarize how the Third Wave developed over su...   \n1  0020ae56ffbf    ebad26  Summarize the various ways the factory would u...   \n2  004e978e639e    3b9047  In complete sentences, summarize the structure...   \n3  005ab0199905    3b9047  In complete sentences, summarize the structure...   \n4  0070c9e7af47    814d6b  Summarize how the Third Wave developed over su...   \n\n    content   wording  summary_length  splling_err_num  \\\n0  0.205683  0.380538              64                5   \n1 -0.548304  0.506755              54                2   \n2  3.128928  4.231226             269               32   \n3 -0.210614 -0.471415              28                5   \n4  3.272894  3.219757             232               29   \n\n                prompt_title  \\\n0             The Third Wave   \n1    Excerpt from The Jungle   \n2  Egyptian Social Structure   \n3  Egyptian Social Structure   \n4             The Third Wave   \n\n                                         prompt_text  prompt_length  \\\n0  Background \\r\\nThe Third Wave experiment took ...            660   \n1  With one member trimming beef in a cannery, an...           1076   \n2  Egyptian society was structured like a pyramid...            625   \n3  Egyptian society was structured like a pyramid...            625   \n4  Background \\r\\nThe Third Wave experiment took ...            660   \n\n   word_overlap_count  bigram_overlap_count  bigram_overlap_ratio  \\\n0                  14                     4              0.063492   \n1                  18                    22              0.415094   \n2                  22                    52              0.194030   \n3                   6                     6              0.222222   \n4                  23                    27              0.116883   \n\n   trigram_overlap_count  trigram_overlap_ratio  quotes_count  fold  \n0                      0               0.000000             0   0.0  \n1                     10               0.192308             0   1.0  \n2                     23               0.086142             2   1.0  \n3                      5               0.192308             0   1.0  \n4                      5               0.021739             4   0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>content</th>\n      <th>wording</th>\n      <th>summary_length</th>\n      <th>splling_err_num</th>\n      <th>prompt_title</th>\n      <th>prompt_text</th>\n      <th>prompt_length</th>\n      <th>word_overlap_count</th>\n      <th>bigram_overlap_count</th>\n      <th>bigram_overlap_ratio</th>\n      <th>trigram_overlap_count</th>\n      <th>trigram_overlap_ratio</th>\n      <th>quotes_count</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000e8c3c7ddb</td>\n      <td>814d6b</td>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>0.205683</td>\n      <td>0.380538</td>\n      <td>64</td>\n      <td>5</td>\n      <td>The Third Wave</td>\n      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n      <td>660</td>\n      <td>14</td>\n      <td>4</td>\n      <td>0.063492</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0020ae56ffbf</td>\n      <td>ebad26</td>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>-0.548304</td>\n      <td>0.506755</td>\n      <td>54</td>\n      <td>2</td>\n      <td>Excerpt from The Jungle</td>\n      <td>With one member trimming beef in a cannery, an...</td>\n      <td>1076</td>\n      <td>18</td>\n      <td>22</td>\n      <td>0.415094</td>\n      <td>10</td>\n      <td>0.192308</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>004e978e639e</td>\n      <td>3b9047</td>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>3.128928</td>\n      <td>4.231226</td>\n      <td>269</td>\n      <td>32</td>\n      <td>Egyptian Social Structure</td>\n      <td>Egyptian society was structured like a pyramid...</td>\n      <td>625</td>\n      <td>22</td>\n      <td>52</td>\n      <td>0.194030</td>\n      <td>23</td>\n      <td>0.086142</td>\n      <td>2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>005ab0199905</td>\n      <td>3b9047</td>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>-0.210614</td>\n      <td>-0.471415</td>\n      <td>28</td>\n      <td>5</td>\n      <td>Egyptian Social Structure</td>\n      <td>Egyptian society was structured like a pyramid...</td>\n      <td>625</td>\n      <td>6</td>\n      <td>6</td>\n      <td>0.222222</td>\n      <td>5</td>\n      <td>0.192308</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0070c9e7af47</td>\n      <td>814d6b</td>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>3.272894</td>\n      <td>3.219757</td>\n      <td>232</td>\n      <td>29</td>\n      <td>The Third Wave</td>\n      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n      <td>660</td>\n      <td>23</td>\n      <td>27</td>\n      <td>0.116883</td>\n      <td>5</td>\n      <td>0.021739</td>\n      <td>4</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"     student_id prompt_id                                               text  \\\n0  000000ffffff    abc123  Summarize... #### Example text 1 ####  Example...   \n1  111111eeeeee    def789  Summarize... #### Example text 2 ####  Example...   \n2  222222cccccc    abc123  Summarize... #### Example text 3 ####  Example...   \n3  333333dddddd    def789  Summarize... #### Example text 4 ####  Example...   \n\n   summary_length  splling_err_num     prompt_title       prompt_text  \\\n0               3                0  Example Title 1  Heading\\nText...   \n1               3                0  Example Title 2  Heading\\nText...   \n2               3                0  Example Title 1  Heading\\nText...   \n3               3                0  Example Title 2  Heading\\nText...   \n\n   prompt_length  word_overlap_count  bigram_overlap_count  \\\n0              3                   0                     0   \n1              3                   0                     0   \n2              3                   0                     0   \n3              3                   0                     0   \n\n   bigram_overlap_ratio  trigram_overlap_count  trigram_overlap_ratio  \\\n0                   0.0                      0                    0.0   \n1                   0.0                      0                    0.0   \n2                   0.0                      0                    0.0   \n3                   0.0                      0                    0.0   \n\n   quotes_count  \n0             0  \n1             0  \n2             0  \n3             0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>summary_length</th>\n      <th>splling_err_num</th>\n      <th>prompt_title</th>\n      <th>prompt_text</th>\n      <th>prompt_length</th>\n      <th>word_overlap_count</th>\n      <th>bigram_overlap_count</th>\n      <th>bigram_overlap_ratio</th>\n      <th>trigram_overlap_count</th>\n      <th>trigram_overlap_ratio</th>\n      <th>quotes_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000ffffff</td>\n      <td>abc123</td>\n      <td>Summarize... #### Example text 1 ####  Example...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>Example Title 1</td>\n      <td>Heading\\nText...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>111111eeeeee</td>\n      <td>def789</td>\n      <td>Summarize... #### Example text 2 ####  Example...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>Example Title 2</td>\n      <td>Heading\\nText...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>222222cccccc</td>\n      <td>abc123</td>\n      <td>Summarize... #### Example text 3 ####  Example...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>Example Title 1</td>\n      <td>Heading\\nText...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>333333dddddd</td>\n      <td>def789</td>\n      <td>Summarize... #### Example text 4 ####  Example...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>Example Title 2</td>\n      <td>Heading\\nText...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:24.011482Z","iopub.execute_input":"2023-10-11T08:39:24.011919Z","iopub.status.idle":"2023-10-11T08:39:24.018681Z","shell.execute_reply.started":"2023-10-11T08:39:24.011887Z","shell.execute_reply":"2023-10-11T08:39:24.017678Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Index(['student_id', 'prompt_id', 'text', 'content', 'wording',\n       'summary_length', 'splling_err_num', 'prompt_title', 'prompt_text',\n       'prompt_length', 'word_overlap_count', 'bigram_overlap_count',\n       'bigram_overlap_ratio', 'trigram_overlap_count',\n       'trigram_overlap_ratio', 'quotes_count', 'fold'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model Function Definition","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    rmse = mean_squared_error(labels, predictions, squared=False)\n    return {\"rmse\": rmse}\n\ndef compute_mcrmse(eval_pred):\n    \"\"\"\n    Calculates mean columnwise root mean squared error\n    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n    \"\"\"\n    preds, labels = eval_pred\n\n    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n    mcrmse = np.mean(col_rmse)\n\n    return {\n        \"content_rmse\": col_rmse[0],\n        \"wording_rmse\": col_rmse[1],\n        \"mcrmse\": mcrmse,\n    }\n\ndef compt_score(content_true, content_pred, wording_true, wording_pred):\n    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n    \n    return (content_score + wording_score)/2","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:24.019918Z","iopub.execute_input":"2023-10-11T08:39:24.020614Z","iopub.status.idle":"2023-10-11T08:39:24.028381Z","shell.execute_reply.started":"2023-10-11T08:39:24.020564Z","shell.execute_reply":"2023-10-11T08:39:24.027440Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Deberta Regressor\n*Edited some codes to fit my need.*","metadata":{}},{"cell_type":"code","source":"class ContentScoreRegressor:\n    def __init__(self, \n                model_name: str,\n                model_dir: str,\n                target: str,\n                hidden_dropout_prob: float,\n                attention_probs_dropout_prob: float,\n                max_length: int,\n                ):\n        self.inputs = [\"prompt_text\", \"prompt_title\", \"text\"]\n        self.input_col = \"input\"\n        \n        self.text_cols = [self.input_col] \n        self.target = target\n        self.target_cols = [target]\n\n        self.model_name = model_name\n        self.model_dir = model_dir\n        self.max_length = max_length\n        \n        # Tokenizer\n\n        if model_name == files[0]:\n            self.tokenizer = AutoTokenizer.from_pretrained(f'/kaggle/input/{model_name}')\n        elif model_name == (files[13] or files[14]):\n            self.tokenizer = T5TokenizerFast.from_pretrained(f'/kaggle/input/transformers/{model_name}')\n        else:\n            self.tokenizer = AutoTokenizer.from_pretrained(f'/kaggle/input/transformers/{model_name}')\n\n        # Config\n\n        if model_name == files[0]:\n            self.model_config = AutoConfig.from_pretrained(f'/kaggle/input/{model_name}')\n        elif model_name == (files[13] or files[14]):\n            self.model_config = T5Config.from_pretrained(f'/kaggle/input/transformers/{model_name}')\n        else:\n            self.model_config = AutoConfig.from_pretrained(f'/kaggle/input/transformers/{model_name}')\n        \n        self.model_config.update({\n            \"hidden_dropout_prob\": hidden_dropout_prob,\n            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n            \"num_labels\": 1,\n            \"problem_type\": \"regression\",\n        })\n        \n        seed_everything(seed=42)\n\n        self.data_collator = DataCollatorWithPadding(\n            tokenizer=self.tokenizer\n        )\n\n\n    def tokenize_function(self, examples: pd.DataFrame):\n        labels = [examples[self.target]]\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return {\n            **tokenized,\n            \"labels\": labels,\n        }\n    \n    def tokenize_function_test(self, examples: pd.DataFrame):\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return tokenized\n        \n    def train(self, \n            fold: int,\n            train_df: pd.DataFrame,\n            valid_df: pd.DataFrame,\n            batch_size: int,\n            learning_rate: float,\n            weight_decay: float,\n            num_train_epochs: float,\n            save_steps: int,\n        ) -> None:\n        \"\"\"fine-tuning\"\"\"\n        token_dict = {\"debertav3base\": [' [CLS] ', ' [SEP]'],\n                      \"albert-large-v2\": [' [CLS] ', ' [SEP]'],\n                      \"bert-base-uncased\": [' [CLS] ', ' [SEP]'],\n                      \"bert-large-uncased\": [' [CLS] ', ' [SEP]'],\n                      \"distilroberta-base\": [' <s> ', ' </s>'],\n                      \"distilbert-base-uncased\": [' [CLS] ', ' [SEP]'],\n                      \"google-electra-base-discriminator\": [' [CLS] ', ' [SEP]'],\n                      \"facebook-bart-base\": [' <s> ', ' </s>'],\n                      \"facebook-bart-large\": [' <s> ', ' </s>'], \n                      \"funnel-transformer-small\": [' <cls> ', ' <sep>'],\n                      \"funnel-transformer-large\": [' <cls> ', ' <sep>'],\n                      \"roberta-base\": [' <s> ', ' </s>'],\n                      \"roberta-large\": ['<s> ', ' </s>'],\n                      \"t5-base\": [' <s> ', ' </s>'],\n                      \"t5-large\": [' <s> ', ' </s>'],\n                      \"xlnet-base-cased\": ['<s> ', ' </s>'],\n                      \"xlnet-large-cased\": [' <s> ', ' </s>']\n                      }\n#         if SEP_TKN != \" #### \":\n#             cls_ = token_dict[self.model_name][0]\n#             sep = token_dict[self.model_name][1]\n#         else:\n#             cls_ = ''\n#             sep = SEP_TKN\n\n        \n        train_df[self.input_col] = (\n                    train_df[\"prompt_title\"] + \" #### \"\n                    + train_df[\"text\"]\n                  )\n\n        valid_df[self.input_col] = (\n                    valid_df[\"prompt_title\"] + \" #### \"\n                    + valid_df[\"text\"]\n                  )\n        \n        train_df = train_df[[self.input_col] + self.target_cols]\n        valid_df = valid_df[[self.input_col] + self.target_cols]\n        \n        if self.model_name == files[0]:\n            model_content = AutoModelForSequenceClassification.from_pretrained(\n                f\"/kaggle/input/{self.model_name}\",\n                config=self.model_config\n            )\n        elif self.model_name == (files[13] or files[14]):\n            model_content = T5ForConditionalGeneration.from_pretrained(\n                f\"/kaggle/input/transformers/{self.model_name}\",\n                config=self.model_config\n            )\n        else:\n            model_content = AutoModelForSequenceClassification.from_pretrained(\n                f\"/kaggle/input/transformers/{self.model_name}\",\n                config=self.model_config\n            )\n\n        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n    \n        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n\n        # eg. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        \n        training_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            load_best_model_at_end=True, # select best model\n            learning_rate=learning_rate,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=8,\n            num_train_epochs=num_train_epochs,\n            weight_decay=weight_decay,\n            report_to='none',\n            greater_is_better=False,\n            save_strategy=\"steps\",\n            evaluation_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            metric_for_best_model=\"rmse\",\n            save_total_limit=1\n        )\n\n        trainer = Trainer(\n            model=model_content,\n            args=training_args,\n            train_dataset=train_tokenized_datasets,\n            eval_dataset=val_tokenized_datasets,\n            tokenizer=self.tokenizer,\n            compute_metrics=compute_metrics,\n            data_collator=self.data_collator\n        )\n\n        trainer.train()\n        \n        model_content.save_pretrained(self.model_dir)\n        self.tokenizer.save_pretrained(self.model_dir)\n\n        \n    def predict(self, \n                test_df: pd.DataFrame,\n                fold: int,\n               ):\n        \"\"\"predict content score\"\"\"\n        token_dict = {\"debertav3base\": [' [CLS] ', ' [SEP]'],\n                      \"albert-large-v2\": [' [CLS] ', ' [SEP]'],\n                      \"bert-base-uncased\": [' [CLS] ', ' [SEP]'],\n                      \"bert-large-uncased\": [' [CLS] ', ' [SEP]'],\n                      \"distilroberta-base\": [' <s> ', ' </s>'],\n                      \"distilbert-base-uncased\": [' [CLS] ', ' [SEP]'],\n                      \"google-electra-base-discriminator\": [' [CLS] ', ' [SEP]'],\n                      \"facebook-bart-base\": [' <s> ', ' </s>'],\n                      \"facebook-bart-large\": [' <s> ', ' </s>'], \n                      \"funnel-transformer-small\": [' <cls> ', ' <sep>'],\n                      \"funnel-transformer-large\": [' <cls> ', ' <sep>'],\n                      \"roberta-base\": [' <s> ', ' </s>'],\n                      \"roberta-large\": ['<s> ', ' </s>'],\n                      \"t5-base\": [' <s> ', ' </s>'],\n                      \"t5-large\": [' <s> ', ' </s>'],\n                      \"xlnet-base-cased\": ['<s> ', ' </s>'],\n                      \"xlnet-large-cased\": [' <s> ', ' </s>']\n                      }\n        \n#         if SEP_TKN != ' #### ':\n#             cls_ = token_dict[self.model_name][0]\n#             sep = token_dict[self.model_name][1]\n#         else:\n#             cls_ = ''\n#             sep = SEP_TKN\n\n        in_text = (\n                   test_df[\"prompt_title\"] + \" #### \"\n                    + test_df[\"text\"]\n                  )\n        \n        test_df[self.input_col] = in_text\n\n        test_ = test_df[[self.input_col]]\n    \n        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n\n        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n        model_content.eval()\n        \n        # e.g. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n\n        test_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            do_train = False,\n            do_predict = True,\n            per_device_eval_batch_size = 4,   \n            dataloader_drop_last = False,\n        )\n\n        # init trainer\n        infer_content = Trainer(\n                      model = model_content, \n                      tokenizer=self.tokenizer,\n                      data_collator=self.data_collator,\n                      args = test_args)\n\n        preds = infer_content.predict(test_tokenized_dataset)[0]\n\n        return preds","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:24.030150Z","iopub.execute_input":"2023-10-11T08:39:24.030439Z","iopub.status.idle":"2023-10-11T08:39:24.053047Z","shell.execute_reply.started":"2023-10-11T08:39:24.030407Z","shell.execute_reply":"2023-10-11T08:39:24.052126Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def train_by_fold(\n        train_df: pd.DataFrame,\n        model_name: str,\n        target:str,\n        save_each_model: bool,\n        n_splits: int,\n        batch_size: int,\n        learning_rate: int,\n        hidden_dropout_prob: float,\n        attention_probs_dropout_prob: float,\n        weight_decay: float,\n        num_train_epochs: int,\n        save_steps: int,\n        max_length:int\n    ):\n\n    # delete old model files\n    if os.path.exists(model_name):\n        shutil.rmtree(model_name)\n    \n#     if '/' in model_name:\n#         model_name_ = model_name.split('/')[1]\n    os.mkdir(model_name)\n#     else:\n#         model_name_ = model_name.copy()\n        \n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        train_data = train_df[train_df[\"fold\"] != fold]\n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        csr.train(\n            fold=fold,\n            train_df=train_data,\n            valid_df=valid_data, \n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            num_train_epochs=num_train_epochs,\n            save_steps=save_steps,\n        )\n\ndef validate(\n    train_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ) -> pd.DataFrame:\n    \"\"\"predict oof data\"\"\"\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{model_name}/fold_{fold}\"\n        \n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir,\n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=valid_data, \n            fold=fold\n        )\n        \n        train_df.loc[valid_data.index, f\"{target}_pred\"] = pred\n\n    return train_df\n    \ndef predict(\n    test_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ):\n    \"\"\"predict using mean folds\"\"\"\n\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        if save_each_model == True:\n            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=test_df, \n            fold=fold\n        )\n        \n        test_df[f\"{target}_pred_{fold}\"] = pred\n    \n    test_df[f\"{target}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n\n    return test_df","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:24.054550Z","iopub.execute_input":"2023-10-11T08:39:24.055071Z","iopub.status.idle":"2023-10-11T08:39:24.068710Z","shell.execute_reply.started":"2023-10-11T08:39:24.055042Z","shell.execute_reply":"2023-10-11T08:39:24.067873Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for target in [\"content\", \"wording\"]:\n    train_by_fold(\n        train,\n        model_name=CFG.model_name,\n        save_each_model=False,\n        target=target,\n        learning_rate=CFG.learning_rate,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_mprob,\n        weight_decay=CFG.weight_decay,\n        num_train_epochs=CFG.num_train_epochs,\n        n_splits=CFG.n_splits,\n        batch_size=CFG.batch_size,\n        save_steps=CFG.save_steps,\n        max_length=CFG.max_length\n    )\n    \n    \n    train = validate(\n        train,\n        target=target,\n        save_each_model=False,\n        model_name=CFG.model_name,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_mprob,\n        max_length=CFG.max_length\n    )\n\n    rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n    print(f\"cv {target} rmse: {rmse}\")\n\n    test = predict(\n        test,\n        target=target,\n        save_each_model=False,\n        model_name=CFG.model_name,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_mprob,\n        max_length=CFG.max_length\n    )","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:39:24.070163Z","iopub.execute_input":"2023-10-11T08:39:24.070383Z","iopub.status.idle":"2023-10-11T08:55:34.214726Z","shell.execute_reply.started":"2023-10-11T08:39:24.070355Z","shell.execute_reply":"2023-10-11T08:55:34.213775Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"fold 0:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2003' max='2003' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2003/2003 03:41, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rmse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>1.102600</td>\n      <td>1.099696</td>\n      <td>1.048664</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.095200</td>\n      <td>1.082463</td>\n      <td>1.040415</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"fold 1:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1580' max='1580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1580/1580 02:49, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rmse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>1.081600</td>\n      <td>1.111071</td>\n      <td>1.054073</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"fold 0:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"fold 1:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"cv content rmse: 1.048071407403576\nfold 0:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"fold 1:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"fold 0:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2003' max='2003' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2003/2003 03:42, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rmse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.867200</td>\n      <td>1.412668</td>\n      <td>1.188557</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.909600</td>\n      <td>1.286071</td>\n      <td>1.134051</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"fold 1:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1580' max='1580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1580/1580 02:50, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rmse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>1.134400</td>\n      <td>0.898658</td>\n      <td>0.947976</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"fold 0:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"fold 1:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"cv wording rmse: 1.0341759862490243\nfold 0:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"fold 1:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:55:34.216190Z","iopub.execute_input":"2023-10-11T08:55:34.216753Z","iopub.status.idle":"2023-10-11T08:55:34.236232Z","shell.execute_reply.started":"2023-10-11T08:55:34.216719Z","shell.execute_reply":"2023-10-11T08:55:34.235065Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"     student_id prompt_id                                               text  \\\n0  000e8c3c7ddb    814d6b  Summarize how the Third Wave developed over su...   \n1  0020ae56ffbf    ebad26  Summarize the various ways the factory would u...   \n2  004e978e639e    3b9047  In complete sentences, summarize the structure...   \n3  005ab0199905    3b9047  In complete sentences, summarize the structure...   \n4  0070c9e7af47    814d6b  Summarize how the Third Wave developed over su...   \n\n    content   wording  summary_length  splling_err_num  \\\n0  0.205683  0.380538              64                5   \n1 -0.548304  0.506755              54                2   \n2  3.128928  4.231226             269               32   \n3 -0.210614 -0.471415              28                5   \n4  3.272894  3.219757             232               29   \n\n                prompt_title  \\\n0             The Third Wave   \n1    Excerpt from The Jungle   \n2  Egyptian Social Structure   \n3  Egyptian Social Structure   \n4             The Third Wave   \n\n                                         prompt_text  prompt_length  \\\n0  Background \\r\\nThe Third Wave experiment took ...            660   \n1  With one member trimming beef in a cannery, an...           1076   \n2  Egyptian society was structured like a pyramid...            625   \n3  Egyptian society was structured like a pyramid...            625   \n4  Background \\r\\nThe Third Wave experiment took ...            660   \n\n   word_overlap_count  bigram_overlap_count  bigram_overlap_ratio  \\\n0                  14                     4              0.063492   \n1                  18                    22              0.415094   \n2                  22                    52              0.194030   \n3                   6                     6              0.222222   \n4                  23                    27              0.116883   \n\n   trigram_overlap_count  trigram_overlap_ratio  quotes_count  fold  \\\n0                      0               0.000000             0   0.0   \n1                     10               0.192308             0   1.0   \n2                     23               0.086142             2   1.0   \n3                      5               0.192308             0   1.0   \n4                      5               0.021739             4   0.0   \n\n   content_pred  wording_pred  \n0     -0.136062     -0.091997  \n1      0.046871     -0.187534  \n2      0.063340     -0.180703  \n3      0.063340     -0.180703  \n4     -0.136062     -0.091997  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>content</th>\n      <th>wording</th>\n      <th>summary_length</th>\n      <th>splling_err_num</th>\n      <th>prompt_title</th>\n      <th>prompt_text</th>\n      <th>prompt_length</th>\n      <th>word_overlap_count</th>\n      <th>bigram_overlap_count</th>\n      <th>bigram_overlap_ratio</th>\n      <th>trigram_overlap_count</th>\n      <th>trigram_overlap_ratio</th>\n      <th>quotes_count</th>\n      <th>fold</th>\n      <th>content_pred</th>\n      <th>wording_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000e8c3c7ddb</td>\n      <td>814d6b</td>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>0.205683</td>\n      <td>0.380538</td>\n      <td>64</td>\n      <td>5</td>\n      <td>The Third Wave</td>\n      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n      <td>660</td>\n      <td>14</td>\n      <td>4</td>\n      <td>0.063492</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>-0.136062</td>\n      <td>-0.091997</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0020ae56ffbf</td>\n      <td>ebad26</td>\n      <td>Summarize the various ways the factory would u...</td>\n      <td>-0.548304</td>\n      <td>0.506755</td>\n      <td>54</td>\n      <td>2</td>\n      <td>Excerpt from The Jungle</td>\n      <td>With one member trimming beef in a cannery, an...</td>\n      <td>1076</td>\n      <td>18</td>\n      <td>22</td>\n      <td>0.415094</td>\n      <td>10</td>\n      <td>0.192308</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.046871</td>\n      <td>-0.187534</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>004e978e639e</td>\n      <td>3b9047</td>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>3.128928</td>\n      <td>4.231226</td>\n      <td>269</td>\n      <td>32</td>\n      <td>Egyptian Social Structure</td>\n      <td>Egyptian society was structured like a pyramid...</td>\n      <td>625</td>\n      <td>22</td>\n      <td>52</td>\n      <td>0.194030</td>\n      <td>23</td>\n      <td>0.086142</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>0.063340</td>\n      <td>-0.180703</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>005ab0199905</td>\n      <td>3b9047</td>\n      <td>In complete sentences, summarize the structure...</td>\n      <td>-0.210614</td>\n      <td>-0.471415</td>\n      <td>28</td>\n      <td>5</td>\n      <td>Egyptian Social Structure</td>\n      <td>Egyptian society was structured like a pyramid...</td>\n      <td>625</td>\n      <td>6</td>\n      <td>6</td>\n      <td>0.222222</td>\n      <td>5</td>\n      <td>0.192308</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.063340</td>\n      <td>-0.180703</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0070c9e7af47</td>\n      <td>814d6b</td>\n      <td>Summarize how the Third Wave developed over su...</td>\n      <td>3.272894</td>\n      <td>3.219757</td>\n      <td>232</td>\n      <td>29</td>\n      <td>The Third Wave</td>\n      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n      <td>660</td>\n      <td>23</td>\n      <td>27</td>\n      <td>0.116883</td>\n      <td>5</td>\n      <td>0.021739</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>-0.136062</td>\n      <td>-0.091997</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## LGBM model","metadata":{}},{"cell_type":"code","source":"targets = [\"content\", \"wording\"]\n\ndrop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\",\n                \"prompt_title\", \n                \"prompt_text\"\n               ] + targets","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:55:34.237923Z","iopub.execute_input":"2023-10-11T08:55:34.238924Z","iopub.status.idle":"2023-10-11T08:55:34.246601Z","shell.execute_reply.started":"2023-10-11T08:55:34.238887Z","shell.execute_reply":"2023-10-11T08:55:34.245669Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model_dict = {}\nif OPTUNA:\n    for target in targets:\n        models = []\n\n        for fold in range(CFG.n_splits):\n\n            X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n            y_train_cv = train[train[\"fold\"] != fold][target]\n\n            X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n            y_eval_cv = train[train[\"fold\"] == fold][target]\n\n            dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n            dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n\n\n            def objective(trial):\n                param_space = {'boosting_type': 'gbdt',\n                    'random_state': 42,\n                    'objective': 'regression',\n                    'metric': 'rmse',\n                    'learning_rate' : trial.suggest_float('learning_rate', 1e-7, 0.1, log=True),\n                    'max_depth' : 2 if IS_DEBUG else trial.suggest_int('max_depth', 3, 10),\n                    'num_leaves': 2 if IS_DEBUG else trial.suggest_int('num_leaves', 2, 1024),\n                    'lambda_l1' : trial.suggest_float('lambda_l1', 1e-7, 0.1, log=True),\n                    'lambda_l2' : trial.suggest_float('lambda_l2', 1e-7, 0.1, log=True)\n                }\n                params = param_space.copy()\n                for param, value in trial.params.items():\n                    params[param] = value\n\n                evaluation_results = {}\n                model = lgb.train(params,\n                                  num_boost_round= 2 if IS_DEBUG else 10000,\n                                    #categorical_feature = categorical_features,\n                                  valid_names=['train', 'valid'],\n                                  train_set=dtrain,\n                                  valid_sets=dval,\n                                  callbacks=[\n                                      lgb.early_stopping(stopping_rounds=30, verbose=True),\n                                       lgb.log_evaluation(100),\n                                      lgb.callback.record_evaluation(evaluation_results)\n                                    ],\n                                  )\n                y_pred = model.predict(X_eval_cv)\n                rmse = np.sqrt(mean_squared_error(y_eval_cv, y_pred))\n                return rmse\n\n            study = optuna.create_study(direction='minimize')\n            study.optimize(objective, n_trials= 2 if IS_DEBUG else 500)\n\n            best_params = study.best_trial.params\n            evaluation_results = {}\n            best_model = lgb.train(best_params,\n                                  num_boost_round = 2 if IS_DEBUG else 10000,\n                                  train_set=dtrain,\n                                  valid_sets=dval,\n                                  callbacks=[\n                                  lgb.early_stopping(stopping_rounds=30, verbose=True),\n                                   lgb.log_evaluation(100),\n                                  lgb.callback.record_evaluation(evaluation_results)\n                                  ])\n\n\n            models.append(best_model)\n\n        model_dict[target] = models\n\nelse:\n    for target in targets:\n        models = []\n\n        for fold in range(CFG.n_splits):\n\n            X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n            y_train_cv = train[train[\"fold\"] != fold][target]\n\n            X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n            y_eval_cv = train[train[\"fold\"] == fold][target]\n\n            dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n            dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n\n            params = {\n                'boosting_type': 'gbdt',\n                'random_state': 42,\n                'objective': 'regression',\n                'metric': 'rmse',\n                'learning_rate': 0.048,\n                'max_depth': 4,  #3\n                'lambda_l1': 0.0,\n                'lambda_l2': 0.011\n            }\n\n            evaluation_results = {}\n            model = lgb.train(params,\n                              num_boost_round=10000,\n                                #categorical_feature = categorical_features,\n                              valid_names=['train', 'valid'],\n                              train_set=dtrain,\n                              valid_sets=dval,\n                              callbacks=[\n                                  lgb.early_stopping(stopping_rounds=30, verbose=True),\n                                   lgb.log_evaluation(100),\n                                  lgb.callback.record_evaluation(evaluation_results)\n                                ],\n                              )\n            models.append(model)\n\n        model_dict[target] = models","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:55:34.248039Z","iopub.execute_input":"2023-10-11T08:55:34.248359Z","iopub.status.idle":"2023-10-11T08:55:34.402538Z","shell.execute_reply.started":"2023-10-11T08:55:34.248328Z","shell.execute_reply":"2023-10-11T08:55:34.401728Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000143 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1173\n[LightGBM] [Info] Number of data points in the train set: 4005, number of used features: 11\n[LightGBM] [Info] Start training from score -0.018940\nTraining until validation scores don't improve for 30 rounds\nDid not meet early stopping. Best iteration is:\n[2]\ttrain's rmse: 1.03267\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1173\n[LightGBM] [Info] Number of data points in the train set: 4005, number of used features: 11\n[LightGBM] [Info] Start training from score -0.018940\nTraining until validation scores don't improve for 30 rounds\nDid not meet early stopping. Best iteration is:\n[2]\ttrain's rmse: 0.979184\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1173\n[LightGBM] [Info] Number of data points in the train set: 4005, number of used features: 11\n[LightGBM] [Info] Start training from score -0.018940\nTraining until validation scores don't improve for 30 rounds\nDid not meet early stopping. Best iteration is:\n[2]\tvalid_0's l2: 0.903336\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000108 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 971\n[LightGBM] [Info] Number of data points in the train set: 3160, number of used features: 11\n[LightGBM] [Info] Start training from score -0.009673\nTraining until validation scores don't improve for 30 rounds\nDid not meet early stopping. Best iteration is:\n[2]\ttrain's rmse: 1.05201\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000106 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 971\n[LightGBM] [Info] Number of data points in the train set: 3160, number of used features: 11\n[LightGBM] [Info] Start training from score -0.009673\nTraining until validation scores don't improve for 30 rounds\nDid not meet early stopping. Best iteration is:\n[2]\ttrain's rmse: 1.05201\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000111 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 971\n[LightGBM] [Info] Number of data points in the train set: 3160, number of used features: 11\n[LightGBM] [Info] Start training from score -0.009673\nTraining until validation scores don't improve for 30 rounds\nDid not meet early stopping. Best iteration is:\n[2]\tvalid_0's l2: 1.10673\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1173\n[LightGBM] [Info] Number of data points in the train set: 4005, number of used features: 11\n[LightGBM] [Info] Start training from score -0.183408\nTraining until validation scores don't improve for 30 rounds\nDid not meet early stopping. Best iteration is:\n[2]\ttrain's rmse: 1.1463\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000440 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1173\n[LightGBM] [Info] Number of data points in the train set: 4005, number of used features: 11\n[LightGBM] [Info] Start training from score -0.183408\nTraining until validation scores don't improve for 30 rounds\nDid not meet early stopping. Best iteration is:\n[2]\ttrain's rmse: 1.15154\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000128 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1173\n[LightGBM] [Info] Number of data points in the train set: 4005, number of used features: 11\n[LightGBM] [Info] Start training from score -0.183408\nTraining until validation scores don't improve for 30 rounds\nDid not meet early stopping. Best iteration is:\n[2]\tvalid_0's l2: 1.29568\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000108 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 971\n[LightGBM] [Info] Number of data points in the train set: 3160, number of used features: 11\n[LightGBM] [Info] Start training from score 0.089443\nTraining until validation scores don't improve for 30 rounds\nDid not meet early stopping. Best iteration is:\n[2]\ttrain's rmse: 0.985896\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000115 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 971\n[LightGBM] [Info] Number of data points in the train set: 3160, number of used features: 11\n[LightGBM] [Info] Start training from score 0.089443\nTraining until validation scores don't improve for 30 rounds\nDid not meet early stopping. Best iteration is:\n[2]\ttrain's rmse: 0.98678\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 971\n[LightGBM] [Info] Number of data points in the train set: 3160, number of used features: 11\n[LightGBM] [Info] Start training from score 0.089443\nTraining until validation scores don't improve for 30 rounds\nDid not meet early stopping. Best iteration is:\n[2]\tvalid_0's l2: 0.969249\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## CV Score","metadata":{}},{"cell_type":"code","source":"# cv\nrmses = []\n\nfor target in targets:\n    models = model_dict[target]\n\n    preds = []\n    trues = []\n    \n    for fold, model in enumerate(models):\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        pred = model.predict(X_eval_cv)\n\n        trues.extend(y_eval_cv)\n        preds.extend(pred)\n        \n    rmse = np.sqrt(mean_squared_error(trues, preds))\n    print(f\"{target}_rmse : {rmse}\")\n    rmses = rmses + [rmse]\n\nprint(f\"mcrmse : {sum(rmses) / len(rmses)}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:55:34.403977Z","iopub.execute_input":"2023-10-11T08:55:34.404503Z","iopub.status.idle":"2023-10-11T08:55:34.432149Z","shell.execute_reply.started":"2023-10-11T08:55:34.404472Z","shell.execute_reply":"2023-10-11T08:55:34.431288Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"content_rmse : 1.0084781411901365\nwording_rmse : 1.0550894788208747\nmcrmse : 1.0317838100055057\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Predict","metadata":{}},{"cell_type":"code","source":"drop_columns = [\n                #\"fold\", \n                \"student_id\", \"prompt_id\", \"text\",\n                \"prompt_title\", \n                \"prompt_text\",\n                \"input\"\n               ] + [\n                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n                ] + [\n                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n                ]","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:55:34.433271Z","iopub.execute_input":"2023-10-11T08:55:34.433499Z","iopub.status.idle":"2023-10-11T08:55:34.438750Z","shell.execute_reply.started":"2023-10-11T08:55:34.433470Z","shell.execute_reply":"2023-10-11T08:55:34.437895Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"pred_dict = {}\nfor target in targets:\n    models = model_dict[target]\n    preds = []\n\n    for fold, model in enumerate(models):\n        X_eval_cv = test.drop(columns=drop_columns)\n\n        pred = model.predict(X_eval_cv)\n        preds.append(pred)\n    \n    pred_dict[target] = preds","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:55:34.440042Z","iopub.execute_input":"2023-10-11T08:55:34.440509Z","iopub.status.idle":"2023-10-11T08:55:34.454642Z","shell.execute_reply.started":"2023-10-11T08:55:34.440480Z","shell.execute_reply":"2023-10-11T08:55:34.453822Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"for target in targets:\n    preds = pred_dict[target]\n    for i, pred in enumerate(preds):\n        test[f\"{target}_pred_{i}\"] = pred\n\n    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:55:34.455948Z","iopub.execute_input":"2023-10-11T08:55:34.456189Z","iopub.status.idle":"2023-10-11T08:55:34.466330Z","shell.execute_reply.started":"2023-10-11T08:55:34.456161Z","shell.execute_reply":"2023-10-11T08:55:34.465515Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:55:34.467754Z","iopub.execute_input":"2023-10-11T08:55:34.468192Z","iopub.status.idle":"2023-10-11T08:55:34.488918Z","shell.execute_reply.started":"2023-10-11T08:55:34.468157Z","shell.execute_reply":"2023-10-11T08:55:34.488083Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"     student_id prompt_id                                               text  \\\n0  000000ffffff    abc123  Summarize... #### Example text 1 ####  Example...   \n1  111111eeeeee    def789  Summarize... #### Example text 2 ####  Example...   \n2  222222cccccc    abc123  Summarize... #### Example text 3 ####  Example...   \n3  333333dddddd    def789  Summarize... #### Example text 4 ####  Example...   \n\n   summary_length  splling_err_num     prompt_title       prompt_text  \\\n0               3                0  Example Title 1  Heading\\nText...   \n1               3                0  Example Title 2  Heading\\nText...   \n2               3                0  Example Title 1  Heading\\nText...   \n3               3                0  Example Title 2  Heading\\nText...   \n\n   prompt_length  word_overlap_count  bigram_overlap_count  ...  \\\n0              3                   0                     0  ...   \n1              3                   0                     0  ...   \n2              3                   0                     0  ...   \n3              3                   0                     0  ...   \n\n   trigram_overlap_count  trigram_overlap_ratio  quotes_count  \\\n0                      0                    0.0             0   \n1                      0                    0.0             0   \n2                      0                    0.0             0   \n3                      0                    0.0             0   \n\n                                               input content_pred_0  \\\n0  Example Title 1 #### Summarize... #### Example...      -0.154804   \n1  Example Title 2 #### Summarize... #### Example...      -0.154804   \n2  Example Title 1 #### Summarize... #### Example...      -0.154804   \n3  Example Title 2 #### Summarize... #### Example...      -0.154804   \n\n   content_pred_1   content  wording_pred_0  wording_pred_1   wording  \n0       -0.009674 -0.082239        -0.21171        0.085454 -0.063128  \n1       -0.009674 -0.082239        -0.21171        0.085454 -0.063128  \n2       -0.009674 -0.082239        -0.21171        0.085454 -0.063128  \n3       -0.009674 -0.082239        -0.21171        0.085454 -0.063128  \n\n[4 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>summary_length</th>\n      <th>splling_err_num</th>\n      <th>prompt_title</th>\n      <th>prompt_text</th>\n      <th>prompt_length</th>\n      <th>word_overlap_count</th>\n      <th>bigram_overlap_count</th>\n      <th>...</th>\n      <th>trigram_overlap_count</th>\n      <th>trigram_overlap_ratio</th>\n      <th>quotes_count</th>\n      <th>input</th>\n      <th>content_pred_0</th>\n      <th>content_pred_1</th>\n      <th>content</th>\n      <th>wording_pred_0</th>\n      <th>wording_pred_1</th>\n      <th>wording</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000ffffff</td>\n      <td>abc123</td>\n      <td>Summarize... #### Example text 1 ####  Example...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>Example Title 1</td>\n      <td>Heading\\nText...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>Example Title 1 #### Summarize... #### Example...</td>\n      <td>-0.154804</td>\n      <td>-0.009674</td>\n      <td>-0.082239</td>\n      <td>-0.21171</td>\n      <td>0.085454</td>\n      <td>-0.063128</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>111111eeeeee</td>\n      <td>def789</td>\n      <td>Summarize... #### Example text 2 ####  Example...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>Example Title 2</td>\n      <td>Heading\\nText...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>Example Title 2 #### Summarize... #### Example...</td>\n      <td>-0.154804</td>\n      <td>-0.009674</td>\n      <td>-0.082239</td>\n      <td>-0.21171</td>\n      <td>0.085454</td>\n      <td>-0.063128</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>222222cccccc</td>\n      <td>abc123</td>\n      <td>Summarize... #### Example text 3 ####  Example...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>Example Title 1</td>\n      <td>Heading\\nText...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>Example Title 1 #### Summarize... #### Example...</td>\n      <td>-0.154804</td>\n      <td>-0.009674</td>\n      <td>-0.082239</td>\n      <td>-0.21171</td>\n      <td>0.085454</td>\n      <td>-0.063128</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>333333dddddd</td>\n      <td>def789</td>\n      <td>Summarize... #### Example text 4 ####  Example...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>Example Title 2</td>\n      <td>Heading\\nText...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>Example Title 2 #### Summarize... #### Example...</td>\n      <td>-0.154804</td>\n      <td>-0.009674</td>\n      <td>-0.082239</td>\n      <td>-0.21171</td>\n      <td>0.085454</td>\n      <td>-0.063128</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Create Submission file","metadata":{}},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:55:34.492897Z","iopub.execute_input":"2023-10-11T08:55:34.493066Z","iopub.status.idle":"2023-10-11T08:55:34.501813Z","shell.execute_reply.started":"2023-10-11T08:55:34.493046Z","shell.execute_reply":"2023-10-11T08:55:34.500808Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"     student_id  content  wording\n0  000000ffffff      0.0      0.0\n1  111111eeeeee      0.0      0.0\n2  222222cccccc      0.0      0.0\n3  333333dddddd      0.0      0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>content</th>\n      <th>wording</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000ffffff</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>111111eeeeee</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>222222cccccc</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>333333dddddd</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:55:34.503144Z","iopub.execute_input":"2023-10-11T08:55:34.504048Z","iopub.status.idle":"2023-10-11T08:55:34.514432Z","shell.execute_reply.started":"2023-10-11T08:55:34.504020Z","shell.execute_reply":"2023-10-11T08:55:34.513698Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"test[[\"student_id\", \"content\", \"wording\"]]","metadata":{"execution":{"iopub.status.busy":"2023-10-11T08:55:34.515796Z","iopub.execute_input":"2023-10-11T08:55:34.516014Z","iopub.status.idle":"2023-10-11T08:55:34.526718Z","shell.execute_reply.started":"2023-10-11T08:55:34.515986Z","shell.execute_reply":"2023-10-11T08:55:34.525708Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"     student_id   content   wording\n0  000000ffffff -0.082239 -0.063128\n1  111111eeeeee -0.082239 -0.063128\n2  222222cccccc -0.082239 -0.063128\n3  333333dddddd -0.082239 -0.063128","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>content</th>\n      <th>wording</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000ffffff</td>\n      <td>-0.082239</td>\n      <td>-0.063128</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>111111eeeeee</td>\n      <td>-0.082239</td>\n      <td>-0.063128</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>222222cccccc</td>\n      <td>-0.082239</td>\n      <td>-0.063128</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>333333dddddd</td>\n      <td>-0.082239</td>\n      <td>-0.063128</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}