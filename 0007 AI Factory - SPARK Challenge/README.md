<p align="center">
   <img width=400 alt="image" src="https://github.com/jasonheesanglee/kaggle/assets/123557477/c0effd40-da67-4f29-9b27-6ae2f75b3edc">
</p>

# AI SPARK Challenge
23-04-24 ~ 23-05-11<br><br>

AI Competition held by AI Factory & SPACE-S.<br>
For details of the competition, AI Factory & SPACE-S, please refer to this [webpage](https://aifactory.space/task/2317/overview).
<br><br>
This is my first time joining for a Data Science competition.<br>
For this challenge, we are permitted to use only the given dataset (TRAIN, TEST).<br>
There are 17 PM, 30 AWS data sets for each TRAIN and TEST, and matching geographic location data set.<br><br>

## Requirements
Here are the specific requirements for the competition.<br>
1. Predict hourly PM2.5 values for each region for a 3-day period using the PM2.5/AWS data of the past period (train set) and the PM2.5/AWS data for a 2-day period (test set).
2. The use of AWS values is mandatory for PM2.5 prediction. Any model that does not use AWS values will be invalidated.
3. Ensemble regulation (additional explanation)<br>
   Prediction must be performed using the same model for all observation stations, and ensemble methods such as averaging/weighted sum/voting of multiple models' inference results are not allowed.<br>
   However, if separate models are used for each target element during inference, and the results are generated by inputting the inferred results into a separate model again (e.g., creating temperature and humidity prediction models and predicting PM2.5 using the predicted temperature and humidity), it is allowed as long as one model receives inputs from multiple stages to generate results.<br>
   In this case, however, the inference process must be connected through a seamless pipeline without any artificial intervention from the initial input to the final result generation.<br>
   The key is to prohibit ensemble models and aim to create a global model rather than creating separate models for each observation station (dedicated models that cannot be used outside the observation station).
4. The train set can be used for predicting the test set, but the test set cannot be used for model training.

## Approach
How to get the missing values.<br>
The key to this mission is to find missing values from the given data.<br>
### Below is my theory.
1. Use either Moving Average or Exponential Smoothing to figure out the missing data for each day.
2. As 4 years of data are given, we could find a trend moving from season to seasons, years to year basis
3. Then apply this seasonal trend and yearly trend to the missing data from step 1.

<p align="center">
   <img width=400 alt="image" src="https://github.com/jasonheesanglee/kaggle/assets/123557477/b5d16772-21d7-4a9b-8995-4c53c2cef487">
</p>

Then, one of my teammates who already has some experience in this kind of challenge, advised me that we could use latitude and longitude to place each observation point and check the difference between each observation point.<br>
Therefore, by using this method (after I research into it), we could also get offsets out of trend.<br>
Let me first set up the ipynb file for this.<br>
There could be more imported libraries the more I get into this.<br><br>
```
# Installing
!pip install numpy
!pip install pandas
!pip install matplotlib
!pip install scipy
!pip install folium

# Importing
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy as sp
import folium
import math
import itertools
import glob
```
### What did I do first to start off?
- I checked the file location with the os function.
   ```
   # checking path of this CURRENT document
   print(os.getcwd())
   ```
- To amplify our understanding, we created a map to visualize the area that we are interested in.
  ```
  map_Kor = folium.Map(location=(36.62,126.984873), zoom_start = 9, tiles="Stamen Terrain")
   
   map_Kor.save("Climate_Map.html")
  ```
- Then markers on the coordinates of each observatory, AWS / PM separated.
  ```
  # reading location data csv files.
   awsmap_csv = pd.read_csv("~/Desktop/SPARK COMPETITION/dataset/META/awsmap.csv")
   pmmap_csv = pd.read_csv("~/Desktop/SPARK COMPETITION/dataset/META/pmmap.csv")
   
   # allocating each column into a list variable.
   aws_loc = awsmap_csv["Location"]
   aws_lat = awsmap_csv["Latitude"]
   aws_lng = awsmap_csv["Longitude"]
   
   pm_loc = pmmap_csv["Location"]
   pm_lat = pmmap_csv["Latitude"]
   pm_lng = pmmap_csv["Longitude"]
   
   # Print out the location on the map, using folium.
   
   aws_num = 0 # set counter to 0; aliased as aws, as there are 2 while loops.
   while aws_num < len(aws_loc):
       folium.Marker(location = [aws_lat[aws_num],aws_lng[aws_num]], popup=aws_loc[aws_num], icon=folium.Icon(color="blue")).add_to(map_Kor)
       aws_num += 1
       
   pm_num = 0 # set counter to 0; aliased as pm, as there are 2 while loops.
   while pm_num < len(pm_loc):
       folium.Marker(location = [pm_lat[pm_num], pm_lng[pm_num]], popup=pm_loc[pm_num], icon=folium.Icon(color="red")).add_to(map_Kor)
       pm_num += 1
   
   map_Kor.save("Climate_Map.html")
  ```
- This is the output.
  ![image](https://github.com/jasonheesanglee/kaggle/assets/123557477/7705bfdb-471b-49a2-9114-fed9fc8ea0ab)

After visualizing the locations, I have constructed the steps that I am planning to go through.
   - Find all null data and visualize it (maybe bar)
   - 4 quadrants, each location as x, number of missing data as y

### Filling up missing values:
1. Separate each year and location
2. Find the trend for each column/year
3. Need scatter plot/graph for each location
4. Make boxplot and find 1st & 3rd quad
5. For values in the normal data range, find mean, median, etc. .describe()
6. For outliers (both low and high), make another .describe()
7. Use an if-else - linear regression model to fill out missing values
8. Compare the trend and find the difference in value.mean() per each city and apply to missing values
9. Find the altitude of each location through map terrain (???)
    a. Inspect Folium Stamen Terrain and check if there is altitude data 
    - But we are comparing the relative values. Should I need to do this?
    - Needless data only slows the running speed.

So, I will try my best to follow this what to do.<br>
Prior to starting off, we need to call the data from CSV files.<br><br>

First, I used `glob` to read all the CSV files without needing to read them one by one.<br>
Then, I converted the CSV files to lists, to DataFrame.<br>

```
# reading All Test & Train csv files.
train_aws_csv = glob.glob("./SPARK COMPETITION/dataset/TRAIN_AWS/*.csv")
train_pm_csv = glob.glob("./SPARK COMPETITION/dataset/TRAIN/*.csv")
test_aws_csv = glob.glob("./SPARK COMPETITION/dataset/TEST_AWS/*.csv")
test_pm_csv = glob.glob("./SPARK COMPETITION/dataset/TEST_INPUT/*.csv")

# converting all csv files to list
train_aws_list = (pd.read_csv(file) for file in train_aws_csv)
train_pm_list = (pd.read_csv(file) for file in train_pm_csv)
test_aws_list = (pd.read_csv(file) for file in test_aws_csv)
test_pm_list = (pd.read_csv(file) for file in test_pm_csv)

# concatenate all csv lists into DataFrame
concat_train_aws = pd.concat(train_aws_list, ignore_index=True)
concat_train_pm = pd.concat(train_pm_list, ignore_index=True)
concat_test_aws = pd.concat(test_aws_list, ignore_index=True)
concat_test_pm = pd.concat(test_pm_list, ignore_index=True)
```

#### Separate each year and location
For the first attempt, I was planning on separating years and then moving on to the next stop.<br>
```
# separating file to each year

# group each file by year.
grouped_train_aws = concat_train_aws.groupby("연도")
grouped_train_pm = concat_train_pm.groupby("연도")

# assign each year into different dataframes
train_aws_y0 = grouped_train_aws.get_group(0)
train_aws_y1 = grouped_train_aws.get_group(1)
train_aws_y2 = grouped_train_aws.get_group(2)
train_aws_y3 = grouped_train_aws.get_group(3)

train_pm_y0 = grouped_train_aws.get_group(0)
train_pm_y1 = grouped_train_aws.get_group(1)
train_pm_y2 = grouped_train_aws.get_group(2)
train_pm_y3 = grouped_train_aws.get_group(3)
```
```
# e.g. train_aws_y0
# removed parenthesis due to show the data without destruction on Notion.

			   연도     일시      지점     기온       풍향       풍속        강수량  습도
0         0  01-01 00:00  성거  0.157978  0.012500  0.012788      0.0  0.615
1         0  01-01 01:00  성거  0.153239  0.000000  0.007673      0.0  0.593
2         0  01-01 02:00  성거  0.137441  0.000000  0.010230      0.0  0.674
3         0  01-01 03:00  성거  0.115324  0.000000  0.010230      0.0  0.705
4         0  01-01 04:00  성거  0.112164  0.000000  0.000000      0.0  0.732
..          ...  ..       ...       ...       ...      ...    ...
1025611   0  12-31 19:00  정산  0.385466  0.041944  0.015345      0.0  0.786
1025612   0  12-31 20:00  정산  0.393365  0.102778  0.017903      0.0  0.820
1025613   0  12-31 21:00  정산  0.399684  0.000000  0.005115      0.0  0.775
1025614   0  12-31 22:00  정산  0.387046  0.000000  0.010230      0.0  0.830
1025615   0  12-31 23:00  정산  0.388626  0.000000  0.005115      0.0  0.833
```

Then, I realized that this is a combined file of all the locations.<br>
It seemed like it is required to separate all of them as we need to use that data to make comparisons.<br><br>

And that’s where the chaos began.<br><br>

I was obsessed with `for loop` and made some mistakes on my first attempt.<br>
I used 3rd level nested loop with over 100,000 rows of data.<br><br>

The lines of code below were not working on the Jupyter Notebook and kept letting it crash.<br>
As Jupyter Notebook was not able to run it, I exported the codes to PyCharm and ran it.<br>
I have added the counter on the first `for` loop to check if it is properly running or not.<br>
By doing so, I found out that It was working well, but my codes were too complicated for it to run smoothly.<br>
The counter went over 120,000,000, stopped manually, and fixed the code afterward.<br><br>
> *I believe that this was where I learned the importance of having more and more experience handling large amounts of data. 
So far, I have only dealt with practice datasets, which were usually less than 100 rows. 3rd level nested loop worked very well without any trouble then, but now I clearly understand the importance of time complexity.*
> So, let’s find out what THE CODE looked like. 
> If you thought these codes below were HTML at a glimpse, I totally understand.

```
years = sorted(concat_train_aws["연도"].unique())
locations = sorted(concat_train_aws["지점"].unique())
aws_train_yearloc = {}
pm_train_yearloc = {}

count = 1
inner_count = 1
inner_inner_count = 1

for year in years:
    for location in locations:
        dfs_aws = []
        for idx, row in concat_train_aws.iterrows():
            if row["연도"] == year and row["지점"] == location:
                dfs_aws.append(row)

            inner_inner_count += 1
            print(inner_inner_count)
        aws_train_yearloc[(year, location)] = pd.concat(dfs_aws, ignore_index=True)
        inner_count += 1
        print(inner_count)

    count += 1
    print(count)

for year in years:
    for location in locations:
        dfs_pm = []
        for idx, row in concat_train_pm.iterrows():
            if row["연도"] == year and row["지점"] == location:

                dfs_pm.append(row)
        pm_train_yearloc[(year, location)] = pd.concat(dfs_pm, ignore_index=True)
```

Below is a code that I modified afterward.<br>
Used single for `loop` with `lambda` function.<br>
```
aws_train_yearloc = {}
pm_train_yearloc = {}

for year, location in aws_train_yearloc:
    aws_rows = concat_train_aws.loc[(concat_train_aws["연도"] == year) & (concat_train_aws["지점"] == location)].groupby(
        ['연도', '월', '지점']).apply(lambda x: x.iloc[-24:])
    aws_train_yearloc[(year, location)] = aws_rows.reset_index(drop=True)

for year, location in pm_yearloc:
    pm_rows = concat_train_pm.loc[(concat_train_pm["연도"] == year) & (concat_train_pm["측정소"] == location)].groupby(
        ['연도', '월', '측정소']).apply(lambda x: x.iloc[-24:])
    pm_train_yearloc[(year, location)] = pm_rows.reset_index(drop=True)
```

Then, I added codes to create a separate folder and stored these data in separate CSV files.<br>
```

if not os.path.exists("/Users/b05/Desktop/SPARK COMPETITION/dataset/AWS TRAIN year_loc data"):
    os.mkdir("/Users/b05/Desktop/SPARK COMPETITION/dataset/AWS TRAIN year_loc data")

if not os.path.exists("/Users/b05/Desktop/SPARK COMPETITION/dataset/PM TRAIN year_loc data"):
    os.mkdir("/Users/b05/Desktop/SPARK COMPETITION/dataset/PM TRAIN year_loc data")

for year, location in aws_train_yearloc:
    filename = f"aws_train_{location}_{year}.csv"
    aws_train_yearloc[(year, location)].to_csv(f"/SPARK COMPETITION/dataset/AWS TRAIN year_loc data")

for year, location in pm_train_yearloc:
    filename = f"pm_train_{location}_{year}.csv"
    pm_train_yearloc[(year, location)].to_csv(f"/SPARK COMPETITION/dataset/PM TRAIN year_loc data")
```
The folder was properly created after running the codes, but the expected `csv` files were not there. Not even one of them.<br>
So I have scrolled up and started reading the codes again.<br>
I found that on some occasions, Python couldn’t call or read folders and files.<br>
Later, I found out that the `.` was the problem.<br>
> If the path address starts with`.`, the program finds the path from the current location. 
If the path address starts with`/`, the program finds the path from the initial directory stated on the address.

So, I have changed all frequently used paths to the absolute direction and converted them to variables.<br>
Then apply the above change to all the path-related codes.<br>
While doing so, I realized that some of the codes were repetitive and could be modified to more simple codes for future use.<br><br>

I checked if the `csv` files were created or not after this modification, but they still didn’t.<br>
So I will resume finding the wrong codes.<br>

It has been about 7 hours since I wrote the sentence above.<br>
During these 7 hours, there was some progress and now it works all good.<br>
I decided to ignore all the codes below the map-generating codes.<br>
I thought it was better to start again from scratch than modify the codes that were not working well.<br><br>

This has changed my perspective, and it was clear to me what to do.<br>
I was going all the way around to achieve one simple goal: <br>

> *Divide each of the original files in the TRAIN folder into 4, according to “연도” (”YEAR”)*

Creating separate CSV files.<br>
It was quite a work.<br>

Before writing the codes above, I tried to understand how the computer works.<br>

First, it will need to have access to each file in a folder.<br>
Second, it will need to get the location name from the file title.<br>
Third, it will need to find the YEARs (“연도”) from each file.<br>
Fourth, it will need to compare separate YEARS.<br>
Fifth, it will need to save with a location name year count to CSV.<br>
and VOILA! Here is the code!<br>
```
# separate csv file by year
# selecting each files within the TRAIN_AWS folder
for train_aws_file in all_file_locations['train_aws']:
    # read each csv file
    df = pd.read_csv(train_aws_file)
    # get location name from file name
    location = os.path.splitext(os.path.basename(train_aws_file))[0]
    # separate by year and save as separate csv files
    for year in range(4):
        year_df = df[df['연도'] == year]
        year_filename = dataset+ f"CITY_YEAR/AWS_TRAIN_CITY_YEAR/train_aws_{location}_{year}.csv"
        year_df.to_csv(year_filename, index=False)
```

With a similar concept, I decided to divide the original files per column and merge them into a new file.<br>
As PM has only one row to divide and merge, I had to fix the code a little, but there was no big trouble.<br>

Well, never mind, PM codes were correct, but not AWS.<br>
Therefore, I had to re-conduct the code.<br>

I was struggling to make it all at once, so I decided to split them up and make them one by one.<br>
By doing so, I could make the function work, but wanted to simplify a bit more.<br>
As I have done similar work when creating codes for file locations, I have implemented a dictionary for here as well.<br>
**Now it is finally time to apply the prediction modules and methods!** <br>

#### Find trend for each column/year
Before finding the trend for each column and year, I thought it would be better to understand the wind direction.<br>
The authority has only provided the numbers from 0 to 1, but not the cardinal direction.<br>
We have all we need to get cardinal direction: `time`, `location`, `relative direction`, `wind speed`.<br>

**Here is my theory:**

- For short-distanced locations, the wind direction should have a high correlation.
- Once locations with highly correlated wind directions are short-listed:
    - Define them as `a` and `b`.
    - Define base time as `t1`.
    - Define the wind direction of `a` as `adir`, of `b` as `bdir`.
    - Define the distance between `a` and `b` as `abdis`.
    - For wind speed `x`m / sec, `abdis` / `x` = `abtime`, for the wind to get from `a` to `b`.
    - Wind speed increment value = (`bdir` at `t1`+`abtime`) / (`adir` at `t1`) * 100 = `incrpercent`
    
    In the equation, it will be as follows.
     ```math
     (bdir@(t_1+abtime))/(adir@t_1) * 100 = incrpercent
     ```
To verify my theory, these are what we could do for now.<br>
1. Find which observatories have correlative wind direction.
2. Convert the distance between `a` and `b` into `km`, from latitudes and longitudes.

First, I have created the Correlation Matrix for all the locations that we have.<br>
![image](https://github.com/jasonheesanglee/kaggle/assets/123557477/89bdfe52-3204-4dc7-aea7-dbcf69a57db8)
As you may have noticed, it is impossible to distinguish which set of locations has high or low correlations.<br>
Therefore, I have decided only to pull out the locations that have high correlation values.<br>

As we learned during the statistics class a few weeks ago, a correlation value over 0.7 is considered to have strong positive correlations.<br>

However, after trying few attempts on the datasets that we have, we found out that there were no correlation values that exceed 0.7.<br>

Here are the results of the code for finding observatories for those having comparatively high correlation values. (over 0.5)<br>
```
Location pairs with correlation greater than 0.5: 
홍성죽도 춘장대 0.5118841268952582
홍성죽도 대천항 0.5031924866239239
서천 춘장대 0.5537784181815018
서천 양화 0.572301354414953
춘장대 대천항 0.5216760555346533
```
<br>
After this attempt, I have decided to use the below locations to apply the math equation I previously created to get the cardinal wind direction <br>

`서천 춘장대 0.5537784181815018`<br>

`서천 양화 0.572301354414953`<br>

However I realized that I didn’t consider the wind speed and time differences when calculating these correlation values.<br>
It is obvious that all the locations have low correlation values since the air is moving, and it is nearly impossible to have any correlation at different locations at the exact same time.<br>

***However,*** <br>
I realized that there was no need to find cardinal degrees or cardinal directions.<br>
As the task is simply asking for the PM2.5 prediction for Year 4 and as all the wind directions are directed with relative direction, it no longer seemed like I need to figure out the cardinal degree/direction.<br>

Moreover, I thought it was better to wisely use the remaining time to work on the data that are already there to be used.<br>

I will first share the codes below, which I constructed to calculate the cardinal direction.<br>
```
# tempDf = awsmap_csv['일시']
tempDf = awsmap_csv.loc[:, "일시"] # As the origin data is in date-time format, I want to separate the times alone.
print(tempDf)

for i in awsmap_csv[["일시"]]:
    time_list.append(i)
print(time_list)

# Load data from CSV files
lati_data = pd.read_csv(awsmap_csv, index_col='Latitude')
longi_data = pd.read_csv(awsmap_csv, index_col='Longitude')
dir_data = {loc: pd.read_csv(f'{loc}_dir_data.csv', index_col='Time') for loc in lat_lon_data.index}
speed_data = {loc: pd.read_csv(f'{loc}_speed_data.csv', index_col='Time') for loc in lat_lon_data.index}

# Define function to calculate wind speed increment value
def calculate_increment_percent(adir, bdir, abtime, t1):
    return (bdir.loc[t1+abtime] / adir.loc[t1]) * 100

# Define a function to find highly correlated locations
def find_highly_correlated_locations(dir_data, speed_data, lat_lon_data):
    # Calculate correlation matrix
    dir_df = pd.concat([dir_data[loc].rename(columns={'Direction': loc}) for loc in dir_data], axis=1)
    corr_matrix = dir_df.corr()

    # Find highly correlated locations (correlation >= 0.8)
    highly_corr = np.where(np.abs(corr_matrix) >= 0.8)
    highly_corr = [(corr_matrix.index[x], corr_matrix.columns[y]) for x, y in zip(*highly_corr) if x != y and x < y]

    # Create a dictionary to store wind speed increment values for each pair of highly correlated locations
    wind_speed_increments = {}

    # Calculate the wind speed increment value for each pair of highly correlated locations
    for loc1, loc2 in highly_corr:
        # Get wind direction and wind speed data for both locations
        dir1 = dir_data[loc1]['풍향']
        dir2 = dir_data[loc2]['풍향']
        speed1 = speed_data[loc1]['풍속']
        speed2 = speed_data[loc2]['풍속']

        # Calculate distances between locations (assuming Earth is a sphere)
        R = 6371  # Earth's radius in km
        lat1, lon1 = lat_lon_data.loc[loc1, ['Latitude', 'Longitude']]
        lat2, lon2 = lat_lon_data.loc[loc2, ['Latitude', 'Longitude']]
        dlat = np.radians(lat2 - lat1)
        dlon = np.radians(lon2 - lon1)
        a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon/2) * np.sin(dlon/2)
        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
        d = R * c  # distance in km

        # Calculate the time it takes for the wind to travel from loc1 to loc2
        time = d / ((speed1 + speed2) / 2)

        # Calculate wind speed increment value
        incr_percent = calculate_increment_percent(dir1, dir2, time, 0)

        # Add wind speed increment value to the dictionary
        wind_speed_increments[(loc1, loc2)] = incr_percent

    return wind_speed_increments

# Call function to find highly correlated locations and their wind speed increment values
wind_speed_increments = find_highly_correlated_locations(dir_data, speed_data, lat_lon_data)
```

Shortly after realizing the above, I joined with others as a team and got some missions from them.<br><br>
**Finding missing values.**<br>

Here are some methods for finding missing values in Time-Series data.<br>

**Forward Fill (FFill)** : This method uses previous values to fill out the missing values.<br>
**Backward Fill (BFill)** : This method uses the next values to fill out the missing values.<br>
**Linear Interpolation** : This method replaces missing values with a linearly interpolated value between the previous and next values.<br>

As the results of each method will be stored separately, I had to choose which one to try first.<br>
So I decided to first go with Linear Interpolation, which I thought applies the trend the best among these three given methods.<br>

Below is the example code of Linear Interpolation.<br>
```
# Create a sample time series data with missing values
date_range = pd.date_range(start='1/1/2021', end='1/10/2021')
ts = pd.Series([1, 2, None, 4, None, 6, 7, 8, None, 10], index=date_range)

# Fill missing values using the Linear Interpolation method
ts_interpolate = ts.interpolate()
print(ts_interpolate)
```
Based on the example, my journey began (with lots of manual tries and errors).<br>
As the [journey](https://www.notion.so/jason-heesang-lee/AI-SPARK-Challenge-9e8c62ccb2684f928204b3d0f9e7976e?pvs=4#377826cf0c6a4c38959a88c901d2bef3) was too long, I will link it to my notion.<br>
This worked very well and added the code below to create CSV file 🙂<br>
```
AWS_paths = all_file_locations["train_aws"]

for train_aws_file in all_file_locations['train_aws']:
    train_aws_file = unicodedata.normalize('NFC', train_aws_file)


for train_aws_file in all_file_locations['train_aws']:
    data = pd.read_csv(train_aws_file) # each file / type = pd.DataFrame
    location_name = train_aws_file.split("/")[-1].split(".")[0]
    for columns_each in data[data.columns[3:8]]:
        data[columns_each] = data[columns_each].interpolate()

    data.to_csv(dataset + f"temporary/{location_name}_filled.csv", index=True)
```
This looks very short, neat, and well-working.<br>
I am satisfied with this code and will duplicate this for the PM files.<br><br>
***No.*** <br>
As per the competition requirements mentioned, it is necessary to use AWS data to find missing values, and predictions for Year 4.<br>
Therefore, I made some hypotheses.<br>
> In order to find missing values of PM data, It is necessary to use AWS data.<br>
We need to know which columns of AWS data have a high correlation with PM data.<br>
For example, correlation between Wind_Direction and Wind_Speed.<br>
However, PM data does not include any of those weather-related data.<br>
Therefore, we could only assume their weather data with the AWS datasets.<br>
It would not be logical if we manually group the nearest AWS data and average them.<br>
There are a few ways to assume PM locations’ weather data using AWS data.<br>
This is similar to the theory that I established for finding the Cardinal Degree of Wind (which was abandoned during my work)<br>

1. Set a limit of e.g. 5 or 10 km (can be decided afterward) radius per PM observatory to group and average the data of the AWS observatories.
2. **Find the distance between each observatory, and apply the change of trend ratio to the location of the PM observatory.**

> Logically thinking, the second option, which I just came up with while going through the notes I took, is the best solution.<br>
By doing so, we could also add the PM observatories’ weather data when predicting Year 4’s PM data, THE FINAL SUBMISSION.

Here, my attempt to find distances between each observatory starts.<br>
You can check my previous code for finding cardinal direction [here](https://www.notion.so/AI-SPARK-Challenge-9e8c62ccb2684f928204b3d0f9e7976e?pvs=21) 

I tried to modify the code I composed for finding cardinal direction, but it was not working so well.<br>
Even though I am interested in finding solutions by going through qualifications, I didn’t have enough time to create a whole math logic.<br><br>

Therefore, I decided to import a library (`haversine`) that others have already created.<br>
```
def obs_distance(df1, df2):
# create empty list to store values
    rows = []
# go through index of df1
    for i in range(len(df1)):
# go through index of df2
        for j in range(len(df2)):
# prevent calculating the distance between the same location
            if i != j:
                df1_loc = df1["Location"][i]
                df2_loc = df2["Location"][j]
                df1_lat = df1["Latitude"][i]
                df2_lat = df2["Latitude"][j]
                df1_lng = df1["Longitude"][i]
                df2_lng = df2["Longitude"][j]
# assign lattitude and longitude as one tuple
                point_1 = (df1_lat, df1_lng)
                point_2 = (df2_lat, df2_lng)
# get distance by using haversine
                distance = hs.haversine(point_1, point_2)
# append all data to row
                rows.append({"Location A": df1_loc, "Location B": df2_loc, "Distance": distance})
# go through each data and get 
    df = pd.concat([pd.DataFrame(row, index=[0]) for row in rows], ignore_index=True)
    return df
```

It was way more effective than I thought, and it saved a lot of time.<br>
Then I applied the description to the data I have.<br>
This worked well, and it seems like I could begin with the next step.<br>
<img width="705" alt="image" src="https://github.com/jasonheesanglee/kaggle/assets/123557477/7980c1f3-f36b-4b45-bc42-56c5add6027c">
<br>
I have checked the statistical data of these distances.<br>
As there are 870, 493, and 272 data per distance set, I assumed the range based on the first quartile would cover all PM observatories.<br>
<img width="707" alt="image" src="https://github.com/jasonheesanglee/kaggle/assets/123557477/41b0f163-9673-4d2c-a61a-a94a15a869e7">
As all PM observatories were included in the chart, we are good to go for the next step.<br>
To find a trend between two locations, we would need to go over all 5 columns that we are aiming to compare.<br>
We could go through them one by one, but it was too much work for a simple operation.<br>
