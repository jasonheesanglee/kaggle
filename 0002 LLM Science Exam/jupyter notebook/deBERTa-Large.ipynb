{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## BASED ON MGÃ¶ksu's NOTEBOOK","metadata":{}},{"cell_type":"markdown","source":"#### It would be great if you could upvote when you fork this notebook xD","metadata":{}},{"cell_type":"markdown","source":"# OpenBook DeBERTaV3-Large with an updated model\n\nThis work is based on the great [work](https://www.kaggle.com/code/nlztrk/openbook-debertav3-large-baseline-single-model) of [nlztrk](https://www.kaggle.com/nlztrk).\n\nI trained a model offline using the dataset I shared [here](https://www.kaggle.com/datasets/mgoksu/llm-science-exam-dataset-w-context). I just added my model to the original notebook. The model is available [here](https://www.kaggle.com/datasets/mgoksu/llm-science-run-context-2).\n\nI also addressed the problem of [CSV Not Found at submission](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/434228) with this notebook by clipping the context like so:\n\n`test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:1500]) + \" #### \" +  test_df[\"prompt\"]`\n\nYou can probably get more than 1500 without getting an OOM.","metadata":{}},{"cell_type":"code","source":"# installing offline dependencies\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":126.809817,"end_time":"2023-08-14T10:09:22.925969","exception":false,"start_time":"2023-08-14T10:07:16.116152","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:20:20.032459Z","iopub.execute_input":"2023-08-31T04:20:20.033105Z","iopub.status.idle":"2023-08-31T04:22:30.448998Z","shell.execute_reply.started":"2023-08-31T04:20:20.033072Z","shell.execute_reply":"2023-08-31T04:22:30.447683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\nfrom __future__ import annotations\n\nfrom collections.abc import Iterable\n\nimport faiss\nfrom faiss import write_index, read_index\n\nfrom sentence_transformers import SentenceTransformer\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader","metadata":{"papermill":{"duration":8.534957,"end_time":"2023-08-14T10:09:31.474781","exception":false,"start_time":"2023-08-14T10:09:22.939824","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:22:30.452580Z","iopub.execute_input":"2023-08-31T04:22:30.453751Z","iopub.status.idle":"2023-08-31T04:22:46.571106Z","shell.execute_reply.started":"2023-08-31T04:22:30.453711Z","shell.execute_reply":"2023-08-31T04:22:46.570205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_documents(documents: Iterable[str],\n                      document_ids: Iterable,\n                      split_sentences: bool = True,\n                      filter_len: int = 5,\n                      disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Main helper function to process documents from the EMR.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param document_type: String denoting the document type to be processed\n    :param document_sections: List of sections for a given document type to process\n    :param split_sentences: Flag to determine whether to further split sections into sentences\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n    \n    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n\n    if split_sentences:\n        df = sentencize(df.text.values, \n                        df.document_id.values,\n                        df.offset.values, \n                        filter_len, \n                        disable_progress_bar)\n    return df\n\n\ndef sectionize_documents(documents: Iterable[str],\n                         document_ids: Iterable,\n                         disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Obtains the sections of the imaging reports and returns only the \n    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n    \"\"\"\n    processed_documents = []\n    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n        row = {}\n        text, start, end = (document, 0, len(document))\n        row['document_id'] = document_id\n        row['text'] = text\n        row['offset'] = (start, end)\n\n        processed_documents.append(row)\n\n    _df = pd.DataFrame(processed_documents)\n    if _df.shape[0] > 0:\n        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n    else:\n        return _df\n\n\ndef sentencize(documents: Iterable[str],\n               document_ids: Iterable,\n               offsets: Iterable[tuple[int, int]],\n               filter_len: int = 5,\n               disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Split a document into sentences. Can be used with `sectionize_documents`\n    to further split documents into more manageable pieces. Takes in offsets\n    to ensure that after splitting, the sentences can be matched to the\n    location in the original documents.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param offsets: Iterable tuple of the start and end indices\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n\n    document_sentences = []\n    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n        try:\n            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n            for o in sentence_offsets:\n                if o[1]-o[0] > filter_len:\n                    sentence = document[o[0]:o[1]]\n                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n                    row = {}\n                    row['document_id'] = document_id\n                    row['text'] = sentence\n                    row['offset'] = abs_offsets\n                    document_sentences.append(row)\n        except:\n            continue\n    return pd.DataFrame(document_sentences)","metadata":{"papermill":{"duration":0.034054,"end_time":"2023-08-14T10:09:31.574046","exception":false,"start_time":"2023-08-14T10:09:31.539992","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:22:46.572505Z","iopub.execute_input":"2023-08-31T04:22:46.572976Z","iopub.status.idle":"2023-08-31T04:22:46.592017Z","shell.execute_reply.started":"2023-08-31T04:22:46.572938Z","shell.execute_reply":"2023-08-31T04:22:46.590730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\nDEVICE = 0\nMAX_LENGTH = 512\nBATCH_SIZE = 12\n\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)","metadata":{"papermill":{"duration":0.036342,"end_time":"2023-08-14T10:09:31.623595","exception":false,"start_time":"2023-08-14T10:09:31.587253","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:22:46.595078Z","iopub.execute_input":"2023-08-31T04:22:46.595518Z","iopub.status.idle":"2023-08-31T04:22:46.621829Z","shell.execute_reply.started":"2023-08-31T04:22:46.595484Z","shell.execute_reply":"2023-08-31T04:22:46.620823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Relevant Title Retrieval","metadata":{}},{"cell_type":"code","source":"trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", 1)\ntrn.head()","metadata":{"papermill":{"duration":0.058533,"end_time":"2023-08-14T10:09:31.695383","exception":false,"start_time":"2023-08-14T10:09:31.63685","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:22:46.623254Z","iopub.execute_input":"2023-08-31T04:22:46.623590Z","iopub.status.idle":"2023-08-31T04:22:46.663025Z","shell.execute_reply.started":"2023-08-31T04:22:46.623559Z","shell.execute_reply":"2023-08-31T04:22:46.662036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\nmodel = model.half()","metadata":{"papermill":{"duration":13.282604,"end_time":"2023-08-14T10:09:44.992949","exception":false,"start_time":"2023-08-14T10:09:31.710345","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:22:46.664545Z","iopub.execute_input":"2023-08-31T04:22:46.664899Z","iopub.status.idle":"2023-08-31T04:22:49.334571Z","shell.execute_reply.started":"2023-08-31T04:22:46.664868Z","shell.execute_reply":"2023-08-31T04:22:49.333394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")","metadata":{"papermill":{"duration":95.926417,"end_time":"2023-08-14T10:11:20.934445","exception":false,"start_time":"2023-08-14T10:09:45.008028","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:22:49.338667Z","iopub.execute_input":"2023-08-31T04:22:49.339008Z","iopub.status.idle":"2023-08-31T04:24:19.136349Z","shell.execute_reply.started":"2023-08-31T04:22:49.338978Z","shell.execute_reply":"2023-08-31T04:24:19.135054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_embeddings = model.encode(trn.prompt.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n_ = gc.collect()","metadata":{"papermill":{"duration":10.891104,"end_time":"2023-08-14T10:11:31.84869","exception":false,"start_time":"2023-08-14T10:11:20.957586","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:24:19.139549Z","iopub.execute_input":"2023-08-31T04:24:19.140028Z","iopub.status.idle":"2023-08-31T04:24:30.964957Z","shell.execute_reply.started":"2023-08-31T04:24:19.139990Z","shell.execute_reply":"2023-08-31T04:24:30.963906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get the top 3 pages that are likely to contain the topic of interest\nsearch_score, search_index = sentence_index.search(prompt_embeddings, 3)","metadata":{"papermill":{"duration":23.339585,"end_time":"2023-08-14T10:11:55.247556","exception":false,"start_time":"2023-08-14T10:11:31.907971","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:24:30.966497Z","iopub.execute_input":"2023-08-31T04:24:30.967301Z","iopub.status.idle":"2023-08-31T04:24:55.612712Z","shell.execute_reply.started":"2023-08-31T04:24:30.967264Z","shell.execute_reply":"2023-08-31T04:24:55.611864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Save memory - delete sentence_index since it is no longer necessary\ndel sentence_index\ndel prompt_embeddings\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"papermill":{"duration":0.877305,"end_time":"2023-08-14T10:11:56.145444","exception":false,"start_time":"2023-08-14T10:11:55.268139","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:24:55.619615Z","iopub.execute_input":"2023-08-31T04:24:55.621448Z","iopub.status.idle":"2023-08-31T04:24:56.516727Z","shell.execute_reply.started":"2023-08-31T04:24:55.621407Z","shell.execute_reply":"2023-08-31T04:24:56.515775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting Sentences from the Relevant Titles","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                     columns=['id', 'file'])","metadata":{"papermill":{"duration":5.737408,"end_time":"2023-08-14T10:12:01.897408","exception":false,"start_time":"2023-08-14T10:11:56.16","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:24:56.518099Z","iopub.execute_input":"2023-08-31T04:24:56.518549Z","iopub.status.idle":"2023-08-31T04:25:01.695965Z","shell.execute_reply.started":"2023-08-31T04:24:56.518511Z","shell.execute_reply":"2023-08-31T04:25:01.694670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get the article and associated file location using the index\nwikipedia_file_data = []\n\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    scr_idx = idx\n    _df = df.loc[scr_idx].copy()\n    _df['prompt_id'] = i\n    wikipedia_file_data.append(_df)\nwikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\nwikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n## Save memory - delete df since it is no longer necessary\ndel df\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"papermill":{"duration":0.799872,"end_time":"2023-08-14T10:12:02.712752","exception":false,"start_time":"2023-08-14T10:12:01.91288","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:25:01.697643Z","iopub.execute_input":"2023-08-31T04:25:01.698088Z","iopub.status.idle":"2023-08-31T04:25:02.676348Z","shell.execute_reply.started":"2023-08-31T04:25:01.698030Z","shell.execute_reply":"2023-08-31T04:25:02.675254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get the full text data\nwiki_text_data = []\n\nfor file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n\n    _df_temp = _df[_df['id'].isin(_id)].copy()\n    del _df\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    wiki_text_data.append(_df_temp)\nwiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n_ = gc.collect()","metadata":{"papermill":{"duration":303.981049,"end_time":"2023-08-14T10:17:06.710072","exception":false,"start_time":"2023-08-14T10:12:02.729023","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:25:02.677712Z","iopub.execute_input":"2023-08-31T04:25:02.678326Z","iopub.status.idle":"2023-08-31T04:29:37.450948Z","shell.execute_reply.started":"2023-08-31T04:25:02.678291Z","shell.execute_reply":"2023-08-31T04:29:37.449808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Parse documents into sentences\nprocessed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)","metadata":{"papermill":{"duration":4.491281,"end_time":"2023-08-14T10:17:11.220342","exception":false,"start_time":"2023-08-14T10:17:06.729061","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:29:37.452879Z","iopub.execute_input":"2023-08-31T04:29:37.453841Z","iopub.status.idle":"2023-08-31T04:29:42.197396Z","shell.execute_reply.started":"2023-08-31T04:29:37.453781Z","shell.execute_reply":"2023-08-31T04:29:42.196372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get embeddings of the wiki text data\nwiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE,\n                                    show_progress_bar=True,\n                                    convert_to_tensor=True,\n                                    normalize_embeddings=True)#.half()\nwiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()","metadata":{"papermill":{"duration":25.110593,"end_time":"2023-08-14T10:17:36.348422","exception":false,"start_time":"2023-08-14T10:17:11.237829","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:29:42.198781Z","iopub.execute_input":"2023-08-31T04:29:42.199242Z","iopub.status.idle":"2023-08-31T04:30:24.841682Z","shell.execute_reply.started":"2023-08-31T04:29:42.199208Z","shell.execute_reply":"2023-08-31T04:30:24.840511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = gc.collect()","metadata":{"papermill":{"duration":0.315807,"end_time":"2023-08-14T10:17:36.679867","exception":false,"start_time":"2023-08-14T10:17:36.36406","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:30:24.843445Z","iopub.execute_input":"2023-08-31T04:30:24.843850Z","iopub.status.idle":"2023-08-31T04:30:25.156701Z","shell.execute_reply.started":"2023-08-31T04:30:24.843794Z","shell.execute_reply":"2023-08-31T04:30:25.155424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Combine all answers\ntrn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n\n## Search using the prompt and answers to guide the search\ntrn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']","metadata":{"papermill":{"duration":0.034767,"end_time":"2023-08-14T10:17:36.730378","exception":false,"start_time":"2023-08-14T10:17:36.695611","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:30:25.158534Z","iopub.execute_input":"2023-08-31T04:30:25.158975Z","iopub.status.idle":"2023-08-31T04:30:25.179196Z","shell.execute_reply.started":"2023-08-31T04:30:25.158934Z","shell.execute_reply":"2023-08-31T04:30:25.178093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nquestion_embeddings = question_embeddings.detach().cpu().numpy()","metadata":{"papermill":{"duration":0.431343,"end_time":"2023-08-14T10:17:37.177862","exception":false,"start_time":"2023-08-14T10:17:36.746519","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:30:25.180548Z","iopub.execute_input":"2023-08-31T04:30:25.180907Z","iopub.status.idle":"2023-08-31T04:30:25.665864Z","shell.execute_reply.started":"2023-08-31T04:30:25.180879Z","shell.execute_reply":"2023-08-31T04:30:25.664773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting Matching Prompt-Sentence Pairs","metadata":{}},{"cell_type":"code","source":"## Parameter to determine how many relevant sentences to include\nNUM_SENTENCES_INCLUDE = 4\n\n## List containing just Context\ncontexts = []\n\nfor r in tqdm(trn.itertuples(), total=len(trn)):\n\n    prompt_id = r.Index\n\n    prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n\n    if prompt_indices.shape[0] > 0:\n        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n        prompt_index.add(wiki_data_embeddings[prompt_indices])\n\n        context = \"\"\n        \n        ## Get the top matches\n        ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n        for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n            context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n        \n    contexts.append(context)","metadata":{"papermill":{"duration":1.609553,"end_time":"2023-08-14T10:17:38.836268","exception":false,"start_time":"2023-08-14T10:17:37.226715","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:30:25.667552Z","iopub.execute_input":"2023-08-31T04:30:25.668269Z","iopub.status.idle":"2023-08-31T04:30:27.840115Z","shell.execute_reply.started":"2023-08-31T04:30:25.668233Z","shell.execute_reply":"2023-08-31T04:30:27.838978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn['context'] = contexts","metadata":{"papermill":{"duration":0.024188,"end_time":"2023-08-14T10:17:38.878394","exception":false,"start_time":"2023-08-14T10:17:38.854206","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:30:27.842763Z","iopub.execute_input":"2023-08-31T04:30:27.845435Z","iopub.status.idle":"2023-08-31T04:30:27.851009Z","shell.execute_reply.started":"2023-08-31T04:30:27.845401Z","shell.execute_reply":"2023-08-31T04:30:27.849904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)","metadata":{"papermill":{"duration":0.050945,"end_time":"2023-08-14T10:17:38.944423","exception":false,"start_time":"2023-08-14T10:17:38.893478","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:30:27.852777Z","iopub.execute_input":"2023-08-31T04:30:27.853244Z","iopub.status.idle":"2023-08-31T04:30:27.906602Z","shell.execute_reply.started":"2023-08-31T04:30:27.853206Z","shell.execute_reply":"2023-08-31T04:30:27.905628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"papermill":{"duration":0.015828,"end_time":"2023-08-14T10:17:39.007683","exception":false,"start_time":"2023-08-14T10:17:38.991855","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test_df = pd.read_csv(\"test_context.csv\")\ntest_df.index = list(range(len(test_df)))\ntest_df['id'] = list(range(len(test_df)))\ntest_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:1750]) + \" #### \" +  test_df[\"prompt\"] + ' #### '\ntest_df['answer'] = 'A'","metadata":{"papermill":{"duration":0.037633,"end_time":"2023-08-14T10:17:39.605345","exception":false,"start_time":"2023-08-14T10:17:39.567712","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:30:27.908343Z","iopub.execute_input":"2023-08-31T04:30:27.908751Z","iopub.status.idle":"2023-08-31T04:30:27.936775Z","shell.execute_reply.started":"2023-08-31T04:30:27.908714Z","shell.execute_reply":"2023-08-31T04:30:27.935762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir = \"/kaggle/input/llm-science-run-context-2\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()","metadata":{"papermill":{"duration":21.360878,"end_time":"2023-08-14T10:18:01.027859","exception":false,"start_time":"2023-08-14T10:17:39.666981","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:30:27.938446Z","iopub.execute_input":"2023-08-31T04:30:27.938878Z","iopub.status.idle":"2023-08-31T04:30:49.199194Z","shell.execute_reply.started":"2023-08-31T04:30:27.938842Z","shell.execute_reply":"2023-08-31T04:30:49.197945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\noptions = 'ABCDE'\nindices = list(range(5))\n\noption_to_index = {option: index for option, index in zip(options, indices)}\nindex_to_option = {index: option for option, index in zip(options, indices)}\n\ndef preprocess(example):\n    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n    # so we'll copy our question 5 times before tokenizing\n    first_sentence = [example['prompt']] * 5\n    second_sentence = []\n    for option in options:\n        second_sentence.append(example[option])\n    # Our tokenizer will turn our text into token IDs BERT can understand\n    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    return tokenized_example","metadata":{"papermill":{"duration":0.026162,"end_time":"2023-08-14T10:18:01.129276","exception":false,"start_time":"2023-08-14T10:18:01.103114","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:30:49.201231Z","iopub.execute_input":"2023-08-31T04:30:49.201647Z","iopub.status.idle":"2023-08-31T04:30:49.213335Z","shell.execute_reply.started":"2023-08-31T04:30:49.201611Z","shell.execute_reply":"2023-08-31T04:30:49.210160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch","metadata":{"papermill":{"duration":0.030447,"end_time":"2023-08-14T10:18:01.175589","exception":false,"start_time":"2023-08-14T10:18:01.145142","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:30:49.215121Z","iopub.execute_input":"2023-08-31T04:30:49.220683Z","iopub.status.idle":"2023-08-31T04:30:49.233641Z","shell.execute_reply.started":"2023-08-31T04:30:49.220646Z","shell.execute_reply":"2023-08-31T04:30:49.232344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\ndata_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)","metadata":{"papermill":{"duration":0.493618,"end_time":"2023-08-14T10:18:01.685989","exception":false,"start_time":"2023-08-14T10:18:01.192371","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:30:49.235077Z","iopub.execute_input":"2023-08-31T04:30:49.235863Z","iopub.status.idle":"2023-08-31T04:30:52.578068Z","shell.execute_reply.started":"2023-08-31T04:30:49.235763Z","shell.execute_reply":"2023-08-31T04:30:52.577100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions.append(outputs.logits.cpu().detach())\n\ntest_predictions = torch.cat(test_predictions)\n\npredictions_as_ids = np.argsort(-test_predictions, 1)\n\npredictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n# predictions_as_answer_letters[:3]\n\npredictions_as_string = test_df['prediction'] = [\n    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n]","metadata":{"papermill":{"duration":1.101895,"end_time":"2023-08-14T10:18:02.804298","exception":false,"start_time":"2023-08-14T10:18:01.702403","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:30:52.582598Z","iopub.execute_input":"2023-08-31T04:30:52.585296Z","iopub.status.idle":"2023-08-31T04:32:55.431499Z","shell.execute_reply.started":"2023-08-31T04:30:52.585260Z","shell.execute_reply":"2023-08-31T04:32:55.430282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test_df[['id', 'prediction']]\nsubmission.to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.033576,"end_time":"2023-08-14T10:19:17.733491","exception":false,"start_time":"2023-08-14T10:19:17.699915","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-31T04:32:55.437512Z","iopub.execute_input":"2023-08-31T04:32:55.437871Z","iopub.status.idle":"2023-08-31T04:32:55.448606Z","shell.execute_reply.started":"2023-08-31T04:32:55.437837Z","shell.execute_reply":"2023-08-31T04:32:55.447519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2023-08-31T04:43:11.728748Z","iopub.execute_input":"2023-08-31T04:43:11.729174Z","iopub.status.idle":"2023-08-31T04:43:11.742589Z","shell.execute_reply.started":"2023-08-31T04:43:11.729123Z","shell.execute_reply":"2023-08-31T04:43:11.741077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}