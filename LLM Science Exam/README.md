# LLM Science Exam
2023-07-12 ~ 2023-10-11

While working my ongoing competition - ICR - Identifying Age-Related Conditions, I noticed that there is another NLP competition was started in Kaggle.<br>
After working on [Monthly DACON - Judicial Precedent Prediction](https://www.notion.so/dc3d9fbca3134a4e8546550c89b2164c?pvs=21) competition, I felt a bit more confident with NLP tasks.<br>
Therefore, I decided to compete for this competition.<br>
Here are the brief requirements of the competition.<br>

## Requirements - Competition Host
### Goal of the Competition
  - Inspired by the [OpenBookQA dataset](https://allenai.org/data/open-book-qa), this competition challenges participants to answer difficult science-based questions *written by a Large Language Model*.<br>
  - Your work will help researchers better understand the ability of LLMs to test themselves, and the potential of LLMs that can be run in resource-constrained environments.

### Context
- As the scope of large language model capabilities expands, a growing area of research is [using LLMs to characterize themselves](https://arxiv.org/abs/2212.09251).<br>
Because many preexisting NLP benchmarks have been shown to be trivial for state-of-the-art models, there has also been interesting work showing that [LLMs can be used to create more challenging tasks to test ever more powerful models](https://arxiv.org/abs/1905.07830).<br>
- At the same time methods like quantization and knowledge distillation are being used to effectively shrink language models and run them on more modest hardware.<br>
The Kaggle environment provides a unique lens to study this as submissions are subject to both GPU and time limits.<br>
- The dataset for this challenge was generated by giving gpt3.5 snippets of text on a range of scientific topics pulled from wikipedia, and asking it to write a multiple choice question (with a known answer), then filtering out easy questions.<br>
- Right now we estimate that the largest models run on Kaggle are around 10 billion parameters, whereas gpt3.5 clocks in at 175 billion parameters.<br>
If a question-answering model can ace a test written by a question-writing model more than 10 times its size, this would be a genuinely interesting result; on the other hand if a larger model can effectively stump a smaller one, this has compelling implications on the ability of LLMs to benchmark and test themselves.<br>

### Evaluation
- Submissions are evaluated according to the Mean Average Precision @ 3 (MAP@3):<br>
```math
MAP @ 3=\frac{1}{U} \sum_{u=1}^U \sum_{k=1}^{\min (n, 3)} P(k) \times rel(k)
```
- where 𝑈 is the number of questions in the test set, 𝑃(𝑘) is the precision at cutoff 𝑘, 𝑛 is the number predictions per question, and 𝑟𝑒𝑙(𝑘) is an indicator function equaling 1 if the item at rank 𝑘 is a relevant (correct) label, zero otherwise.<br>
- Once a correct label has been scored for an individual question in the test set, that label is no longer considered relevant for that question, and additional predictions of that label are skipped in the calculation.<br>For example, if the correct label is `A` for an observation, the following predictions all score an average precision of `1.0`.
```
[A, B, C, D, E]
[A, A, A, A, A]
[A, B, A, C, A]
```

### Submission File
For each `id` in the test set, you may predict up to 3 labels for your `prediction`.<br>
The file should contain a header and have the following format:<br>
```
id, prediction
0, A B C
1, B C A
2, C A B
etc.
```

## Approach
- To perform better on the competitions, the first thing I have done was to take a look on others’ work.<br><br>
- After going through around 30 notebooks, I realized that many of them were using `T5` as their Main Deep Learning model.<br>
  Then, I found the [research paper on T5](https://arxiv.org/pdf/1910.10683.pdf) from `arxiv` and started going through it.
- While going through the paper, I could understand how this model work and why people chose this model as their main model.<br>
  According to the paper, this model was designed to perform Text-to-Text tasks as an encoder-decoder model, unlikely to the previous model `BERT`, which was an encoder-only model.<br>
  One of the point the researchers pointed out on their paper was that using data from Wikipedia increased the performance on Question Answering tasks.<br>
  > *As a final example, using data from Wikipedia produced significant (but less dramatic) gains on SQuAD, which is a question-answering data set with passages sourced from Wikipedia.*
  
  Moreover, they mentioned that in the previous research on BERT, pre-training the model with research papers increased performance on scientific tasks.
  > *Similar observations have been made in prior work, e.g. Beltagy et al. (2019) found that pre-training BERT on text from research papers improved its performance on scientific tasks.
  > The main lesson behind these findings is that pre-training on in-domain unlabeled data can improve performance on downstream tasks.*
  
- Combining these two points, I decided to crawl scientific research paper, generate set of questions and answers to use as additional training data.<br>After a bit of research on discussion section of the competition, I realized that there are already some dataset that other people have created.<br>As I was unfamiliar of generating question & answers from plain text, I decided to use their dataset.<br>

### `DeBERTA`
- I have found `DeBERTa` has become popular among the competitors in another ongonig competition CommonLit- Evaluating Students Summary, and therefore I have also tried to implement it here.<br><br>
  There are few versions for DeBERTa and they are :<br>
- `deberta-base`
- `deberta-xlarge-mnli`
- `deberta-v2-xxlarge`
- `deberta-v3-small`
- `deberta-v3-base`
- `deberta-v3-large`

  Among these models, I decided to go with `deberta-v3-base` and `deberta-v3-large`.<br>
  This decision was made very simply, I wanted to use the latest model.<br><br>

- I have first started with `deberta-v3-large` but unfortunately the notebook kept threw either `Out of Memory Error` or `Notebook Timeout Error`.<br>
After the failure of the first attempt with `deberta-v3-large`, I tried my best to figure out why it was throwing exception.<br>
I tried to modify all those hyperparameters, number of layers and etc.<br>
Later found out that the model was to large that it wouldn’t finish the task within 9 hours (even with the GPU).<br>
Therefore I had to go with `deberta-v3-base` and it worked great up to max_length = 384, `batch_size = 32`, `num_sentences_include = 20` and `filter_len = 20`.<br>

### RAG
- After going through others’ notebooks, I learned that the point was on **RAG, Retrieval Augmented Generation**.<br><br>

- Based on one [website](https://www.promptingguide.ai/techniques/rag) that explains about RAG, LLM does not require additional background knowledge to achieve common tasks like sentiment analysis and named entity recognition.<br>However, LLM also need to learn the knowledge from external source to solve complex problems, like Science Exams, based on fact without hallucination.<br><br>

- **RAG** combines the information retrieval component with a text generator model.<br>
**RAG** can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of entire model.<br>
It takes an input and retrieves a set of relevent/supporting documents given a source (e.g. Wikipedia).<br>
The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output.<br>
This makes RAG adaptive for situations where facts could evolve overtime.<br>
This is very useful as LLMs’s parametric knowledge is static.<br>
RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation.<br><br>

    Below is the overview of how the approach works.<br>
    <p align="center">
      <img width="1000" alt="image" src="https://github.com/jasonheesanglee/kaggle/assets/123557477/2b50e067-fbb0-4ef4-b6ac-44982e351975">
    </p>

### LLM
The competition has ended, and below is my throwback.<br><br>

These tryouts with BERT descendant models didn’t make great result.<br>
Therefore, I decided to tryout the LLM models as the competition title pointed out.<br><br>

Pure LLaMa2, without any pre-training, was my first target.<br>
It is obvious now that it wouldn’t work well, however I didn’t know about it well then.<br>
I struggled learning about the model, tried to implement it for the task, it gave the score only around `0.3`. Which was worse than the `deberta` tryouts (around `0.7`).<br><br>

Then, I decided to try Mistral 7B after checking a recent news that this was a brand new LLM.<br>
This gave me a good result (`0.853`), but it I was still not enough to reach the **medal-range**.<br><br>

Then I tried with `Platypus2`  for the last week of the competition, which other competitions were trying with.<br>
This gave me `0.874`, the best result out of all the tryouts I have done.

#### Public LeaderBoard - 15%
<img width="1135" alt="image" src="https://github.com/jasonheesanglee/kaggle/assets/123557477/2ffdcfc1-f320-46e1-9d4a-108c7cf54ea9">

#### Private LeaderBoard - 15%
<img width="1140" alt="image" src="https://github.com/jasonheesanglee/kaggle/assets/123557477/177fd88f-5de6-4779-80e1-55b30d8d78b0">
