# LLM Science Exam
2023-07-12 ~ 2023-10-11

While working my ongoing competition - ICR - Identifying Age-Related Conditions, I noticed that there is another NLP competition was started in Kaggle.<br>
After working on [Monthly DACON - Judicial Precedent Prediction](https://www.notion.so/dc3d9fbca3134a4e8546550c89b2164c?pvs=21) competition, I felt a bit more confident with NLP tasks.<br>
Therefore, I decided to compete for this competition.<br>
Here are the brief requirements of the competition.<br>

## Requirements - Competition Host
### Goal of the Competition
  - Inspired by the [OpenBookQA dataset](https://allenai.org/data/open-book-qa), this competition challenges participants to answer difficult science-based questions *written by a Large Language Model*.<br>
  - Your work will help researchers better understand the ability of LLMs to test themselves, and the potential of LLMs that can be run in resource-constrained environments.

### Context
- As the scope of large language model capabilities expands, a growing area of research is [using LLMs to characterize themselves](https://arxiv.org/abs/2212.09251).<br>
Because many preexisting NLP benchmarks have been shown to be trivial for state-of-the-art models, there has also been interesting work showing that [LLMs can be used to create more challenging tasks to test ever more powerful models](https://arxiv.org/abs/1905.07830).<br>
- At the same time methods like quantization and knowledge distillation are being used to effectively shrink language models and run them on more modest hardware.<br>
The Kaggle environment provides a unique lens to study this as submissions are subject to both GPU and time limits.<br>
- The dataset for this challenge was generated by giving gpt3.5 snippets of text on a range of scientific topics pulled from wikipedia, and asking it to write a multiple choice question (with a known answer), then filtering out easy questions.<br>
- Right now we estimate that the largest models run on Kaggle are around 10 billion parameters, whereas gpt3.5 clocks in at 175 billion parameters.<br>
If a question-answering model can ace a test written by a question-writing model more than 10 times its size, this would be a genuinely interesting result; on the other hand if a larger model can effectively stump a smaller one, this has compelling implications on the ability of LLMs to benchmark and test themselves.<br>

### Evaluation
Submissions are evaluated according to the Mean Average Precision @ 3 (MAP@3):<br>
```math
MAP @ 3=\frac{1}{U} \sum_{u=1}^U \sum_{k=1}^{\min (n, 3)} P(k) \times rel(k)
```
where 𝑈 is the number of questions in the test set, 𝑃(𝑘) is the precision at cutoff 𝑘, 𝑛 is the number predictions per question, and 𝑟𝑒𝑙(𝑘) is an indicator function equaling 1 if the item at rank 𝑘 is a relevant (correct) label, zero otherwise.

Once a correct label has been scored for an individual question in the test set, that label is no longer considered relevant for that question, and additional predictions of that label are skipped in the calculation. For example, if the correct label is `A` for an observation, the following predictions all score an average precision of `1.0`.
